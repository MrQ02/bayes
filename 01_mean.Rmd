# Mean Models {#m}

## AR (Autoregressive model)

### Model

**AR(p) - General**:

$$\begin{equation*} x_t=\alpha_0+\sum_{i=1}^{p}{\alpha_i x_{t-i}}+\varepsilon_t
\end{equation*}$$

- $x_t$: time series to be modeled
- $x_{t-i}$: past time series values
- $p$: lag limit
- $\alpha_i$: params to be estimated
- $\varepsilon_t \sim N(0,\sigma^2)$: i.i.d. white noise

<p>&nbsp;</p>

**AR(p) - Markovian**:

$$\begin{align*}
        p(y_{1:T})&=p(y_{1:p})\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\
        p(\boldsymbol{y}|y_{1:p})&=\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\
        &=\prod_{t=p+1}^{T}{N(y_t|\boldsymbol{f}_t^\prime\boldsymbol{\phi},v)}\\
        &=N(\boldsymbol{y}|\boldsymbol{F}^\prime\boldsymbol{\phi},v\boldsymbol{I}_n)
    \end{align*}$$

- $T=n+p$
- $\phi=(\phi_1,\cdots,\phi_p)$
- $\boldsymbol{f}_t=(y_{t-1},\cdots,y_{t-p})$
- $\boldsymbol{F}=[\boldsymbol{f}_T,\cdots,\boldsymbol{f}_{p+1}]$

<p>&nbsp;</p>

**AR(p) - State-space**:

$$\begin{align*}
        y_t&=\boldsymbol{F}^\prime\boldsymbol{x}_t\\
        \boldsymbol{x}_t&=\boldsymbol{G}\boldsymbol{x}_{t-1}+\boldsymbol{\omega}_t
    \end{align*}$$

- $\boldsymbol{x}_t=(y_t,\cdots,y_{t-p+1})$: state vector
- $\boldsymbol{\omega}_t=(\epsilon_t,0,\cdots,0)$: error vector
- $\boldsymbol{F}=(1,0,\cdots,0)$
- $\boldsymbol{G}=\begin{bmatrix}\phi_1 & \phi_2 & \cdots & \phi_{p-1} & \phi_p \\ 1 & 0 & \cdots & 0 & 0 \\ 0 & 1 & \cdots & 0 & 0 \\ \vdots & & \ddots & 0 & \vdots \\ 0 & 0 & \cdots & 1 & 0\end{bmatrix}$

<p>&nbsp;</p>

**AR(p) - Autoorrelation**:

$$\begin{align*}
        &\rho(h)-\sum_{i=1}^{p}{\phi_i\rho(h-i)}=0\\
        &\rho(h)=\sum_{j=1}^{r}{\alpha_j^hp_j(h)}
    \end{align*}$$
    
- $\alpha_1,\cdots,\alpha_r$: reciprocal roots of characteristic polynomial
- $m_1,\cdots,m_r$: root multiplicity ($\sum_{i=1}^r{m_i}=p$)
- $p_j(h)$: polynomial of degree $m_j-1$

<p>&nbsp;</p>

### Estimation

**LSE - AR(1)**:

$$\begin{align*}
        \hat{\alpha}_{ols}&=\argmin_{\alpha}{\sum_{t=1}^{T}{(x_t-\alpha x_{t-1})^2}}\\
        &=\bigg(\sum_{t=1}^{T}{x_{t-1}^2}\bigg)^{-1}\bigg(\sum_{t=1}^{T}{x_{t-1}x_t}\bigg)
    \end{align*}$$

<p>&nbsp;</p>

**MLE - AR(1)**:

- Conditional likelihood:
    
$$\begin{equation*}
        \mathcal{L}(\boldsymbol{x|x_{-1}})=\bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^T\exp{\bigg(-\frac{1}{2\sigma^2}\sum_{t=1}^{T}{(x_t-\alpha x_{t-1})^2}\bigg)}
    \end{equation*}$$

- Conditional log likelihood:

$$\begin{equation*}
        l(\boldsymbol{x|x_{-1}})=-T\log{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{t=1}^{T}{(x_t-\alpha x_{t-1})^2}
    \end{equation*}$$
    
- Conditional ML estimator:

$$\begin{align*}
        \hat{\alpha}_{ml}&=\argmax_\alpha{l(\boldsymbol{x|x_{-1}})} \\
        &=\argmax_\alpha{\bigg(-\sum_{t=1}^{T}{(x_t-\alpha x_{t-1})^2}\bigg)} \\
        &=\argmin_\alpha{\sum_{t=1}^{T}{(x_t-\alpha x_{t-1})^2}} \\
    \end{align*}$$

<p>&nbsp;</p>

## MA (Moving Average model)

### Model

**MA(q)**:

$$\begin{equation*}
        x_t=\mu+\sum_{i=1}^{q}{\beta_i \varepsilon_{t-i}}+\varepsilon_t
    \end{equation*}$$

- $\mu$: mean of $x_t$
- $\varepsilon_{t-i}$: past error values
- $q$: lag limit
- $\beta_i$: params to be estimated
- $\varepsilon_t$: unobservable white noise at the current time

<p>&nbsp;</p>

### Estimation

**LSE - MA(1)**:

$$\begin{equation*}
        \hat{\beta}_{ols}=\argmin_\beta{\sum_{t=1}^T{(x_t-\beta\varepsilon_{t-1})^2}}
    \end{equation*}$$
    
How to deal with unknown random disturbances:

\begin{align*}
        &\varepsilon_1=x_1-\beta\varepsilon_0 \\
        &\varepsilon_2=x_2-\beta(x_1-\beta\varepsilon_0) \\
        &\varepsilon_3=x_3-\beta(x_2-\beta(x_1-\beta\varepsilon_0)) \\
        
&\varepsilon_{t-1}=(-\beta)^{t-1}\varepsilon_0+\sum_{k=0}^{t-2}{(-\beta)^kx_{t-1-k}}\end{align*}

Thus, we obtain the Nonlinear LS estimator:

$$\begin{equation*}
        \hat{\beta}_{nls}=\argmin_{\beta}{\sum_{t=1}^{T}{\bigg(x_t-\beta\sum_{k=0}^{t-2}{(-\beta)^kx_{t-1-k}}\bigg)^2}}
    \end{equation*}$$

<p>&nbsp;</p>

**MLE - MA(1)**:

How to deal with unknown random disturbances:

\begin{align*}
        &\mathcal{L}(x_1)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{\varepsilon_1^2}{2\sigma^2}\bigg)} \\
        &\mathcal{L}(x_2|x_1)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{(x_2-\beta\varepsilon_1)^2}{2\sigma^2}\bigg)}=\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{\varepsilon_2^2}{2\sigma^2}\bigg)} \\
        &\cdots \\
        &\mathcal{L}(x_t|x_{t-1},\cdots,x_1)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{(x_t-\beta\varepsilon_{t-1})^2}{2\sigma^2}\bigg)}\\
        &=\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{\varepsilon_t^2}{2\sigma^2}\bigg)}
    \end{align*}
    
- Conditional likelihood:

$$\begin{equation*}
        \mathcal{L}(\boldsymbol{x|x_{-1}})=\bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^T\exp{\bigg(-\frac{1}{2\sigma^2}\sum_{t=1}^T{\varepsilon_t^2}\bigg)}
    \end{equation*}$$
    
- Conditional log likelihood:

$$\begin{equation*}
        l(\boldsymbol{x|x_{-1}})=-T\log{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{t=1}^T{\varepsilon_t^2}
    \end{equation*}$$
    
- Conditional ML estimator:

$$\begin{equation*}
        \hat{\beta}_{ml}=\argmax_\beta{l(\boldsymbol{x|x_{-1}})}=\argmin_\beta{\sum_{t=1}^T{\varepsilon_t^2}}
    \end{equation*}$$
    
<p>&nbsp;</p>

## ARMA (Autoregressive Moving Average model)

### Model

**ARMA(p,q) - General**:

$$\begin{equation*}
        x_t=\alpha_0+\sum_{i=1}^{p}{\alpha_i x_{t-i}}+\sum_{i=1}^{q}{\beta_i \varepsilon_{t-i}}+\varepsilon_t
    \end{equation*}$$

- Explanatory variables: $x_{t-i}\ \&\ \varepsilon_{t-i}$
- Parameters: $\alpha_i\ \&\ \beta_i$
- Hyperparameters: $p\ \&\ q$
- Random disturbance: $\varepsilon_t$

<p>&nbsp;</p>

**ARMA(p,q) - State-space**:

\begin{align*}
        y_t&=\boldsymbol{E}_m^\prime\boldsymbol{\theta}_t\\
        \boldsymbol{\theta}_t&=\boldsymbol{G}\boldsymbol{\theta}_{t-1}+\boldsymbol{\omega}_t
    \end{align*}
    
- $\boldsymbol{E}_m=(1,0,\cdots,0)$, shape $m=\max{(p,q+1)}$
- $\boldsymbol{\omega}_t=(1,\theta_1,\cdots,\theta_{m-1})\epsilon_t$
- $\boldsymbol{G}=\begin{bmatrix}\phi_1 & \phi_2 & \cdots & \phi_{m-1} & \phi_m \\ 1 & 0 & \cdots & 0 & 0 \\ 0 & 1 & \cdots & 0 & 0 \\ \vdots & & \ddots & 0 & \vdots \\ 0 & 0 & \cdots & 1 & 0\end{bmatrix}$ 

<p>&nbsp;</p>

### Estimation

**LSE - ARMA(p,q)**:

- Minimize:
    
    $$\begin{equation*}
        S(\boldsymbol{\theta})=\sum_{t=1}^{T}{\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}}
    \end{equation*}$$
    
    - $\boldsymbol{\theta}=(\alpha_1,\cdots,\alpha_p,\beta_1,\cdots,\beta_q)$: parameter set

- Conditional LS:

    $$\begin{equation*}
        S_c(\boldsymbol{\theta})=\sum_{t=p+1}^T{\epsilon_t(\boldsymbol{\theta})^2}
    \end{equation*}$$
    
    - $\epsilon_t(\boldsymbol{\theta})=y_t-\hat{y}_t$
    
<p>&nbsp;</p>

**MLE - ARMA(p,q)**:

- Conditional likelihood:

$$\begin{equation*}
        p(y_{1:T}|\boldsymbol{\theta},v)=\prod_{t=1}^T{p(y_t|y_{1:(t-1)},\boldsymbol{\theta},v)}
    \end{equation*}$$

- Conditional log likelihood:
    
    $$\begin{equation*}
        \log{p(y_{1:T}|\boldsymbol{\theta},v)}=-\frac{T}{2}\log{2\pi v}-\frac{1}{2}\sum_{t=1}^T{\Big(\log{r_t^{t-1}}+\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}\Big)}
    \end{equation*}$$

    - $y_t^{t-1}=\text{E}[y_t|y_{1:(t-1)}]$
    - $vr_t^{t-1}=\text{Var}[y_t|y_{1:(t-1)}]$

<p>&nbsp;</p>

**Bayesian - ARMA(p,q)**:

Likelihood based on parameters:

$$\begin{equation*}
        p(y_{1:T}|\boldsymbol{\varphi})=\big(\frac{1}{\sqrt{2\pi v}}\big)^T\exp{\bigg(-\frac{1}{2v}\sum_{t=1}^T{(y_t-\mu_t)^2}\bigg)}
    \end{equation*}$$
    
- $\boldsymbol{\varphi}=(\boldsymbol{\alpha},\boldsymbol{\beta},v,\boldsymbol{x}_0,\boldsymbol{\epsilon}_0)$
- $\boldsymbol{\epsilon}_0=(\epsilon_0,\epsilon_{-1},\cdots,\epsilon_{1-q})$
- $\mu_1=\sum_{i=1}^p{\alpha_iy_{1-i}}+\sum_{i=1}^q{\beta_i\epsilon_{1-i}}$
- $\mu_t=\sum_{i=1}^p{\alpha_iy_{t-i}}+\sum_{i=1}^{t-1}{\beta_i(y_{t-i}-\mu_{t-i})}+\sum_{i=1}^q{\beta_i\epsilon_{t-i}},t=2:q$
- $\mu_t=\sum_{i=1}^p{\alpha_iy_{t-i}}+\sum_{i=1}^{t-1}{\beta_i(y_{t-i}-\mu_{t-i})},t=q+1:T$

<p>&nbsp;</p>

Prior:

$$\begin{equation*}
        \pi(\boldsymbol{\varphi})=\pi(\boldsymbol{x}_0,\boldsymbol{\epsilon}_0|\boldsymbol{\alpha},\boldsymbol{\beta},v)\pi(v)\pi(\boldsymbol{\alpha},\boldsymbol{\beta})
    \end{equation*}$$
    
- $\pi(\boldsymbol{x}_0,\boldsymbol{\epsilon}_0|\boldsymbol{\alpha},\boldsymbol{\beta},v)=N(\boldsymbol{0},v\Omega)$
- $\pi(v)\propto\frac{1}{v}$
- $\pi(\boldsymbol{\alpha},\boldsymbol{\beta})$: uniform distribution
- $v\Omega=\text{Cov}[\boldsymbol{x}_0,\boldsymbol{\epsilon}_0]$

<p>&nbsp;</p>

Joint Posterior:

$$\begin{equation*}
        p(\boldsymbol{\varphi}|y_{1:T})\propto (v)^{-\frac{T+2}{2}}\exp{-\frac{1}{2v}\sum_{t=1}^T{(y_t-\mu_t)^2}}\times N((\boldsymbol{x}_0,\boldsymbol{\epsilon}_0)|\boldsymbol{0},v\Omega)
    \end{equation*}$$

<p>&nbsp;</p>
 
MCMC:

- Sample $(v|\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{x}_0,\boldsymbol{\epsilon}_0)$ from inverse-gamma full conditional distribution:
- Sample $(\boldsymbol{x}_0,\boldsymbol{\epsilon}_0|\boldsymbol{\alpha},\boldsymbol{\beta},v)$ using a Metropolis step with Gaussian proposal distributions
- Sample $(\boldsymbol{\alpha},\boldsymbol{\beta}|v,\boldsymbol{x}_0,\boldsymbol{\epsilon}_0)$ from PACF

<p>&nbsp;</p>

## ARIMA (Autoregressive Integrated Moving Average model)

### Model

**Differencing**:

\begin{align*}
    &x^{(1)}_t=x_t-x_{t-1}\\
    &x^{(2)}_t=x^{(1)}_t-x^{(1)}_{t-1}\\
    &\cdots\\
    &x^{(d)}_t=x^{(d-1)}_t-x^{(d-1)}_{t-1}
\end{align*}

**ARIMA(p,d,q)**:

$$\begin{equation*}
    \Phi(L)(1-L)^dx_t=\Theta(L)\varepsilon_t
\end{equation*}$$

- AR operator: $\Phi(L)=1-\sum_{i=1}^{p}{\alpha_iL^i}$
- MA operator: $\Theta(L)=1+\sum_{i=1}^{q}{\beta_iL^i}$

<p>&nbsp;</p>

### Estimation

**LSE - ARIMA(p,1,q)**:

- Parameter Collection: $\boldsymbol{\theta}=(\alpha_0,\alpha_1,\cdots,\alpha_p,\beta_1,\cdots,\beta_q)$

-  Independent Variable Collection: $\boldsymbol{\omega}_t=(1,z_{t-1},\cdots,z_{t-p},\hat{\varepsilon}_{t-1},\cdots,\hat{\varepsilon}_{t-q})$

- \textbf{ARIMA($p,1,q$)}:

$$\begin{equation*}
        z_t=\boldsymbol{\theta}^T\boldsymbol{\omega}_t+\varepsilon_t
    \end{equation*}$$
$$\begin{equation*}
        \hat{\boldsymbol{\theta}}_{ols}=\argmin_{\boldsymbol{\theta}}{\sum_{t=2}^{T}{(z_t-\boldsymbol{\theta}^T\boldsymbol{\omega}_t)^2}}
    \end{equation*}$$