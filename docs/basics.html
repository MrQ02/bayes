<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Basics of Neural Networks | Deep Learning</title>
  <meta name="description" content="1 Basics of Neural Networks | Deep Learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Basics of Neural Networks | Deep Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="MrQ02/dl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Basics of Neural Networks | Deep Learning" />
  
  
  

<meta name="author" content="Renyi Qu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="imp.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="lib/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="lib/css/style.css" type="text/css" />
<link rel="stylesheet" href="lib/css/lesson.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Deep Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basics of Neural Networks</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basics.html"><a href="basics.html#neural-network-representation"><i class="fa fa-check"></i><b>1.1</b> Neural Network Representation</a></li>
<li class="chapter" data-level="1.2" data-path="basics.html"><a href="basics.html#activation-functions"><i class="fa fa-check"></i><b>1.2</b> Activation Functions</a></li>
<li class="chapter" data-level="1.3" data-path="basics.html"><a href="basics.html#training"><i class="fa fa-check"></i><b>1.3</b> Training</a></li>
<li class="chapter" data-level="1.4" data-path="basics.html"><a href="basics.html#gradient-descent"><i class="fa fa-check"></i><b>1.4</b> Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="imp.html"><a href="imp.html"><i class="fa fa-check"></i><b>2</b> Improvements on Neural Networks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="imp.html"><a href="imp.html#traintest-split"><i class="fa fa-check"></i><b>2.1</b> Train/Test Split</a></li>
<li class="chapter" data-level="2.2" data-path="imp.html"><a href="imp.html#initialization"><i class="fa fa-check"></i><b>2.2</b> Initialization</a></li>
<li class="chapter" data-level="2.3" data-path="imp.html"><a href="imp.html#data-fitting"><i class="fa fa-check"></i><b>2.3</b> Data Fitting</a></li>
<li class="chapter" data-level="2.4" data-path="imp.html"><a href="imp.html#regularization"><i class="fa fa-check"></i><b>2.4</b> Regularization</a></li>
<li class="chapter" data-level="2.5" data-path="imp.html"><a href="imp.html#optimization"><i class="fa fa-check"></i><b>2.5</b> Optimization</a></li>
<li class="chapter" data-level="2.6" data-path="imp.html"><a href="imp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>2.6</b> Hyperparameter Tuning</a></li>
<li class="chapter" data-level="2.7" data-path="imp.html"><a href="imp.html#batch-normalization"><i class="fa fa-check"></i><b>2.7</b> Batch Normalization</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>3</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cnn.html"><a href="cnn.html#basics-of-cnn"><i class="fa fa-check"></i><b>3.1</b> Basics of CNN</a></li>
<li class="chapter" data-level="3.2" data-path="cnn.html"><a href="cnn.html#cnn-examples"><i class="fa fa-check"></i><b>3.2</b> CNN Examples</a></li>
<li class="chapter" data-level="3.3" data-path="cnn.html"><a href="cnn.html#object-detection"><i class="fa fa-check"></i><b>3.3</b> Object Detection</a></li>
<li class="chapter" data-level="3.4" data-path="cnn.html"><a href="cnn.html#face-recognition"><i class="fa fa-check"></i><b>3.4</b> Face Recognition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rnn.html"><a href="rnn.html"><i class="fa fa-check"></i><b>4</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="4.1" data-path="rnn.html"><a href="rnn.html#basics-of-rnn"><i class="fa fa-check"></i><b>4.1</b> Basics of RNN</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="rnn.html"><a href="rnn.html#intuition-of-sequence-models"><i class="fa fa-check"></i><b>4.1.1</b> <strong>Intuition of Sequence Models</strong></a></li>
<li class="chapter" data-level="4.1.2" data-path="rnn.html"><a href="rnn.html#intuition-of-rnn"><i class="fa fa-check"></i><b>4.1.2</b> <strong>Intuition of RNN</strong></a></li>
<li class="chapter" data-level="4.1.3" data-path="rnn.html"><a href="rnn.html#rnn-types"><i class="fa fa-check"></i><b>4.1.3</b> <strong>RNN Types</strong></a></li>
<li class="chapter" data-level="4.1.4" data-path="rnn.html"><a href="rnn.html#language-model"><i class="fa fa-check"></i><b>4.1.4</b> <strong>Language Model</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="rnn.html"><a href="rnn.html#rnn-variations"><i class="fa fa-check"></i><b>4.2</b> RNN Variations</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="rnn.html"><a href="rnn.html#gru-gated-recurrent-unit"><i class="fa fa-check"></i><b>4.2.1</b> <strong>GRU</strong> (Gated Recurrent Unit)</a></li>
<li class="chapter" data-level="4.2.2" data-path="rnn.html"><a href="rnn.html#lstm-long-short-term-memory"><i class="fa fa-check"></i><b>4.2.2</b> <strong>LSTM</strong> (Long Short-Term Memory)</a></li>
<li class="chapter" data-level="4.2.3" data-path="rnn.html"><a href="rnn.html#bidirectional-rnn"><i class="fa fa-check"></i><b>4.2.3</b> <strong>Bidirectional RNN</strong></a></li>
<li class="chapter" data-level="4.2.4" data-path="rnn.html"><a href="rnn.html#deep-rnn"><i class="fa fa-check"></i><b>4.2.4</b> <strong>Deep RNN</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="rnn.html"><a href="rnn.html#word-embeddings"><i class="fa fa-check"></i><b>4.3</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="rnn.html"><a href="rnn.html#featurized-representation"><i class="fa fa-check"></i><b>4.3.1</b> <strong>Featurized Representation</strong></a></li>
<li class="chapter" data-level="4.3.2" data-path="rnn.html"><a href="rnn.html#learning-1-word2vec"><i class="fa fa-check"></i><b>4.3.2</b> Learning 1: <strong>Word2Vec</strong></a></li>
<li class="chapter" data-level="4.3.3" data-path="rnn.html"><a href="rnn.html#learning-2-negative-sampling"><i class="fa fa-check"></i><b>4.3.3</b> Learning 2: <strong>Negative Sampling</strong></a></li>
<li class="chapter" data-level="4.3.4" data-path="rnn.html"><a href="rnn.html#learning-3-glove-global-vectors"><i class="fa fa-check"></i><b>4.3.4</b> Learning 3: <strong>GloVe</strong> (Global Vectors)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rnn.html"><a href="rnn.html#sequence-modeling"><i class="fa fa-check"></i><b>4.4</b> Sequence Modeling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="rnn.html"><a href="rnn.html#sentiment-classification"><i class="fa fa-check"></i><b>4.4.1</b> <strong>Sentiment Classification</strong></a></li>
<li class="chapter" data-level="4.4.2" data-path="rnn.html"><a href="rnn.html#seq2seq"><i class="fa fa-check"></i><b>4.4.2</b> <strong>Seq2Seq</strong></a></li>
<li class="chapter" data-level="4.4.3" data-path="rnn.html"><a href="rnn.html#beam-search"><i class="fa fa-check"></i><b>4.4.3</b> <strong>Beam Search</strong></a></li>
<li class="chapter" data-level="4.4.4" data-path="rnn.html"><a href="rnn.html#bleu-score"><i class="fa fa-check"></i><b>4.4.4</b> <strong>Bleu Score</strong></a></li>
<li class="chapter" data-level="4.4.5" data-path="rnn.html"><a href="rnn.html#attention-model"><i class="fa fa-check"></i><b>4.4.5</b> <strong>Attention Model</strong></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basics" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Basics of Neural Networks</h1>
<div id="neural-network-representation" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Neural Network Representation</h2>
<center>
<img src="../../images/DL/NN.png" width="400"/>
</center>
<p><strong>Input Matrix</strong>:</p>
<p><span class="math display">\[\begin{equation}
X=\begin{bmatrix}
x_1^{(1)} &amp; \cdots &amp; x_1^{(m)} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{n_x}^{(1)} &amp; \cdots &amp; x_{n_x}^{(m)}
\end{bmatrix}=\begin{bmatrix}
x^{(1)} &amp; \cdots &amp; x^{(m)}
\end{bmatrix}\quad\quad\quad X\in\mathbb{R}^{n_x\times m}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(x_j^{(i)}\)</span>: the <span class="math inline">\(j\)</span>th feature of the <span class="math inline">\(i\)</span>th training example</li>
<li><span class="math inline">\(m\)</span>: # training examples: each column vector of <span class="math inline">\(x\)</span> represents one training example</li>
<li><span class="math inline">\(n_x\)</span>: # input features: each row vector of <span class="math inline">\(x\)</span> represents one type of input feature</li>
</ul>
<p>for easier understanding in this session, we use one training example / input vector at each training step:</p>
<p><span class="math display">\[\begin{equation}
x^{(i)}=\begin{bmatrix}
x_1^{(i)} \\ \vdots \\ x_{n_x}^{(i)}
\end{bmatrix}\quad\quad\quad x^{(i)}\in\mathbb{R}^{n_x}
\end{equation}\]</span></p>
<p><strong>Output Vector</strong>:</p>
<p><span class="math display">\[\begin{equation}
\hat{Y}=\begin{bmatrix}
\hat{y}^{(1)} &amp; \cdots &amp; \hat{y}^{(m)}
\end{bmatrix}\quad\quad\quad \hat{Y}\in\mathbb{R}^{m}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{y}^{(i)}\)</span>: the predicted output value of the <span class="math inline">\(i\)</span>th training example</li>
</ul>
<p>for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the “<span class="math inline">\(\hat{}\)</span>” symbol.</p>
<p><strong>Weight Matrix</strong>:</p>
<p><span class="math display">\[\begin{equation}
W^{[k]}=\begin{bmatrix}
w_{1,1}^{[k]} &amp; \cdots &amp; w_{1,n_{k-1}}^{[k]} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{n_k,1}^{[k]} &amp; \cdots &amp; w_{n_k,n_{k-1}}^{[k]}
\end{bmatrix}=\begin{bmatrix}
w_1^{[k]} \\ \vdots \\ w_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad W^{[k]}\in\mathbb{R}^{n_k\times n_{k-1}}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(w_{j,l}^{[k]}\)</span>: the weight value for the <span class="math inline">\(l\)</span>th input at the <span class="math inline">\(j\)</span>th node on the <span class="math inline">\(k\)</span>th layer</li>
<li><span class="math inline">\(n_k\)</span>: # nodes/neurons on the <span class="math inline">\(k\)</span>th layer (the current layer)</li>
<li><span class="math inline">\(n_{k-1}\)</span>: # nodes/neurons on the <span class="math inline">\(k-1\)</span>th layer (the previous layer)</li>
</ul>
<p><strong>Bias Vector</strong>:</p>
<p><span class="math display">\[\begin{equation}
b^{[k]}=\begin{bmatrix}
b_1^{[k]} \\ \vdots \\ b_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad b^{[k]}\in\mathbb{R}^{n_k}
\end{equation}\]</span></p>
<p><strong>Linear Combination</strong>:</p>
<p><span class="math display">\[\begin{equation}
z_j^{[k]}=w_j^{[k]}\cdot a^{[k-1]}+b_j^{[k]} \quad\quad\quad z_j^{[k]}\in\mathbb{R}^{n_k}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(z_j^{[k]}\)</span>: the unactivated output value from the <span class="math inline">\(j\)</span>th node of the <span class="math inline">\(k\)</span>th layer</li>
</ul>
<p><strong>Activation</strong>:</p>
<p><span class="math display">\[\begin{equation}
a^{[k]}=\begin{bmatrix}
a_1^{[k]} \\ \vdots \\ a_{n_k}^{[k]}
\end{bmatrix}=\begin{bmatrix}
g(z_1^{[k]}) \\ \vdots \\ g(z_{n_k}^{[k]})
\end{bmatrix}\quad\quad\quad a^{[k]}\in\mathbb{R}^{n_k}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(g(z)\)</span>: Activation function (to add <strong>nonlinearity</strong>)</li>
</ul>
</div>
<div id="activation-functions" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Activation Functions</h2>
(Blame github pages for not supporting colspan/rowspan)
<table>
<thead>
<tr style="text-align: center">
<th>
Sigmoid
</th>
<th>
Tanh
</th>
<th>
ReLU
</th>
<th>
Leaky ReLU
</th>
</tr>
</thead>
<tbody style="text-align: center">
<tr>
<td>
<span class="math inline">\(g(z)=\frac{1}{1+e^{-z}}\)</span>
</td>
<td>
<span class="math inline">\(g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\)</span>
</td>
<td>
<span class="math inline">\(g(z)=\max{(0,z)}\)</span>
</td>
<td>
<span class="math inline">\(g(z)=\max{(\varepsilon z,z)}\)</span>
</td>
</tr>
<tr>
<td>
<img src="../../images/DL/sigmoid.png" width="100"/>
</td>
<td>
<img src="../../images/DL/tanh.png" width="100"/>
</td>
<td>
<img src="../../images/DL/relu.png" width="100"/>
</td>
<td>
<img src="../../images/DL/leakyrelu.png" width="100"/>
</td>
</tr>
<tr>
<td>
<small><span class="math inline">\(g&#39;(z)=g(z)\cdot (1-g(z))\)</span></small>
</td>
<td>
<small><span class="math inline">\(g&#39;(z)=1-(g(z))^2\)</span></small>
</td>
<td>
<small><span class="math display">\[g&#39;(z)=\begin{cases} 0&amp;z&lt;0 \\ 1&amp;z&gt;0\end{cases}\]</span></small>
</td>
<td>
<small><span class="math display">\[g&#39;(z)=\begin{cases} \varepsilon&amp;z&lt;0 \\ 1&amp;z&gt;0\end{cases}\]</span></small>
</td>
</tr>
<tr>
<td>
<small>centered at <span class="math inline">\(y=0.5\)</span><br><span class="math inline">\(\Rightarrow\)</span>only good for binary classification</small>
</td>
<td>
<small>centered at <span class="math inline">\(y=0\)</span><br><span class="math inline">\(\Rightarrow\)</span>better than sigmoid in many cases</small>
</td>
<td>
<small>faster computing<br><strike>vanishing gradient</strike><br>model sparsity (some neurons can be inactivated)</small>
</td>
<td>
<small>faster computing<br><strike>vanishing gradient</strike><br>model sparsity (some neurons can be inactivated)</small>
</td>
</tr>
<tr>
<td>
<span class="math inline">\(|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0\)</span><br><span class="math inline">\(\Rightarrow\)</span> vanishing gradient
</td>
<td>
<span class="math inline">\(|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0\)</span><br><span class="math inline">\(\Rightarrow\)</span> vanishing gradient
</td>
<td>
too many neurons get inactivated<br><span class="math inline">\(\Rightarrow\)</span>dying ReLU
</td>
<td>
<span class="math inline">\(\varepsilon\)</span> usually set to 0.01<br><strike>dying ReLU</strike><br>widely used on Kaggle
</td>
</tr>
</tbody>
</table>
<ul>
<li>Why need activation funcs? To add nonlinearity.
<ol style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(g(z)=z\)</span> (i.e. <span class="math inline">\(\nexists g(z)\)</span>)</li>
<li><span class="math inline">\(\Longrightarrow z^{[1]}=w^{[1]}x+b^{[1]}\)</span></li>
<li><span class="math inline">\(\Longrightarrow z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})=w&#39;x+b&#39;\)</span></li>
<li>This is just linear regression. Hidden layers exist for no reason.</li>
</ol></li>
</ul>
</div>
<div id="training" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Training</h2>
<p><a name="fp"></a>
<strong>Forward Propagation</strong></p>
<center>
<img src="../../images/DL/fp.png" width="500"/>
</center>
<p><a name="bp"></a>
<strong>Backward Propagation</strong></p>
<center>
<img src="../../images/DL/bp.png" width="500"/>
</center>
<p><a name="fbss"></a>
<strong>Example: Forward &amp; Backward Step: Stochastic</strong>: 2 nodes &amp; 3 inputs &amp; no bias</p>
<ul>
<li>Forward Step:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}=\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix}
\end{equation}\]</span></p>
<ul>
<li>Backward Step:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1}}x_1 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_2 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_3 \\
\frac{\partial{\mathcal{L}}}{\partial{z_2}}x_1 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_2 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_3
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}x^T
\end{equation}\]</span></p>
<p><a name="fbsb"></a>
<strong>Example: Forward &amp; Backward Step: Mini-batch</strong>: 2 nodes &amp; 3 inputs &amp; bias &amp; 2 training examples</p>
<ul>
<li>Forward Step:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1^{(1)} &amp; x_1^{(2)} \\ 
x_2^{(1)} &amp; x_2^{(2)} \\ 
x_3^{(1)} &amp; x_3^{(2)}
\end{bmatrix}+\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}=\begin{bmatrix}
z_1^{(1)} &amp; z_1^{(2)} \\
z_2^{(1)} &amp; z_2^{(2)}
\end{bmatrix}
\end{equation}\]</span></p>
<ul>
<li>Backward Step:</li>
</ul>
<center>
<small><span class="math display">\[\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_1^{(2)} &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_2^{(2)} &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_3^{(2)} \\
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_1^{(2)} &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_2^{(2)} &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_3^{(2)} \\
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}X^T
\end{equation}\]</span></small>
</center>
<p><span class="math display">\[\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{b}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{b_1}} \\ \frac{\partial{\mathcal{L}}}{\partial{b_2}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}} \\ 
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}
\end{bmatrix}=\sum_{i=1}^{2}{\frac{\partial{\mathcal{L}}}{\partial{z^{(i)}}}}
\end{equation}\]</span></p>
<p><a name="rd"></a>
<strong>Reverse Differentiation</strong>: a simple procedure summarized for a clearer understanding of backprop from Node A to Node B:
1. Find one single path of “A<span class="math inline">\(\rightarrow\)</span>B”
2. Multiply all edge derivatives
3. Add the multiple to the overall derivative
4. Repeat 1-3</p>
<p>e.g.<br />
Path 1:</p>
<center>
<img src="../../images/DL/rd1.png" width="500"/>
</center>
<p>Path 2:</p>
<center>
<img src="../../images/DL/rd2.png" width="500"/>
</center>
<p>Path 3:</p>
<center>
<img src="../../images/DL/rd3.png" width="500"/>
</center>
And so on ……<br />

<center>
<strong><i>Reverse Differentiation <span class="math inline">\(\times\)</span> Backward Step = Backward Propagation</i></strong>
</center>
</div>
<div id="gradient-descent" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Gradient Descent</h2>
<p><span class="math display">\[\begin{equation}
W := W-\alpha\frac{\partial\mathcal{L}}{\partial W}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Stochastic GD</strong> (using 1 training example for each GD step)</p>
<p><span class="math display">\[\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}(\hat{Y_i}-Y_i)^2 \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align}\]</span></p></li>
<li><p><strong>Mini-batch GD</strong> (using mini-batches of size <span class="math inline">\(m&#39;\ (\text{s.t.}\ m=km&#39;, k\in Z)\)</span> for each GD step)</p>
<p><span class="math display">\[\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}\sum_{i=1}^{m&#39;}{(\hat{Y_i}-Y_i)^2} \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align}\]</span></p></li>
<li><p><strong>Batch GD</strong> (using the whole training set for each GD step)</p>
<p><span class="math display">\[\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}\sum_{i=1}^{m}{(\hat{Y_i}-Y_i)^2} \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align}\]</span></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="imp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jules32/bookdown-tutorial/edit/master/01_basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["series.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
