[["index.html", "Deep Learning 1 Deep Learning", " Deep Learning Renyi Qu 2021/01/24 1 Deep Learning References: Andrew Ng. Deep Learning Specialization. Coursera Inc. Goodfellow, I. Bengio, Y. Courville, A. Deep Learning. MIT Press. "],["basics.html", "2 Basics of Neural Networks 2.1 Neural Network Representation 2.2 Activation Functions 2.3 Training 2.4 Gradient Descent", " 2 Basics of Neural Networks 2.1 Neural Network Representation Input Matrix: \\[\\begin{equation} X=\\begin{bmatrix} x_1^{(1)} &amp; \\cdots &amp; x_1^{(m)} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n_x}^{(1)} &amp; \\cdots &amp; x_{n_x}^{(m)} \\end{bmatrix}=\\begin{bmatrix} x^{(1)} &amp; \\cdots &amp; x^{(m)} \\end{bmatrix}\\quad\\quad\\quad X\\in\\mathbb{R}^{n_x\\times m} \\end{equation}\\] \\(x_j^{(i)}\\): the \\(j\\)th feature of the \\(i\\)th training example \\(m\\): # training examples: each column vector of \\(x\\) represents one training example \\(n_x\\): # input features: each row vector of \\(x\\) represents one type of input feature for easier understanding in this session, we use one training example / input vector at each training step: \\[\\begin{equation} x^{(i)}=\\begin{bmatrix} x_1^{(i)} \\\\ \\vdots \\\\ x_{n_x}^{(i)} \\end{bmatrix}\\quad\\quad\\quad x^{(i)}\\in\\mathbb{R}^{n_x} \\end{equation}\\] Output Vector: \\[\\begin{equation} \\hat{Y}=\\begin{bmatrix} \\hat{y}^{(1)} &amp; \\cdots &amp; \\hat{y}^{(m)} \\end{bmatrix}\\quad\\quad\\quad \\hat{Y}\\in\\mathbb{R}^{m} \\end{equation}\\] \\(\\hat{y}^{(i)}\\): the predicted output value of the \\(i\\)th training example for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the \\(\\hat{}\\) symbol. Weight Matrix: \\[\\begin{equation} W^{[k]}=\\begin{bmatrix} w_{1,1}^{[k]} &amp; \\cdots &amp; w_{1,n_{k-1}}^{[k]} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{n_k,1}^{[k]} &amp; \\cdots &amp; w_{n_k,n_{k-1}}^{[k]} \\end{bmatrix}=\\begin{bmatrix} w_1^{[k]} \\\\ \\vdots \\\\ w_{n_k}^{[k]} \\end{bmatrix}\\quad\\quad\\quad W^{[k]}\\in\\mathbb{R}^{n_k\\times n_{k-1}} \\end{equation}\\] \\(w_{j,l}^{\\[k\\]}\\): the weight value for the \\(l\\)th input at the \\(j\\)th node on the \\(k\\)th layer \\(n_k\\): # nodes/neurons on the \\(k\\)th layer (the current layer) \\(n_{k-1}\\): # nodes/neurons on the \\(k-1\\)th layer (the previous layer) Bias Vector: \\[\\begin{equation} b^{[k]}=\\begin{bmatrix} b_1^{[k]} \\\\ \\vdots \\\\ b_{n_k}^{[k]} \\end{bmatrix}\\quad\\quad\\quad b^{[k]}\\in\\mathbb{R}^{n_k} \\end{equation}\\] Linear Combination: \\[\\begin{equation} z_j^{[k]}=w_j^{[k]}\\cdot a^{[k-1]}+b_j^{[k]} \\quad\\quad\\quad z_j^{[k]}\\in\\mathbb{R}^{n_k} \\end{equation}\\] \\(z_j^{\\[k\\]}\\): the unactivated output value from the \\(j\\)th node of the \\(k\\)th layer Activation: \\[\\begin{equation} a^{[k]}=\\begin{bmatrix} a_1^{[k]} \\\\ \\vdots \\\\ a_{n_k}^{[k]} \\end{bmatrix}=\\begin{bmatrix} g(z_1^{[k]}) \\\\ \\vdots \\\\ g(z_{n_k}^{[k]}) \\end{bmatrix}\\quad\\quad\\quad a^{[k]}\\in\\mathbb{R}^{n_k} \\end{equation}\\] \\(g(z)\\): Activation function (to add nonlinearity) 2.2 Activation Functions (Blame github pages for not supporting colspan/rowspan) Sigmoid Tanh ReLU Leaky ReLU \\(g(z)=\\frac{1}{1+e^{-z}}\\) \\(g(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\) \\(g(z)=\\max{(0,z)}\\) \\(g(z)=\\max{(\\varepsilon z,z)}\\) \\(g&#39;(z)=g(z)\\cdot (1-g(z))\\) \\(g&#39;(z)=1-(g(z))^2\\) \\[g&#39;(z)=\\begin{cases} 0&amp;z&lt;0 \\\\ 1&amp;z&gt;0\\end{cases}\\] \\[g&#39;(z)=\\begin{cases} \\varepsilon&amp;z&lt;0 \\\\ 1&amp;z&gt;0\\end{cases}\\] centered at \\(y=0.5\\)\\(\\Rightarrow\\)only good for binary classification centered at \\(y=0\\)\\(\\Rightarrow\\)better than sigmoid in many cases faster computingvanishing gradientmodel sparsity (some neurons can be inactivated) faster computingvanishing gradientmodel sparsity (some neurons can be inactivated) \\(|z|\\uparrow\\uparrow \\rightarrow\\frac{da}{dz}\\approx 0\\)\\(\\Rightarrow\\) vanishing gradient \\(|z|\\uparrow\\uparrow \\rightarrow\\frac{da}{dz}\\approx 0\\)\\(\\Rightarrow\\) vanishing gradient too many neurons get inactivated\\(\\Rightarrow\\)dying ReLU \\(\\varepsilon\\) usually set to 0.01dying ReLUwidely used on Kaggle Why need activation funcs? To add nonlinearity. Suppose \\(g(z)=z\\) (i.e. \\(\\nexists g(z)\\)) \\(\\Longrightarrow z^{[1]}=w^{[1]}x+b^{[1]}\\) \\(\\Longrightarrow z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})=w&#39;x+b&#39;\\) This is just linear regression. Hidden layers exist for no reason. 2.3 Training Forward Propagation Backward Propagation Example: Forward &amp; Backward Step: Stochastic: 2 nodes &amp; 3 inputs &amp; no bias Forward Step: \\[\\begin{equation} \\begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\ w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}=\\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} \\end{equation}\\] Backward Step: \\[\\begin{equation} \\frac{\\partial{\\mathcal{L}}}{\\partial{W}}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,1}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,2}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,3}}} \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,1}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,2}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,3}}} \\end{bmatrix}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1}}x_1 &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1}}x_2 &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1}}x_3 \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2}}x_1 &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2}}x_2 &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2}}x_3 \\end{bmatrix}=\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}}x^T \\end{equation}\\] Example: Forward &amp; Backward Step: Mini-batch: 2 nodes &amp; 3 inputs &amp; bias &amp; 2 training examples Forward Step: \\[\\begin{equation} \\begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\ w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\end{bmatrix}\\begin{bmatrix} x_1^{(1)} &amp; x_1^{(2)} \\\\ x_2^{(1)} &amp; x_2^{(2)} \\\\ x_3^{(1)} &amp; x_3^{(2)} \\end{bmatrix}+\\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}=\\begin{bmatrix} z_1^{(1)} &amp; z_1^{(2)} \\\\ z_2^{(1)} &amp; z_2^{(2)} \\end{bmatrix} \\end{equation}\\] Backward Step: \\[\\begin{equation} \\frac{\\partial{\\mathcal{L}}}{\\partial{W}}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,1}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,2}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{1,3}}} \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,1}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,2}}} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{w_{2,3}}} \\end{bmatrix}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(1)}}}x_1^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(2)}}}x_1^{(2)} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(1)}}}x_2^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(2)}}}x_2^{(2)} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(1)}}}x_3^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(2)}}}x_3^{(2)} \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(1)}}}x_1^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(2)}}}x_1^{(2)} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(1)}}}x_2^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(2)}}}x_2^{(2)} &amp; \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(1)}}}x_3^{(1)}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(2)}}}x_3^{(2)} \\\\ \\end{bmatrix}=\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}}X^T \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial{\\mathcal{L}}}{\\partial{b}}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{b_1}} \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{b_2}} \\end{bmatrix}=\\begin{bmatrix} \\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(1)}}}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_1^{(2)}}} \\\\ \\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(1)}}}+\\frac{\\partial{\\mathcal{L}}}{\\partial{z_2^{(2)}}} \\end{bmatrix}=\\sum_{i=1}^{2}{\\frac{\\partial{\\mathcal{L}}}{\\partial{z^{(i)}}}} \\end{equation}\\] Reverse Differentiation: a simple procedure summarized for a clearer understanding of backprop from Node A to Node B: 1. Find one single path of A\\(\\rightarrow\\)B 2. Multiply all edge derivatives 3. Add the multiple to the overall derivative 4. Repeat 1-3 e.g. Path 1: Path 2: Path 3: And so on  Reverse Differentiation \\(\\times\\) Backward Step = Backward Propagation 2.4 Gradient Descent \\[\\begin{equation} W := W-\\alpha\\frac{\\partial\\mathcal{L}}{\\partial W} \\end{equation}\\] Stochastic GD (using 1 training example for each GD step) \\[\\begin{align} \\mathcal{L}(\\hat{Y},Y)&amp;=\\frac{1}{2}(\\hat{Y_i}-Y_i)^2 \\\\ W&amp;=W-\\alpha\\frac{\\partial\\mathcal{L}}{\\partial W} \\end{align}\\] Mini-batch GD (using mini-batches of size \\(m&#39;\\ (\\text{s.t.}\\ m=km&#39;, k\\in Z)\\) for each GD step) \\[\\begin{align} \\mathcal{L}(\\hat{Y},Y)&amp;=\\frac{1}{2}\\sum_{i=1}^{m&#39;}{(\\hat{Y_i}-Y_i)^2} \\\\ W&amp;=W-\\alpha\\frac{\\partial\\mathcal{L}}{\\partial W} \\end{align}\\] Batch GD (using the whole training set for each GD step) \\[\\begin{align} \\mathcal{L}(\\hat{Y},Y)&amp;=\\frac{1}{2}\\sum_{i=1}^{m}{(\\hat{Y_i}-Y_i)^2} \\\\ W&amp;=W-\\alpha\\frac{\\partial\\mathcal{L}}{\\partial W} \\end{align}\\] "],["imp.html", "3 Improvements on Neural Networks 3.1 Train/Test Split 3.2 Initialization 3.3 Data Fitting 3.4 Regularization 3.5 Optimization 3.6 Hyperparameter Tuning 3.7 Batch Normalization", " 3 Improvements on Neural Networks 3.1 Train/Test Split Dataset = training set + development/validation set + test set Split ratio: old era: 70/0/30%, 60/20/20%,  big data era: 98/1/1%, 99.5/0.4/0.1%, 99.5/0.5/0%,  \\ (trend: testset as small as possible) All 3 subsets should come from the exact same distribution (mismatch) 3.2 Initialization \\(W\\) should be initialized with small random values to break symmetry (to make sure that different hidden nodes can learn different things) \\(b\\) can be initialized to zeros (\\(\\because\\) symmetry is still broken when \\(W\\) is randomly initialized) Different initializations \\(\\rightarrow\\) different results Refer to keras documentation for initializers. 3.3 Data Fitting Underfitting: Proper fitting: Overfitting: Tradeoff: train error vs validation error: - train err too small \\(\\longrightarrow\\) high variance (overfitting) (e.g. train err = 1%; val err = 11%) - train err too big \\(\\longrightarrow\\) high bias (underfitting) (e.g. train err = 17%; val err = 16%) - train err too big &amp; val err even bigger \\(\\longrightarrow\\) both probs (e.g. train err = 17%; val err = 34%) - train err too small &amp; val err also small \\(\\longrightarrow\\) congratulations! (e.g. train err = 0.5%; val err = 1%) The Procedure: 3.4 Regularization Idea: add a regularization term to the original loss function: \\[\\begin{equation} \\mathcal{J}(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}{\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})}+\\frac{\\lambda}{2m}f(w) \\end{equation}\\] \\(\\lambda\\): regularization parameter \\(f(w)\\): regularization on \\(w\\) How does regularization prevent overfitting? set \\(\\lambda\\) as big as possible \\(\\Rightarrow w^{[l]}\\approx 0\\) \\(\\Rightarrow z^{[l]}\\approx 0\\) \\(\\Rightarrow\\) as if some hidden nodes dont exist any more \\(\\Rightarrow\\) less complexity \\(\\Rightarrow\\) variance \\(\\downarrow\\) Regularization on LogReg: - L2 Regularization: \\[\\begin{equation} \\mathcal{J}(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}{\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})}+\\frac{\\lambda}{2m}\\|w\\|^2_2 \\\\ \\|w\\|^2_2=\\sum_{j=1}^{n_x}w_j^2=w^Tw \\end{equation}\\] L1 Regularization: \\[\\begin{equation} \\mathcal{J}(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}{\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})}+\\frac{\\lambda}{2m}\\|w\\|_1 \\\\ \\|w\\|_1=\\sum_{j=1}^{n_x}{|w|} \\end{equation}\\] Regularization on NN: \\[\\begin{equation} \\mathcal{J}(W^{[k]},b^{[k]})=\\frac{1}{m}\\sum_{i=1}^{m}{\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})}+\\frac{\\lambda}{2m}\\sum_{l=1}^L{\\|W^{[l]}\\|^2_F} \\end{equation}\\] Frobenius Norm: \\[\\begin{equation} \\|W^{[l]}\\|^2_F=\\sum_{i=1}^{n_{l-1}}\\sum_{j=1}^{n_l}{(w_{ij}^{[l]})^2} \\end{equation}\\] Weight Decay on GD: \\[\\begin{align} W^{[l]}&amp;:=w^{[l]}-\\alpha\\cdot\\frac{\\partial{\\mathcal{L}}}{\\partial{W^{[l]}}} \\\\ &amp;=w^{[l]}-\\alpha\\cdot\\Big(\\frac{\\partial{\\mathcal{L}}}{\\partial{W^{[l]}}}(\\text{original})+\\frac{\\lambda}{m}W^{[l]}\\Big) \\end{align}\\] Dropout: each node has a probability to be kicked out of the NN (\\(\\Rightarrow\\) NN becomes smaller &amp; simpler) \\[only used in training\\] Make a Boolean matrix corresponding to the matrix of activation values: \\[\\begin{align} A^{[k]}&amp;=\\begin{bmatrix} a_{11}^{[k]} &amp; \\cdots &amp; a_{1m}^{[k]} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n_k1}^{[k]} &amp; \\cdots &amp; a_{n_km}^{[k]} \\end{bmatrix}\\quad\\quad\\quad A^{[k]}\\in\\mathbb{R}^{n_k\\times m} \\\\ \\\\ B^{[k]}&amp;=\\begin{bmatrix} b_{11}^{[k]} &amp; \\cdots &amp; b_{1m}^{[k]} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n_k1}^{[k]} &amp; \\cdots &amp; b_{n_km}^{[k]} \\end{bmatrix}\\quad\\quad\\quad B^{[k]}\\in\\mathbb{R}^{n_k\\times m} \\end{align}\\] where \\(b_{ji}^{[k]}\\in\\\\{\\text{True}, \\text{False}\\\\}\\). The Boolean values are assigned randomly based on a keep-probability \\(p\\) (can be chosen differently for diff layers). Multiply both matrices element-wise: \\[\\begin{equation} A^{[k]}=A^{[k]}* B^{[k]} \\end{equation}\\] so that some activation values are now zero (they are kicked out of the neural network) Invert the matrix element-wise: \\[\\begin{equation} A^{[k]}=A^{[k]}/p \\end{equation}\\] to ensure consistency in activation values Data Augmentation: modify the dataset to get more data (mostly used in Computer Vision) \\[Benefit: a very low-cost regularization\\] Examples: - flip picture - slight rotation - zoom in/out - distortions -  Early Stopping: stop the training iterations in the middle Why do we stop in the middle? The goal of our training is NOT to finish training BUT to find the optimal weight parameters that minimizes the cost/error. As shown in the figure, sometimes we should just stop in the middle with the minimal validation error instead of keeping the training going to get overfitting. Orthogonalization: implement controls that only affect ONE single component of your algorithms performance at a time Feature Scaling (normalization): normalize inputs for higher efficiency Set to zero mean: \\[\\begin{align} \\mu&amp;=\\frac{1}{m}\\sum_{i=1}^{m}{x^{(i)}} \\\\ x&amp;=x-\\mu \\end{align}\\] Normalize variance: \\[\\begin{align} \\sigma^2&amp;=\\frac{1}{m}\\sum_{i=1}^{m}{x^{(i)}\\text{**}2}\\quad\\quad \\text{**: element-wise squaring} \\\\ x&amp;=x/\\sigma^2 \\end{align}\\] Gradient Checking Why? Backprop is a very complex system of mathematical computations. It is very possible that there might be some miscalculation or bugs in these tremendous differentiations, even though the entire training appears as if its working properly. Gradient Checking is the approach to prevent such issue by checking if each gradient is calculated properly. Equation \\[\\begin{equation} \\frac{\\partial{\\mathcal{J}}}{\\partial{w}}=\\lim_{\\varepsilon\\rightarrow 0}\\frac{\\mathcal{J}(w+\\varepsilon)-\\mathcal{J}(w-\\varepsilon)}{2\\varepsilon}\\approx\\frac{\\mathcal{J}(w+\\varepsilon)-\\mathcal{J}(w-\\varepsilon)}{2\\varepsilon} \\end{equation}\\] Implementation: Calculate the difference between actual gradient and approximated gradient to see if the difference is reasonable: \\[\\begin{equation} \\text{diff}=\\frac{||g-g&#39;||_ 2}{||g||_ 2+||g&#39;||_ 2} \\end{equation}\\] 3.5 Optimization Mini-Batch Gradient Descent Why? To allow faster and more efficient computing when there is a large number of training examples (e.g. \\(m=10000000\\)) Implementation (see gradient descent for more details) \\[\\begin{align} \\mathcal{L}(\\hat{Y},Y)&amp;=\\frac{1}{2}\\sum_{i=1}^{m&#39;}{(\\hat{Y_i}-Y_i)^2} \\\\ W&amp;=W-\\alpha\\frac{\\partial\\mathcal{L}}{\\partial W} \\end{align}\\] Performance BGD vs MBGD BGD vs SGD BGD: large steps, low noise, too long per iteration SGD: small steps, insane noise, lose vectorization MBGD: in between \\(\\rightarrow\\) optimal in most cases Gradient Descent with Momentum Exponentially Weighted (Moving) Average Intuition The blue dots represent the raw data points, while the red and green curves represent the two EMAs of the blue dots. As clearly indicated by the figure, EMA is used to reduce the huge oscillation of such time-series data. Formula \\[\\begin{equation} V_t=\\beta V_{t-1}+(1-\\beta)\\theta_t \\end{equation}\\] \\(\\theta_t\\): the original time-series data point at time \\(t\\) \\(V_t\\): the EMA data point at time \\(t\\) \\(\\beta\\): an indicator of how many time units (e.g. days) this algorithm is approximately averaging over: \\[\\begin{equation} \\text{#time units}=\\frac{1}{1-\\beta} \\end{equation}\\] e.g. \\(\\beta=0.9 \\rightarrow\\) average over 10 days; \\(\\beta=0.96 \\rightarrow\\) average over 25 days Performance: easy computation + one-line code + memory efficiency Bias Correction Assume \\(\\beta=0.99\\): \\[\\begin{align} &amp;V_0=0 \\\\ &amp;V_1=0.99 V_0+0.01\\theta_1=0.01\\theta_1 \\\\ &amp;V_2=0.99 V_2+0.01\\theta_2=0.099\\theta_1+0.01\\theta_2 \\\\ &amp;... \\end{align}\\] Notice that \\(V_1 \\&amp; V_2\\) are very tiny portions of \\(\\theta_1 \\&amp; \\theta_2\\), meaning that they do not accurately represent the actual data points. Thus, it is necessary to rescale the early EMA values, with the following formula: \\[\\begin{equation} V_t:=\\frac{V_t}{1-\\beta^t} \\end{equation}\\] In the later calculations, bias correction is not so necessary. Momentum: application of EMA in GD Compute \\(dW,db\\) on the current MB Compute EMA \\[\\begin{align} &amp;V_{dW}:=\\beta V_{dW}+(1-\\beta)dW \\\\ &amp;V_{db}:=\\beta V_{db}+(1-\\beta)db \\end{align}\\] Compute GD \\[\\begin{align} &amp;W:=W-\\alpha V_{dW} \\\\ &amp;b:=b-\\alpha V_{db} \\end{align}\\] \\(\\beta\\) is often chosen as \\(0.9\\) in GD with Momentum. Why named momentum? Think of \\(dW\\) as acceleration, \\(V_{dW}\\) as velocity, and \\(\\beta\\) as friction. Performance Red steps represent Momentum, while blue steps represent normal GD. Slower learning vertically + Faster learning horizontally \\(\\rightarrow\\) Momentum is always better than SGD RMSprop (Root Mean Square Propagation) Intuition: a modified version of GD with Momentum Why? To further minimize the oscillation of GD and maximize the speed of convergence. Steps: Compute \\(dW,db\\) on the current MB Compute RMS step \\[\\begin{align} &amp;S_{dW}:=\\beta S_{dW}+(1-\\beta)dW^2 \\\\ &amp;S_{db}:=\\beta S_{db}+(1-\\beta)db^2 \\end{align}\\] where \\(dW^2=dW* dW\\) Compute GD \\[\\begin{align} &amp;W:=W-\\alpha \\frac{dW}{\\sqrt{S_{dW}}+\\varepsilon} \\\\ &amp;b:=b-\\alpha \\frac{db}{\\sqrt{S_{db}}+\\varepsilon} \\end{align}\\] \\(\\varepsilon\\) is added to ensure \\(\\text{denominator}\\neq0\\) (normally \\(\\varepsilon=10^{-8}\\)) Adam Intuition: Momentum + RMSprop Steps: Compute \\(dW,db\\) on the current MB Compute Momentum: \\[\\begin{align} &amp;V_{dW}:=\\beta_1 V_{dW}+(1-\\beta_1)dW \\\\ &amp;V_{db}:=\\beta_1 V_{db}+(1-\\beta_1)db \\end{align}\\] Compute RMSprop: \\[\\begin{align} &amp;S_{dW}:=\\beta_2 S_{dW}+(1-\\beta_2)dW^2 \\\\ &amp;S_{db}:=\\beta_2 S_{db}+(1-\\beta_2)db^2 \\end{align}\\] Bias Correction: \\[\\begin{align} &amp;V_{dW}:=\\frac{V_{dW}}{1-\\beta_1^t}, V_{db}:=\\frac{V_{db}}{1-\\beta_1^t} \\\\ &amp;S_{dW}:=\\frac{S_{dW}}{1-\\beta_2^t}, S_{db}:=\\frac{S_{db}}{1-\\beta_2^t} \\end{align}\\] Compute GD: \\[\\begin{align} &amp;W:=W-\\alpha \\frac{V_{dW}}{\\sqrt{S_{dW}}+\\varepsilon} \\\\ &amp;b:=b-\\alpha \\frac{V_{db}}{\\sqrt{S_{db}}+\\varepsilon} \\end{align}\\] Hyperparameter choices: \\(\\alpha\\): depends \\(\\beta_1: 0.9\\) \\(\\beta_2: 0.999\\) \\(\\varepsilon: 10^{-8}\\) Learning Rate Decay Intuition: as \\(\\alpha\\) slowly decreases, training steps become smaller \\(\\rightarrow\\) oscillating closely around the minimum (instead of jumping over the minimum) Main Method: \\[\\begin{equation} \\alpha=\\frac{1}{1+r_{\\text{decay}}\\cdot \\text{#epoch}}\\cdot\\alpha_0 \\end{equation}\\] where 1 epoch means passing through data once. Normally, \\(\\alpha_0=0.2,r_{\\text{decay}}=1\\) Other Methods: Exponential Decay: \\[\\begin{equation} \\alpha=0.95^{\\text{#epoch}}\\cdot\\alpha_0 \\end{equation}\\] Root Decay: \\[\\begin{equation} \\alpha=\\frac{k}{\\sqrt{\\text{#epoch}}}\\cdot\\alpha_0 \\end{equation}\\] Discrete Staircase: Manual Decay Problems with optimization As learnt in Calculus, no matter how we try to find the optimum, we always have problems: Local Optima: we get stuck in local optima instead of moving to global optima Saddle Points: we find GD=0 at saddle points before we find global optima Plateau: long saddle that makes learning super slow 3.6 Hyperparameter Tuning Intuition: try to find the optimal hyperparameter for the NN List of Hyperparameters (in the order of priority) - Tier 1: \\(\\alpha\\) - Tier 2: #hidden units, MB size - Tier 3: #layers, \\(\\alpha\\) decay - Tier 4: \\(\\beta_1\\), \\(\\beta_2\\), \\(\\varepsilon\\) Random Picking: e.g. \\(n^{[l]}\\in \\[50,100\\], L\\in \\[2,4\\]\\) Appropriate Scale: e.g. \\(\\alpha\\in \\[0.0001,1\\]\\) is obviously NOT an appropriate scale, because 90% of the values are in \\(\\[0.1,1\\]\\). Instead, \\(\\alpha\\in\\[0.0001,1\\]_ {\\text{log}}\\) is an appropriate scales because the random picking is equally distributed on the log scale. e.g. for \\(\\beta\\in\\[0.9,0.999\\]\\), the code implementation should be - \\(r\\in\\[-3,-1\\]\\) - \\(\\beta=1-10^r\\) 3.7 Batch Normalization Intuition: Feature scaling normalizes the inputs to speed up learning for the 1st layer. Similarly, can we normalize \\(a^{\\[l-1\\]}\\) to train \\(W^{\\[l\\]} \\&amp; b^{\\[l\\]}\\) faster? Obviously. Implementation: Calculate mean &amp; variance \\[\\begin{align} \\mu&amp;=\\frac{1}{m}\\sum_{i=1}^{m}{z^{[l](i)}} \\\\ \\sigma^2&amp;=\\frac{1}{m}\\sum_{i=1}^{m}{(z^{[l](i)}-\\mu)^2} \\end{align}\\] Normalize Node Output: \\[\\begin{equation} z_{\\text{norm}}^{[l](i)}=\\gamma\\frac{z^{[l](i)}-\\mu}{\\sqrt{\\sigma^2+\\varepsilon}}+\\beta \\end{equation}\\] \\(\\gamma\\ \\&amp;\\ \\beta\\) = learnable parameters \\(\\gamma\\neq\\sqrt{\\sigma^2+\\varepsilon}\\) and \\(\\beta\\neq\\mu\\) Make sure to add \\(\\gamma\\ \\&amp;\\ \\beta\\) to the dictionary of parameter updates during coding Batch Normalization eliminates \\(b^{[l]}\\) during \\(\\mu\\) calculation "],["cnn.html", "4 Convolutional Neural Networks 4.1 Basics of CNN 4.2 CNN Examples 4.3 Object Detection 4.4 Face Recognition", " 4 Convolutional Neural Networks 4.1 Basics of CNN Intuition of CNN CNN is mostly used in Computer Vision (image classification, object detection, neural style transfer, etc.) Input: images \\(\\rightarrow\\) volume of numerical values in the shape of width \\(\\times\\) height \\(\\times\\) color-scale (color-scale=3 \\(\\rightarrow\\) RGB; color-scale=1 \\(\\rightarrow\\) BW) In the gif above, the input shape is \\(5\\times5\\times3\\), meaning that the image is colored and the image size \\(5\\times5\\). The \\(7\\times7\\times3\\) results from padding, which will be discussed below. Convolution: For each color layer of the input image, we apply a 2d filter that scans through the layer in order. For each block that the filter scans, we multiply the corresponding filter value and the cell value, and we sum them up. We sum up the output values from all layers of the filter (and add a bias value to it) and output this value to the corresponding output cell. (If there are multiple filters, ) After the first filter finishes scanning, the next filter starts scanning and outputs into a new layer. In the gif above, Apply 2 filters of the shape \\(3\\times3\\times3\\). 1st filter - 1st layer - 1st block: \\[\\begin{equation} 0+0+0+0+0+0+0+(1\\times-1)+0=-1 \\end{equation}\\] 1st filter - 2nd layer - 1st block: \\[\\begin{equation} 0+0+0+0+(2\\times-1)+(1\\times1)+0+(2\\times1)+0=1 \\end{equation}\\] 1st filter - 3rd layer - 1st block: \\[\\begin{equation} 0+0+0+0+(2\\times1)+0+0+(1\\times-1)+0=1 \\end{equation}\\] Sum up + bias \\(\\rightarrow\\) 1st cell of 1st output layer \\[\\begin{equation} -1+1+1+1=2 \\end{equation}\\] Repeat till we finish scanning Edge Detection &amp; Filter Sample filters Gray Scale: 1 = lighter, 0 = gray, -1 = darker Notice that we dont really need to define any filter values. Instead, we are supposed to train the filter values. All the convolution operations above are just the same as the operations in ANN. Filters here correspond to \\(W\\) in ANN. Padding Problem: corner cells &amp; edge cells are detected much fewer times than the middle cells \\(\\rightarrow\\) info loss of corner &amp; edge Solution: pad the edges of the image with 0 cells (as shown in the gif above) Stride: the step size the filter takes (\\(s=2\\) in the gif above) General Formula of Convolution: \\[\\begin{equation} \\text{Output Size}=\\left\\lfloor\\frac{n+2p-f}{s}+1\\right\\rfloor\\times\\left\\lfloor\\frac{n+2p-f}{s}+1\\right\\rfloor \\end{equation}\\] \\(n\\times n\\): image size \\(f\\times f\\): filter size \\(p\\): padding \\(s\\): stride Floor: ignore the computation when the filter sweeps the region outside the image matrix CNN Layers: Convolution (CONV): as described above Pooling (POOL): to reduce #params &amp; computations (most common pooling size = \\(2\\times2\\)) Max Pooling Divide the matrix evenly into regions Take the max value in that region as output value Average Pooling Divide the matrix evenly into regions Take the average value of the cells in that region as output value Stochastic Pooling Divide the matrix evenly into regions Normalize each cell based on the regional sum: \\[\\begin{equation} p_i=\\frac{a_i}{\\sum_{k\\in R_j}{a_k}} \\end{equation}\\] Take a random cell based on multinomial distribution as output value Fully Connected (FC): to flatten the 2D/3D matrices into a single vector (each neuron is connected with all input values) 4.2 CNN Examples LeNet-5: LeNet-5 Digit Recognizer Layer Shape Total Size #params INPUT 32 x 32 x 3 3072 0 CONV1 (Layer 1) 28 x 28 x 6 4704 156 POOL1 (Layer 1) 14 x 14 x 6 1176 0 CONV2 (Layer 2) 10 x 10 x 16 1600 416 POOL2 (Layer 2) 5 x 5 x 16 400 0 FC3 (Layer 3) 120 x 1 120 48001 FC4 (Layer 4) 84 x 1 84 10081 Softmax 10 x 1 10 841 Calculation of #params for CONV: \\((f\\times f+1)\\times n_f\\) \\(f\\): filter size \\(+1\\): bias \\(n_f\\): #filter AlexNet: winner of 2012 ImageNet Large Scale Visual Recognition Challenge Layer Shape Total Size #params INPUT 227 x 227 x 3 154587 0 CONV1 (Layer 1) 55 x 55 x 96 290400 11712 POOL1 (Layer 1) 27 x 27 x 96 69984 0 CONV2 (Layer 2) 27 x 27 x 256 186624 6656 POOL2 (Layer 2) 13 x 13 x 256 43264 0 CONV3 (Layer 3) 13 x 13 x 384 64896 3840 CONV4 (Layer 3) 13 x 13 x 384 64896 3840 CONV5 (Layer 3) 13 x 13 x 256 43264 2560 POOL5 (Layer 3) 6 x 6 x 256 9216 0 FC5 (Flatten) 9216 x 1 9216 0 FC6 (Layer 4) 4096 x 1 4096 37748737 FC7 (Layer 5) 4096 x 1 4096 16777217 Softmax 1000 x 1 1000 4096000 Significantly bigger than LeNet-5 (60M params to be trained) Require multiple GPUs to speed the training up VGG: made by Visual Geometry Group from Oxford Too large: 138M params Inception ResNets Residual Block \\[\\begin{equation} a^{[l+2]}=g(z^{[l+2]}+a^{[l]}) \\end{equation}\\] Intuition: we add activation values from layer \\(l\\) to the activation in layer \\(l+2\\) Why ResNets? ResNets allow parametrization for the identity function \\(f(x)=x\\) ResNets are proven to be more effective than plain networks: ResNets add more complexity to the NN in a very simple way The idea of ResNets further inspired the development of RNN 1x1 Conv (i.e. Network in Network [NiN]) WHY??? This sounds like the stupidest idea ever!! Watch this. In a normal CNN layer like this, we need to do in total 210M calculations. However, if we add a 1x1 Conv layer in between, we only need to do in total 17M calculations. Therefore, 1x1 Conv is significantly more useful than what newbies expect. When we would like to keep the matrix size but reduce #layers, using 1x1 Conv can significantly reduce #computations needed, thus requiring less computing power. The Inception: We need to go deeper! Inception Module Inception Network Conv1D &amp; Conv3D: Although CNN (Conv2D) is undoubtedly most useful in Computer Vision, there are also some other forms of CNN used in other fields: Conv1D: e.g. text classification, heartbeat detection,  use a 1D filter to convolve a 1D input vector e.g. \\(14\\times1\\xrightarrow{5\\times1,16}10\\times16\\xrightarrow{5\\times16,32}6\\times32\\) However, this is almost never used since we have RNN Conv3D: e.g. CT scan,  use a 3D filter to convolve a 3D input cube e.g. \\(14\\times14\\times14\\times1\\xrightarrow{5\\times5\\times5\\times1,16}10\\times10\\times10\\times16\\xrightarrow{5\\times5\\times5\\times16,32}6\\times6\\times6\\times32\\) 4.3 Object Detection Object Localization \\(\\rightarrow\\) 1 obj; Detection \\(\\rightarrow\\) multiple objs. Bounding Box: to capture the obj in the img with a box Params: \\(b_x, b_y\\) = central point \\(b_h, b_w\\) = full height/width New target label (in place of image classification output): \\[\\begin{equation} y=\\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{equation}\\] \\(p_c\\): is there any object in this box? if \\(p_c=0\\), we ignore the remaining params \\(c_i\\): class label \\(i\\) (e.g. \\(c_1\\): cat, \\(c_2\\): dog, \\(c_3\\): bird, ) Landmark Detection: to capture the obj in the img with points Params: \\((l_{ix},l_{iy})\\) = each landmark point New target label: \\[\\begin{equation} y=\\begin{bmatrix} p_c \\\\ l_{1x} \\\\ l_{1y} \\\\ \\vdots \\\\ l_{nx} \\\\ l_{ny} \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{equation}\\] THE LABELS MUST BE CONSISTENT! Always start from the exact same location of the object! (e.g. if you start with the left corner of the left eye for one image, you should always start with the left corner of the left eye for all images.) #landmarks should be the same! I personally have a very awful experience with Landmark Detection. When the algorithms of object detection were not yet well-known in the IT industry, I worked on a project of digital screen defects detection in a Finnish company. Since digital screen defects are 1) black &amp; white 2) in very simple geometric shapes, the usage of bounding boxes could have significantly reduced the complexity of both data collection and NN model building. However, the team insisted to use landmark detection. Due to 1) that screen defects are unstructured 2) that the number of landmark points for two different screen defects can hardly be the same, the dataset was basically unusable, and none of the models we built could learn accurate patterns from it, leading to an unfortunate failure. I personally would argue that bounding box is much better than landmark detection in most practical cases. Sliding Window Apply a sliding window with a fixed size to scan every part of the img left-right and top-bottom (just like CONV), and feed each part to CNN In order to capture the same type of objects in different sizes and positions in the img, shrink the img (i.e. enlarge the sliding window) and scan again, and repeat. Problem: HUGE computational cost! Solution: (contemporary) Convert FC layer into CONV layer Share the former FC info with latter convolutions First run of the CNN. Second run of the same CNN with a bigger size of the same img (due to sliding window). Notice that the FC info from the first run is shared in the second run. Latter runs of the same CNN with bigger sizes of the same img (due to sliding window). Notice that the FC info from all previous runs is shared in this run, thus saving computation power and memories. Intersection over Union Is the purple box a good prediction of the car location? Intersection over Union is defined as: \\[\\begin{equation} \\text{IoU}=\\frac{\\text{area of intersection}}{\\text{area of union}} \\end{equation}\\] In this case, area of intersection is the intersection between the red and purple box, and area of union is the total area covered by the red and purple box. If \\(\\text{IoU}\\leq 0.5\\), then the prediction box is correct. (Other threshold values are also okay but 0.5 is conventional.) YOLO (You Only Look Once) Grids: divide the image into grids &amp; use each grid as a bounding box when \\(p_c=0\\), we ignore the entire grid \\(p_c=1\\) only when the central point of the object \\(\\in\\) the grid target output: \\(Y.\\text{shape}=n_{\\text{grid}}\\times n_{\\text{grid}}\\times y.\\text{length}\\) Non-Max Suppression: what happens when the grid is too small to capture the entire object? Discard all boxes with \\(p_c\\leq 0.6\\) Pick the box with the largest \\(p_c\\) as the prediction Discard any remaining box with \\(\\text{IoU}\\geq 0.5\\) with the prediction Repeat till there is only one box left. Anchor Boxes: what happens when two objects overlap? (e.g. a hot girl standing in front of a car) Predefine Anchor boxes for different objects Redefine the target value as a combination of Anchor 1 + Anchor 2 \\[\\begin{equation} y=\\begin{bmatrix} p_{c1} \\\\ \\vdots \\\\ p_{c2} \\\\ \\vdots \\end{bmatrix} \\end{equation}\\] Each object in the image is assigned to grid cell that contains objects central point &amp; anchor box for the grid cell with the highest \\(\\text{IoU}\\) General Procedure: Divide the images into grids and label the objects Train the CNN Get the prediction for each anchor box in each grid cell Get rid of low probability predictions Get final predictions through non-max suppression for each class R-CNN TO BE CONTINUED 4.4 Face Recognition Face Verification vs Face Recognition Verification Input image, name/ID Output whether the input image is that of the claimed person (1:1) Recognition Input image Output name/ID if the image is any of the \\(K\\) ppl in the database (1:K) Siamese Network One Shot Learning: learn a similarity function The major difference between normal image classification and face recognition is that we dont have enough training examples. Therefore, rather than learning image classification, we Calculate the degree of diff between the imgs as \\(d\\) If \\(d\\leq\\tau\\): same person; If \\(d&gt;\\tau\\): diff person Preparation &amp; Objective: Encode \\(x^{(i)}\\) as \\(f(x^{(i)})\\) (defined by the params of the NN) Compute \\(d(x^{(i)},x^{(j)})=\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) i.e. distance between the two encoding vectors if \\(x^{(i)},x^{(j)}\\) are the same person, \\(\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) is small if \\(x^{(i)},x^{(j)}\\) are different people, \\(\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) is large Method 1: Triplet Loss Learning Objective: distinguish between Anchor image &amp; Positive/Negative images (i.e. A vs P / A vs N) Initial Objective: \\(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2 \\leq \\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2\\) Intuition: We want to make sure the difference of A vs P is smaller than the difference of A vs N, so that this Anchor image is classified as positive (i.e. recognized) Problem: \\(\\exists\\ &quot;0-0\\leq0&quot;\\), in which case we cant tell any difference Final Objective: \\(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2-\\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2+\\alpha\\leq0\\) Intuition: We apply a margin \\(\\alpha\\) to solve the problem and meanwhile make sure A vs N is significantly larger than A vs P Loss Function: \\[\\begin{equation} \\mathcal{L}(A,P,N)=\\max{(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2-\\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2+\\alpha, 0)} \\end{equation}\\] Intuition: As long as this thing is less than 0, the loss is 0 and thats a successful recognition! Training Process: Given 10k imgs of 1k ppl: use the 10k images to generate triplets \\(A^{(i)}, P^{(i)}, N^{(i)}\\) Make sure to have multiple imgs of the same person in the training set random choosing Choose triplets that are quite hard to train on Method 2: Binary Classification Learning Objective: Check if two imgs represent the same person or diff ppl \\(y=1\\): same person \\(y=0\\): diff ppl Training output: \\[\\begin{equation} \\hat{y}=\\sigma\\Bigg(\\sum_{k=1}^{128}{w_i \\Big|f(x^{(i)})_ k-f(x^{(j)})_ k\\Big|+b}\\Bigg) \\end{equation}\\] Precompute the output vectors \\(f(x^{(i)})\\ \\&amp;\\ f(x^{(j)})\\) so that you dont have to compute them again during each training process Neural Style Transfer Intuition: Content(C) + Style(S) = Generated Image(G) Combine Content image with Style image to Generate a brand new image Cost Function: \\[\\begin{equation} \\mathcal{J}(G)=\\alpha\\mathcal{J}_ \\text{content}(C,G)+\\beta\\mathcal{J}_ \\text{style}(S,G) \\end{equation}\\] \\(\\mathcal{J}\\): the diff between C/S and G \\(\\alpha,\\beta\\): weight params Style: correlation between activations across channels When there is some pattern in one patch, and there is another pattern that changes similarly in the other patch, they are correlated. e.g. vertical texture in one patch \\(\\leftrightarrow\\) orange color in another patch The more often they occur together, the more correlated they are. Content Cost Function: \\[\\begin{equation} \\mathcal{J}_ \\text{content}(C,G)=\\frac{1}{2}\\left\\lVert{a^{[l](C)}-a^{[1](G)}}\\right\\lVert^2 \\end{equation}\\] Use hidden layer \\(l\\) to compute content cost Use pre-trained CNN (e.g. VGG) If \\(a^{\\[l\\](C)}\\ \\&amp;\\ a^{\\[l\\](G)}\\) are similar, then both imgs have similar content Style Cost Function: \\[\\begin{equation} \\mathcal{J}_ \\text{style}(S,G)=\\sum_l{\\lambda^{[l]}\\mathcal{J}_ \\text{style}^{[l]}(S,G)} \\end{equation}\\] Style Cost per layer: \\[\\begin{equation} \\mathcal{J}^{[l]}_ \\text{style}(S,G)=\\frac{1}{(2n_h^{[l]}n_w^{[l]}n_c^{[l]})^2}\\left\\lVert{G^{[l](S)}-G^{[1](G)}}\\right\\lVert^2_F \\end{equation}\\] the first term is simply a normalization param Style Matrix: \\[\\begin{equation} G_{kk&#39;}^{[l]}=\\sum_{i=1}^{n_H^{[l]}}{\\sum_{j=1}^{n_W^{[l]}}{a_{i,j,k}^{[l]}\\cdot a_{i,j,k&#39;}^{[l]}}} \\end{equation}\\] \\(a_{i,j,k}^{\\[l]}\\): activation at height \\(i\\), width \\(j\\), channel \\(k\\) \\(G^{\\[l]}.\\text{shape}=n_c^{\\[l]}\\times n_c^{\\[l]}\\) Intuition: sum up the multiplication of the two activations on the same cell in two different channels Training Process: Intialize \\(G\\) randomly (e.g. 100 x 100 x 3) Use GD to minimize \\(\\mathcal{J}(G)\\): \\(G := G-\\frac{\\partial{\\mathcal{J}(G)}}{\\partial{G}}\\) "],["rnn.html", "5 Recurrent Neural Networks 5.1 Basics of RNN 5.2 RNN Variations 5.3 Word Embeddings 5.4 Sequence Modeling", " 5 Recurrent Neural Networks 5.1 Basics of RNN 5.1.1 Intuition of Sequence Models These are called sequence modeling: Speech recognition Music generation Sentiment classification DNA sequence analysis Machine translation Video activity recognition Name entity recognition  Forget about the tedious definitions. As a basic intuition of what we are doing in sequence modeling, here is a very simple example: We have a sentence: Pewdiepie and MrBeast are two of the greatest youtubers in human history. We want to know: where are the names in this sentence? (i.e. name entity recognition) We convert the input sentence into \\(X\\): \\(x^{\\langle 1 \\rangle}x^{\\langle 2 \\rangle}...x^{\\langle t \\rangle}...x^{\\langle 12 \\rangle}\\) where \\(x^{\\langle t \\rangle}\\) represents each word in the sentence. But how does it represent a word? Notice that we used the capitalized \\(X\\) for a single sentence. Actually, \\(X.\\text{shape}=5000\\times12\\), and \\(x.\\text{shape}=5000\\times1\\). Why? We first make a vocabulary list like \\(\\text{list}=\\[\\text{a; and; ...; history; ...; MrBeast; ...}]\\). Then, we convert each word into a one-hot vector representing the index of the word in the dictionary, e.g.: \\[\\begin{equation} x^{\\langle 1 \\rangle}=\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\longleftarrow 425,\\ x^{\\langle 2 \\rangle}=\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\longleftarrow 3578,\\ \\cdots\\cdots \\end{equation}\\] We then label the output as \\(y: 1\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\) and train our NN on this. Accordingly, we can use most of the sequences in our daily life as datasets and build our NN models on them to solve such ML problems. 5.1.2 Intuition of RNN We have very briefly mentioned that Conv1D can be used to scan through a sequence, extract features and make predictions. Then why dont we just stick to Conv1D or use normal ANNs? The scope of sequence modeling is not necessarily recognition or classification, meaning that our inputs &amp; outputs can be in very diff lengths for diff examples. Neither ANNs nor CNNs share features learned across diff positions of a text or a sequence, whereas context matters quite a lot in most sequence modeling problems. Therefore, we need to define a brand new NN structure that can perfectly align with sequence modeling - RNN: Forward propagation: - \\(a^{\\langle 0 \\rangle}=\\textbf{0}\\) - \\(a^{\\langle t \\rangle}=g(W_{a}\\[a^{\\langle t-1 \\rangle}; x^{\\langle t \\rangle}]+b_a)\\ \\ \\ \\ \\|\\ g:\\ \\text{tanh/ReLU}\\) where $W_a=\\[W_{aa}\\ W_{ax}]$ with a shape of $(100,10100)$ if we assume a dictionary of 10000 words (i.e. $x^{\\langle t \\rangle}.\\text{shape}=(10000,100)$) and the activation length of 100. \\(\\hat{y}^{\\langle t \\rangle}=g(W_{y}a^{\\langle t \\rangle}+b_y)\\ \\ \\ \\ \\|\\ g:\\ \\text{sigmoid}\\) Backward propagation: - \\(\\mathcal{L}^{\\langle t \\rangle}(\\hat{y}^{\\langle t \\rangle},y^{\\langle t \\rangle})=-\\sum_i{y_i^{\\langle t \\rangle}\\log{\\hat{y}_ i^{\\langle t \\rangle}}}\\ \\ \\ \\ \\|\\ \\)Same loss function as LogReg 5.1.3 RNN Types There is nothing much to explain here. The images are pretty clear. 5.1.4 Language Model Intuition of Softmax &amp; Conditional Probability The core of RNN is to calculate the likelihood of a sequence: \\(P(y^{\\langle 1 \\rangle},y^{\\langle 2 \\rangle},...,y^{\\langle t \\rangle})\\) and output the one with the highest probability. For example, the sequence the apple and pair salad has a much smaller possibility to occur than the sequence the apple and pear salad. Therefore, RNN will output the latter. This seems much like Softmax, and indeed it is. Recall from the formula of conditional probability, we can separate the likelihood into: \\[\\begin{equation} P\\big(y^{\\langle 1 \\rangle},y^{\\langle 2 \\rangle},...,y^{\\langle t \\rangle}\\big)=P\\big(y^{\\langle 1 \\rangle}\\big)P\\big(y^{\\langle 2 \\rangle}|y^{\\langle 1 \\rangle}\\big)...P\\big(y^{\\langle t \\rangle}|y^{\\langle 1 \\rangle},y^{\\langle 2 \\rangle},...,y^{\\langle t-1 \\rangle}\\big) \\end{equation}\\] For example, to generate the sentence I like cats. we calculate: \\[\\begin{equation} P\\big(\\text{&quot;I like cats&quot;}\\big)=P\\big(\\text{&quot;I&quot;}\\big)P\\big(\\text{&quot;like&quot;}|\\text{&quot;I&quot;}\\big)P\\big(\\text{&quot;cats&quot;}|\\text{&quot;I like&quot;}\\big) \\end{equation}\\] Language Modeling Procedure Data Preparation Training set: large corpus of English text (or other languages) Tokenize: mark every word into a token &lt;EOS&gt;: End of Sentence token &lt;UNK&gt;: Unknown word token e.g. I hate Minecraft and kids. \\(\\Rightarrow\\) I hate &lt;UNK&gt; and kids. &lt;EOS&gt; Training We use the sentence I hate Minecraft and kids. &lt;EOS&gt; as one training example. At the beginning, we initialize \\(a^{&lt;0&gt;}\\) and \\(x^{&lt;1&gt;}\\) as \\(\\vec{0}\\) and let the RNN try to guess the first word. At each step, we use the original word at the same index \\(y^{\\&lt;i-1&gt;}\\) and the previous activation \\(a^{\\&lt;i-1&gt;}\\) to let the RNN try to guess the next word \\(\\hat{y}^{\\&lt;i&gt;}\\) from Softmax regression. During the training process, we try to minimize the loss function \\(\\mathcal{L}(\\hat{y},y)\\) to ensure the training is effective to predict the sentence correctly. Sequence Sampling After the RNN is trained, we can use it to generate a sentence by itself. In each step, the RNN will take the previous word it generated \\(\\hat{y}^{\\&lt;i-1&gt;}\\) as \\(x^{\\&lt;i&gt;}\\) to generate the next word \\(\\hat{y}^{\\&lt;i&gt;}\\). Character-level LM Dictionary Normal LM: [a, abandon, , zoo, &lt;UNK&gt;] Char-lv LM: [a, b, c, , z] Pros &amp; Cons Pros: never need to worry about unknown words &lt;UNK&gt; Cons: sequence becomes much much longer; the RNN doesnt really learn anything about the words. Problems with current RNN One of the most significant problems with our current simple RNN is vanishing gradients. As shown in the figures above, the next word always has a very strong dependency on the previous word, and the dependency between two words weakens as the distance between them gets longer. In other words, the current RNN are very bad at catching long-line dependencies, for example, the cat, which already , was full. the cats, which already , were full. be verbs have high dependencies on the subject, but RNN doesnt know that. Since the distance between these two words are too long, the gradient on the subject nouns would barely affect the training on the be verbs. 5.2 RNN Variations RNN GRU LSTM As shown above, there are currently 3 most used RNN blocks. The original RNN block activates the linear combination of \\(a^{\\&lt;t-1&gt;}\\) and \\(x^{\\&lt;t&gt;}\\) with a \\(\\text{tanh}\\) function and then passes the output value onto the next block. However, because of the previously mentioned problem with the original RNN, scholars have created some variations, such as GRU &amp; LSTM. 5.2.1 GRU (Gated Recurrent Unit) As the name implies, GRU is an advancement of normal RNN block with gates. There are 2 gates in GRU: R gate: (Remember) determine whether to remember the previous cell U gate: (Update) determine whether to update the computation with the candidate Computing process of GRU: Compute R gate: \\[\\begin{equation} \\Gamma_r=\\sigma\\big(w_r\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_r\\big) \\end{equation}\\] Compute U gate: \\[\\begin{equation} \\Gamma_u=\\sigma\\big(w_u\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_u\\big) \\end{equation}\\] Compute Candidate: \\[\\begin{equation} \\tilde{c}^{&lt;t&gt;}=\\tanh{\\big(w_c\\big[\\Gamma_r * a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_c\\big)} \\end{equation}\\] When \\(\\Gamma_r=0\\), \\(\\tilde{c}^{\\&lt;t&gt;}=\\tanh{\\big(w_cx^{\\&lt;t&gt;}+b_c\\big)}\\), the previous word has no effect on the word choice of this cell. Compute Memory Cell: \\[\\begin{equation} c^{&lt;t&gt;}=\\Gamma_u \\cdot \\tilde{c}^{&lt;t&gt;} + (1-\\Gamma_u) \\cdot c^{&lt;t-1&gt;} \\end{equation}\\] When \\(\\Gamma_u=1\\), \\(c^{\\&lt;t&gt;}=\\tilde{c}^{\\&lt;t&gt;}\\). The candidate updates. When \\(\\Gamma_u=0\\), \\(c^{\\&lt;t&gt;}=c^{\\&lt;t-1&gt;}\\). The candidate does not update. Output: \\[\\begin{equation} a^{&lt;t&gt;}=c^{&lt;t&gt;} \\end{equation}\\] 5.2.2 LSTM (Long Short-Term Memory) LSTM is an advancement of GRU. While GRU relatively saves more computing power, LSTM is more powerful. There are 3 gates in LSTM: F gate: (Forget) determine whether to forget the previous cell U gate: (Update) determine whether to update the computation with the candidate O gate: (Update) Compute the normal activation Computing process of GRU: Compute F gate: \\[\\begin{equation} \\Gamma_f=\\sigma\\big(w_f\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_f\\big) \\end{equation}\\] Compute U gate: \\[\\begin{equation} \\Gamma_u=\\sigma\\big(w_u\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_u\\big) \\end{equation}\\] Compute O gate: \\[\\begin{equation} \\Gamma_o=\\sigma\\big(w_o\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_o\\big) \\end{equation}\\] Compute Candidate: \\[\\begin{equation} \\tilde{c}^{&lt;t&gt;}=\\tanh{\\big(w_c\\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\\big]+b_c\\big)} \\end{equation}\\] Compute Memory Cell: \\[\\begin{equation} c^{&lt;t&gt;}=\\Gamma_u \\cdot \\tilde{c}^{&lt;t&gt;} + \\Gamma_f \\cdot c^{&lt;t-1&gt;} \\end{equation}\\] Output: \\[\\begin{equation} a^{&lt;t&gt;}=\\Gamma_o \\cdot \\tanh{c^{&lt;t&gt;}} \\end{equation}\\] Peephole Connection: as shown in the formulae, the gate values \\(\\Gamma \\propto c^{\\&lt;t-1&gt;}\\), therefore, we can always include \\(c^{\\&lt;t-1&gt;}\\) into gate calculations to simplify the computing. 5.2.3 Bidirectional RNN Problem: Sometimes, our choices of previous words are dependent on the latter words. For example, Teddy Roosevelt was a nice president. Teddy bears are now on sale!!! The word Teddy represents two completely different things, but without the context from the latter part, we cannot determine what the Teddy stands for. (This example is cited from Andrew Ngs Coursera Specialization) Solution: We make the RNN bidirectional: Each output is calculated as: \\(\\hat{y}^{\\&lt;t&gt;}=g\\Big(W_y\\Big[\\overrightarrow{a}^{\\&lt;t&gt;};\\overleftarrow{a}^{\\&lt;t&gt;}\\Big]+b_y\\Big)\\) 5.2.4 Deep RNN Dont be fascinated by the name. Its just stacks of RNN layers: 5.3 Word Embeddings Word embedding is a vectorized representation of a word. Because our PC cannot directly understand the meaning of words, we need to convert these words into numerical values first. So far, we have been using One-hot Encoding: \\[\\begin{equation} x^{&lt;1&gt;}=\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\longleftarrow 425,\\ x^{&lt;2&gt;}=\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\longleftarrow 3578,\\ \\cdots\\cdots \\end{equation}\\] Problem: our RNN doesnt really learn anything about these words from one-hot representation. 5.3.1 Featurized Representation Intuition: Suppose we have an online shopping review: Love this dress! Sexy and comfy! we can represent this sentence as: We predefine a certain number of features (e.g. gender, royalty, food, size, cost, etc.). Then, we give each word (column categories) their relevance to each feature (row categories). As shown in the picture for example, dress is very closely related to the feature gender, therefore given the value 1. Meanwhile, love is very closely related to the feature positive, therefore given the value 0.99. After we define all the featurized values for the words, we get a vectorized representation of each word: \\[\\begin{equation} \\text{love}=e_ {1479}=\\begin{bmatrix} 0.03 \\\\ 0.01 \\\\ 0.99 \\\\ 1.00 \\\\ \\vdots \\end{bmatrix},\\text{comfy}=e_ {987}=\\begin{bmatrix} 0.01 \\\\ 0.56 \\\\ 0.98 \\\\ 0.00 \\\\ \\vdots \\end{bmatrix},\\cdots\\cdots\\end{equation}\\] This way, our RNN will get to know the rough meanings of these words. For example, when it needs to generate the next word of this sentence: I want a glass of orange _____. Since it knows that orange is a fruit and that glass is closely related to liquid, there is a much higher possibility that our RNN will choose juice to fill in the blank. Embedding matrix: To acquire the word embeddings such as \\(\\vec{e}_ {1479}\\) and \\(\\vec{e}_ {987}\\) above, we can multiply our embedding matrix with the one-hot encoding: \\[\\begin{equation} E\\times \\vec{o}_ j=\\vec{e}_ j \\end{equation}\\] where \\(E\\) is our featurized representation (i.e. embedding matrix) and \\(\\vec{o}_ j\\) is the one-hot encoding of the word (i.e. the index of the word). In practice, this is too troublesome since the dimensions of our \\(E\\) tend to be huge (e.g. \\((500,10000)\\)). Thus, we use specialized function to look up an embedding directly from the embedding matrix. Analogies: One of the most useful properties of word embeddings is analogies. For example, man \\(\\rightarrow\\) woman=king \\(\\rightarrow\\) ?. Suppose we have the following featurized representation: man woman king queen gender -1 1 -0.99 0.99 royal 0.01 0.02 0.97 0.96 age 0.01 0.01 0.78 0.77 food 0.03 0.04 0.04 0.02 In order to learn the analogy, our RNN will have the following thinking process: \\[\\begin{align} &amp;\\text{Goal: look for}\\ w: \\mathop{\\arg\\max}_ w{sim(e_w, e_{\\text{king}}-(e_{\\text{man}}-e_{\\text{woman}}))} \\\\ &amp;\\because e_{\\text{man}}-e_{\\text{woman}}\\approx\\begin{bmatrix}-2 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}, e_{\\text{king}}-e_{\\text{queen}}\\approx\\begin{bmatrix}-2 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\\\ &amp;\\therefore e_{\\text{man}}-e_{\\text{woman}}\\approx e_{\\text{king}}-e_{\\text{queen}} \\\\ &amp;\\text{Calculate cosine similarity: } sim(\\vec{u},\\vec{v})=\\cos{\\phi}=\\frac{\\vec{u}^T\\vec{v}}{\\|\\vec{u}\\|_ 2\\|\\vec{v}\\|_ 2} \\\\ &amp;\\text{Confirm: }e_w\\approx e_{queen} \\end{align}\\] 5.3.2 Learning 1: Word2Vec Problem: We definitely do not want to write the embedding matrix by ourselves. Instead, we train a NN model to learn the word embeddings. Suppose we have a sentence Pewdiepie and MrBeast are two of the greatest youtubers in human history. Before Word2Vec, lets define context &amp; target: Context: words around target word last 4 words: two of the greatest ______ 4 words on both sides: two of the greatest ______ in human history. last 1 word: greatest ______ skip-gram: (any nearby word)  MrBeast  ______  Target: the word we want our NN to generate youtubers Algorithm: Randomly choose context &amp; target words with skip-gram. (e.g. context MrBeast &amp; target youtubers) Learn mapping of \\(c\\ (\\text{&quot;mrbeast&quot;}\\[1234])\\rightarrow t\\ (\\text{&quot;youtubers&quot;}\\[765])\\) Use softmax to calculate the probability of appearance of target given context: \\[\\begin{equation} \\hat{y}=P(t|c)=\\frac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{n}{e^{\\theta_j^Te_c}}} \\end{equation}\\] Minimize the loss function: \\[\\begin{equation} \\mathcal{L}(\\hat{y},y)=-\\sum_{i=1}^{n}{y_i\\log{\\hat{y}_ i}} \\end{equation}\\] Notes: * Computation of softmax is very slow: Hierarchical Softmax (i.e. Huffman Tree + LogReg) can solve this problem - with common words at the top and useless words at the bottom. * \\(c\\ \\&amp;\\ t\\) should not be entirely random: words like the/at/on/it/ should not be chosen. 5.3.3 Learning 2: Negative Sampling Problem: Given a pair of words, predict whether its a context-target pair. For example, given the word orange as the context, we want our model to know that orange &amp; juice is a context-target pair but orange &amp; king is not. Algorithm: Pick a context-target pair \\((c,t)\\) (the target should be near the context) from the text corpus as a positive example. Pick random words \\(\\\\{t_1,\\cdots,t_k\\\\}\\) from the dictionary and form word pairs \\(\\\\{(c,t_1),\\cdots,(c,t_k)\\\\}\\) as negative examples based on the following probability that the creator recommended: \\[\\begin{equation} P(w_i)=\\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=1}^{n}{f(w_i)^{\\frac{3}{4}}}} \\end{equation}\\] where \\(w_i\\) is the \\(i\\)th word in the dictionary. Train a binary classifier based on the training examples from previous steps: \\[\\begin{equation} \\hat{y}_ i=P(y=1|c,t_i)=\\sigma(\\theta_{t_i}^Te_c) \\end{equation}\\] Repeat Step 1-3 till we form our final embedding matrix \\(E\\). Negative Sampling is relatively faster and less costly compared to Word2Vec, since it replaces softmax with binary classification. 5.3.4 Learning 3: GloVe (Global Vectors) Problem: Learn word embeddings based on how many times target \\(i\\) appears in context of word \\(j\\). Algorithm: Minimize \\[\\begin{equation} \\sum_{i=1}^{n}{\\sum_{j=1}^{n}{f\\big(X_{ij}\\big)\\big(\\theta_i^Te_j+b_i+b&#39;_ j-\\log{X_{ij}}\\big)^2}} \\end{equation}\\] \\(X_{ij}\\): #times \\(i\\) appears in context of \\(j\\) \\(f\\big(X_{ij}\\big)\\): weighing term \\(f\\big(X_{ij}\\big)=0\\) if \\(X_{ij}=0\\) \\(f\\big(X_{ij}\\big)\\) high for uncommon words \\(f\\big(X_{ij}\\big)\\) low for too-common words \\(b_i:t\\), \\(b&#39;_ j:c\\) Compute the final embedding of word \\(w\\): \\[\\begin{equation} e_w^{\\text{final}}=\\frac{e_w+\\theta_w}{2} \\end{equation}\\] 5.4 Sequence Modeling 5.4.1 Sentiment Classification Problem Setting: (many-to-one) given text, predict sentiment. Model: 5.4.2 Seq2Seq Problem Setting: (many-to-many) given an entire sequence, generate a new sequence. Example 1: Machine Translation: Machine Translation vs Language Model: * Language Model: maximize \\(P(y^{\\&lt;1&gt;},\\cdots,y^{\\&lt;T_y&gt;})\\) * Machine Translation: maximize \\(P(y^{\\&lt;1&gt;},\\cdots,y^{\\&lt;T_y&gt;} \\| \\vec{x})\\) Example 2: Image Captioning 5.4.3 Beam Search Problem: So far, when we choose a word from softmax for each RNN block, we are doing greedy search, that we only look for local optimum instead of global optimum. That is, we only choose the word with the highest \\(P(y^{\\&lt;1&gt;}\\|\\vec{x})\\) and then the word with the highest \\(P(y^{\\&lt;2&gt;}\\|\\vec{x})\\) and then  As we already know, local optimum does not necessarily represent global optimum. In the world of NLP, the word going always has a much higher probability to appear than the word visiting, but in certain situations when we need to use visiting, the algorithm will still choose going, therefore generating a weird sequence as a whole. Beam Search Algorithm: Define a beam size of \\(B\\) (usually \\(B\\in\\\\{1\\times10^n,3\\times10^n\\\\},\\ n\\in\\mathbb{Z}^+\\)). Look at the top \\(B\\) words with the highest \\(P\\)s for the first word. (i.e. look for \\(P(\\vec{y}^{\\&lt;1&gt;}\\|\\vec{x})\\)) Repeat till &lt;EOS&gt;. Choose the sequence with the highest combined probability. Improvement: The original Beam Search is very costly in computing, therefore it is necessary to refine it: \\[\\begin{align} &amp;\\because P(y^{&lt;1&gt;},\\cdots,y^{&lt;T_y&gt;}|x)=\\prod_{t=1}^{T_y}{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\\cdots,y^{&lt;t-1&gt;})} \\\\ &amp;\\therefore \\text{goal}=\\mathop{\\arg\\max}_ y{\\prod_{t=1}^{T_y}{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\\cdots,y^{&lt;t-1&gt;})}} \\\\ &amp;\\Rightarrow \\mathop{\\arg\\max}_ y{\\sum_{t=1}^{T_y}{\\log{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\\cdots,y^{&lt;t-1&gt;})}}} \\\\ &amp;\\Rightarrow \\mathop{\\arg\\max}_ y{\\frac{1}{T_y^{\\alpha}}\\sum_{t=1}^{T_y}{\\log{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\\cdots,y^{&lt;t-1&gt;})}}} \\end{align}\\] \\(\\prod\\rightarrow\\sum{\\log}\\): log scaling \\(\\frac{1}{T_y^{\\alpha}}\\): length normalization (when you add more negative values (\\(\\log{(P&lt;1)}&lt;0\\)), the sum becomes more negative) \\(\\alpha\\): learning rate just a coefficient Error Analysis: Suppose we want to analyze the following error: Human: Jimmy visits Africa in September. (\\(y^\\*\\)) Algorithm: Jimmy visited Africa last September. (\\(\\hat{y}\\)) If \\(P(y^\\*\\|x)&gt;P(\\hat{y}\\|x)\\), Beam search is at fault \\(\\rightarrow\\) increase \\(B\\) If \\(P(y^\\*\\|x)\\leq P(\\hat{y}\\|x)\\), RNN is at fault \\(\\rightarrow\\) improve RNN (data augmentation, regularization, architecture, etc.) 5.4.4 Bleu Score Problem: For many sequence modeling problems (especially seq2seq), there is no fixed correct answer. For example, there are many different Chinese translated versions of the same fiction Sherlock Holmes, and they are all correct. In this case, how do we define correctness for machine translation? Bilingual Evaluation Understudy: \\[\\begin{equation} p_n=\\frac{\\sum_{\\text{n-gram}\\in\\hat{y}}{\\text{count}_ {clip}(\\text{n-gram})}}{\\sum_{\\text{n-gram}\\in\\hat{y}}{\\text{count}(\\text{n-gram})}} \\end{equation}\\] \\(\\text{n-gram}\\): \\(n\\) consecutive words (e.g. bigram: I have a pen. \\(\\rightarrow\\) I have, have a, a pen) \\(\\text{count}_ {clip}(\\text{n-gram})\\): maximal #times an n-gram appears in one of the reference sequences \\(\\text{count}(\\text{n-gram})\\): #times an n-gram appears in \\(\\hat{y}\\) For example, input: Le chat est sur le tapis. Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: the cat the cat on the mat. The unigrams here are: the, cat, on, mat. Then, \\[\\begin{equation} p_1=\\frac{2+1+1+1}{3+2+1+1}=\\frac{5}{7} \\end{equation}\\] The bigrams here are: the cat, cat the, cat on, on the, the mat. Then, \\[\\begin{equation} p_2=\\frac{1+0+1+1+1}{2+1+1+1+1}=\\frac{2}{3} \\end{equation}\\] The final Bleu score will be calculated as: \\[\\begin{equation} \\text{BLEU}=BP\\times e^{\\frac{1}{4}\\sum_{n=1}^{4}{p_n}} \\end{equation}\\] usually we take \\(n=4\\) as the upper limit for n-grams. \\(BP\\): param to penalize short outputs (\\(\\because\\) short outputs tend to have high BLEU scores.) \\(BP=1\\) if \\(\\text{len}(\\hat{y})&gt;\\text{len}(\\text{ref})\\) \\(BP=e^{\\frac{1-\\text{len}(\\hat{y})}{\\text{len}(\\text{ref})}}\\) if \\(\\text{len}(\\hat{y})\\leq\\text{len}(\\text{ref})\\) 5.4.5 Attention Model Problem: Our Seq2Seq model memorizes the entire sequence and then start to generate output sequence. However, a better approach to such problems like machine translation is actually to memorize part of the sequence, translate it, then memorize the next part of the sequence, translate it, and then keep going. Memorizing the entire fiction series of Sherlock Holmes and then translate it is just inefficient. Model: Attention Model = Encoding BRNN + Decoding RNN Algorithm: Combine BRNN activations: \\[\\begin{equation} a^{&lt;t&#39;&gt;}=\\Big(\\overleftarrow{a}^{&lt;t&#39;&gt;},\\overrightarrow{a}^{&lt;t&#39;&gt;}\\Big) \\end{equation}\\] where \\(t&#39;\\) refers to the index of the encoding BRNN layer. Calculate the amount of attention that \\(y^{\\&lt;t&gt;}\\) should pay to \\(a^{\\&lt;t&#39;&gt;}\\): \\[\\begin{equation} \\alpha^{&lt;t,t&#39;&gt;}=\\frac{e^{(e^{&lt;t,t&#39;&gt;})}}{\\sum_{t&#39;=1}^{T_x}{e^{(e^{&lt;t,t&#39;&gt;})}}} \\end{equation}\\] where \\(e^{\\&lt;t,t&#39;&gt;}=W_e^{\\&lt;t,t&#39;&gt;}\\[s^{\\&lt;t-1&gt;};a^{\\&lt;t&#39;&gt;}] +b_e^{\\&lt;t,t&#39;&gt;}\\) is a linear combination of both encoding activation \\(a^{\\&lt;t&#39;&gt;}\\) and decoding activation \\(s^{\\&lt;t-1&gt;}\\). \\(t\\) refers to the index of the decoding RNN layer. Calculate the total attention at \\(t\\): \\[\\begin{equation} c^{&lt;t&gt;}=\\sum_{t&#39;}{\\alpha^{&lt;t,t&#39;&gt;}a^{&lt;t&#39;&gt;}} \\end{equation}\\] Include the total attention into the input for output calculation: \\[\\begin{equation} \\hat{y}^{&lt;t&gt;}=s^{&lt;t&gt;}=g\\big(W_y[\\hat{y}^{&lt;t-1&gt;};c^{&lt;t&gt;}]+b_y\\big) \\end{equation}\\] "]]
