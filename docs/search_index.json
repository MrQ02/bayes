[["index.html", "Time Series Analysis References", " Time Series Analysis Renyi Qu 2021/01/25 References Bollerslev, T. (1986). Generalized Autoregressive Conditional Heteroskedasticity. Journal of Econometrics. vol. 31. https://doi.org/10.1016/0304-4076(86)90063-1. Enders, W. (2015). Applied Econometric Time Series. JohnWiley &amp; Sons Inc. DOI:10.1198/tech.2004.s813. Engle, R.F. (1982). Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation. Econometrica. vol. 50, issue. 4. https://doi.org/10.2307/1912773. "],["m.html", "1 Mean Models 1.1 AR (Autoregressive model) 1.2 MA (Moving Average model) 1.3 ARMA (Autoregressive Moving Average model) 1.4 ARIMA (Autoregressive Integrated Moving Average model)", " 1 Mean Models 1.1 AR (Autoregressive model) 1.1.1 Model AR(p) - General: \\[\\begin{equation*} x_t=\\alpha_0+\\sum_{i=1}^{p}{\\alpha_i x_{t-i}}+\\varepsilon_t \\end{equation*}\\] \\(x_t\\): time series to be modeled \\(x_{t-i}\\): past time series values \\(p\\): lag limit \\(\\alpha_i\\): params to be estimated \\(\\varepsilon_t \\sim N(0,\\sigma^2)\\): i.i.d. white noise AR(p) - Markovian: \\[\\begin{align*} p(y_{1:T})&amp;=p(y_{1:p})\\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\\\ p(\\boldsymbol{y}|y_{1:p})&amp;=\\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\\\ &amp;=\\prod_{t=p+1}^{T}{N(y_t|\\boldsymbol{f}_t^\\prime\\boldsymbol{\\phi},v)}\\\\ &amp;=N(\\boldsymbol{y}|\\boldsymbol{F}^\\prime\\boldsymbol{\\phi},v\\boldsymbol{I}_n) \\end{align*}\\] \\(T=n+p\\) \\(\\phi=(\\phi_1,\\cdots,\\phi_p)\\) \\(\\boldsymbol{f}_t=(y_{t-1},\\cdots,y_{t-p})\\) \\(\\boldsymbol{F}=[\\boldsymbol{f}_T,\\cdots,\\boldsymbol{f}_{p+1}]\\) AR(p) - State-space: \\[\\begin{align*} y_t&amp;=\\boldsymbol{F}^\\prime\\boldsymbol{x}_t\\\\ \\boldsymbol{x}_t&amp;=\\boldsymbol{G}\\boldsymbol{x}_{t-1}+\\boldsymbol{\\omega}_t \\end{align*}\\] \\(\\boldsymbol{x}_t=(y_t,\\cdots,y_{t-p+1})\\): state vector \\(\\boldsymbol{\\omega}_t=(\\epsilon_t,0,\\cdots,0)\\): error vector \\(\\boldsymbol{F}=(1,0,\\cdots,0)\\) \\(\\boldsymbol{G}=\\begin{bmatrix}\\phi_1 &amp; \\phi_2 &amp; \\cdots &amp; \\phi_{p-1} &amp; \\phi_p \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; 0 &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0\\end{bmatrix}\\) AR(p) - Autoorrelation: \\[\\begin{align*} &amp;\\rho(h)-\\sum_{i=1}^{p}{\\phi_i\\rho(h-i)}=0\\\\ &amp;\\rho(h)=\\sum_{j=1}^{r}{\\alpha_j^hp_j(h)} \\end{align*}\\] \\(\\alpha_1,\\cdots,\\alpha_r\\): reciprocal roots of characteristic polynomial \\(m_1,\\cdots,m_r\\): root multiplicity (\\(\\sum_{i=1}^r{m_i}=p\\)) \\(p_j(h)\\): polynomial of degree \\(m_j-1\\) 1.1.2 Estimation LSE - AR(1): \\[\\begin{align*} \\hat{\\alpha}_{ols}&amp;=\\mathop{\\mathrm{argmin}}_{\\alpha}{\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}}\\\\ &amp;=\\bigg(\\sum_{t=1}^{T}{x_{t-1}^2}\\bigg)^{-1}\\bigg(\\sum_{t=1}^{T}{x_{t-1}x_t}\\bigg) \\end{align*}\\] MLE - AR(1): Conditional likelihood: \\[\\begin{equation*} \\mathcal{L}(\\boldsymbol{x|x_{-1}})=\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^T\\exp{\\bigg(-\\frac{1}{2\\sigma^2}\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}\\bigg)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} l(\\boldsymbol{x|x_{-1}})=-T\\log{\\sqrt{2\\pi}\\sigma}-\\frac{1}{2\\sigma^2}\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2} \\end{equation*}\\] Conditional ML estimator: \\[\\begin{align*} \\hat{\\alpha}_{ml}&amp;=\\mathop{\\mathrm{argmax}}_\\alpha{l(\\boldsymbol{x|x_{-1}})} \\\\ &amp;=\\mathop{\\mathrm{argmax}}_\\alpha{\\bigg(-\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}\\bigg)} \\\\ &amp;=\\mathop{\\mathrm{argmin}}_\\alpha{\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}} \\\\ \\end{align*}\\] 1.2 MA (Moving Average model) 1.2.1 Model MA(q): \\[\\begin{equation*} x_t=\\mu+\\sum_{i=1}^{q}{\\beta_i \\varepsilon_{t-i}}+\\varepsilon_t \\end{equation*}\\] \\(\\mu\\): mean of \\(x_t\\) \\(\\varepsilon_{t-i}\\): past error values \\(q\\): lag limit \\(\\beta_i\\): params to be estimated \\(\\varepsilon_t\\): unobservable white noise at the current time 1.2.2 Estimation LSE - MA(1): \\[\\begin{equation*} \\hat{\\beta}_{ols}=\\mathop{\\mathrm{argmin}}_\\beta{\\sum_{t=1}^T{(x_t-\\beta\\varepsilon_{t-1})^2}} \\end{equation*}\\] How to deal with unknown random disturbances: \\[\\begin{align*} &amp;\\varepsilon_1=x_1-\\beta\\varepsilon_0 \\\\ &amp;\\varepsilon_2=x_2-\\beta(x_1-\\beta\\varepsilon_0) \\\\ &amp;\\varepsilon_3=x_3-\\beta(x_2-\\beta(x_1-\\beta\\varepsilon_0)) \\\\ &amp;\\varepsilon_{t-1}=(-\\beta)^{t-1}\\varepsilon_0+\\sum_{k=0}^{t-2}{(-\\beta)^kx_{t-1-k}}\\end{align*}\\] Thus, we obtain the Nonlinear LS estimator: \\[\\begin{equation*} \\hat{\\beta}_{nls}=\\mathop{\\mathrm{argmin}}_{\\beta}{\\sum_{t=1}^{T}{\\bigg(x_t-\\beta\\sum_{k=0}^{t-2}{(-\\beta)^kx_{t-1-k}}\\bigg)^2}} \\end{equation*}\\] MLE - MA(1): How to deal with unknown random disturbances: \\[\\begin{align*} &amp;\\mathcal{L}(x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_1^2}{2\\sigma^2}\\bigg)} \\\\ &amp;\\mathcal{L}(x_2|x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{(x_2-\\beta\\varepsilon_1)^2}{2\\sigma^2}\\bigg)}=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_2^2}{2\\sigma^2}\\bigg)} \\\\ &amp;\\cdots \\\\ &amp;\\mathcal{L}(x_t|x_{t-1},\\cdots,x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{(x_t-\\beta\\varepsilon_{t-1})^2}{2\\sigma^2}\\bigg)}\\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_t^2}{2\\sigma^2}\\bigg)} \\end{align*}\\] Conditional likelihood: \\[\\begin{equation*} \\mathcal{L}(\\boldsymbol{x|x_{-1}})=\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^T\\exp{\\bigg(-\\frac{1}{2\\sigma^2}\\sum_{t=1}^T{\\varepsilon_t^2}\\bigg)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} l(\\boldsymbol{x|x_{-1}})=-T\\log{\\sqrt{2\\pi}\\sigma}-\\frac{1}{2\\sigma^2}\\sum_{t=1}^T{\\varepsilon_t^2} \\end{equation*}\\] Conditional ML estimator: \\[\\begin{equation*} \\hat{\\beta}_{ml}=\\mathop{\\mathrm{argmax}}_\\beta{l(\\boldsymbol{x|x_{-1}})}=\\mathop{\\mathrm{argmin}}_\\beta{\\sum_{t=1}^T{\\varepsilon_t^2}} \\end{equation*}\\] 1.3 ARMA (Autoregressive Moving Average model) 1.3.1 Model ARMA(p,q) - General: \\[\\begin{equation*} x_t=\\alpha_0+\\sum_{i=1}^{p}{\\alpha_i x_{t-i}}+\\sum_{i=1}^{q}{\\beta_i \\varepsilon_{t-i}}+\\varepsilon_t \\end{equation*}\\] Explanatory variables: \\(x_{t-i}\\ \\&amp;\\ \\varepsilon_{t-i}\\) Parameters: \\(\\alpha_i\\ \\&amp;\\ \\beta_i\\) Hyperparameters: \\(p\\ \\&amp;\\ q\\) Random disturbance: \\(\\varepsilon_t\\) ARMA(p,q) - State-space: \\[\\begin{align*} y_t&amp;=\\boldsymbol{E}_m^\\prime\\boldsymbol{\\theta}_t\\\\ \\boldsymbol{\\theta}_t&amp;=\\boldsymbol{G}\\boldsymbol{\\theta}_{t-1}+\\boldsymbol{\\omega}_t \\end{align*}\\] \\(\\boldsymbol{E}_m=(1,0,\\cdots,0)\\), shape \\(m=\\max{(p,q+1)}\\) \\(\\boldsymbol{\\omega}_t=(1,\\theta_1,\\cdots,\\theta_{m-1})\\epsilon_t\\) \\(\\boldsymbol{G}=\\begin{bmatrix}\\phi_1 &amp; \\phi_2 &amp; \\cdots &amp; \\phi_{m-1} &amp; \\phi_m \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; 0 &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0\\end{bmatrix}\\) 1.3.2 Estimation LSE - ARMA(p,q): Minimize: \\[\\begin{equation*} S(\\boldsymbol{\\theta})=\\sum_{t=1}^{T}{\\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}} \\end{equation*}\\] \\(\\boldsymbol{\\theta}=(\\alpha_1,\\cdots,\\alpha_p,\\beta_1,\\cdots,\\beta_q)\\): parameter set Conditional LS: \\[\\begin{equation*} S_c(\\boldsymbol{\\theta})=\\sum_{t=p+1}^T{\\epsilon_t(\\boldsymbol{\\theta})^2} \\end{equation*}\\] \\(\\epsilon_t(\\boldsymbol{\\theta})=y_t-\\hat{y}_t\\) MLE - ARMA(p,q): Conditional likelihood: \\[\\begin{equation*} p(y_{1:T}|\\boldsymbol{\\theta},v)=\\prod_{t=1}^T{p(y_t|y_{1:(t-1)},\\boldsymbol{\\theta},v)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} \\log{p(y_{1:T}|\\boldsymbol{\\theta},v)}=-\\frac{T}{2}\\log{2\\pi v}-\\frac{1}{2}\\sum_{t=1}^T{\\Big(\\log{r_t^{t-1}}+\\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}\\Big)} \\end{equation*}\\] \\(y_t^{t-1}=\\text{E}[y_t|y_{1:(t-1)}]\\) \\(vr_t^{t-1}=\\text{Var}[y_t|y_{1:(t-1)}]\\) Bayesian - ARMA(p,q): Likelihood based on parameters: \\[\\begin{equation*} p(y_{1:T}|\\boldsymbol{\\varphi})=\\big(\\frac{1}{\\sqrt{2\\pi v}}\\big)^T\\exp{\\bigg(-\\frac{1}{2v}\\sum_{t=1}^T{(y_t-\\mu_t)^2}\\bigg)} \\end{equation*}\\] \\(\\boldsymbol{\\varphi}=(\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v,\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) \\(\\boldsymbol{\\epsilon}_0=(\\epsilon_0,\\epsilon_{-1},\\cdots,\\epsilon_{1-q})\\) \\(\\mu_1=\\sum_{i=1}^p{\\alpha_iy_{1-i}}+\\sum_{i=1}^q{\\beta_i\\epsilon_{1-i}}\\) \\(\\mu_t=\\sum_{i=1}^p{\\alpha_iy_{t-i}}+\\sum_{i=1}^{t-1}{\\beta_i(y_{t-i}-\\mu_{t-i})}+\\sum_{i=1}^q{\\beta_i\\epsilon_{t-i}},t=2:q\\) \\(\\mu_t=\\sum_{i=1}^p{\\alpha_iy_{t-i}}+\\sum_{i=1}^{t-1}{\\beta_i(y_{t-i}-\\mu_{t-i})},t=q+1:T\\) Prior: \\[\\begin{equation*} \\pi(\\boldsymbol{\\varphi})=\\pi(\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)\\pi(v)\\pi(\\boldsymbol{\\alpha},\\boldsymbol{\\beta}) \\end{equation*}\\] \\(\\pi(\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)=N(\\boldsymbol{0},v\\Omega)\\) \\(\\pi(v)\\propto\\frac{1}{v}\\) \\(\\pi(\\boldsymbol{\\alpha},\\boldsymbol{\\beta})\\): uniform distribution \\(v\\Omega=\\text{Cov}[\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0]\\) Joint Posterior: \\[\\begin{equation*} p(\\boldsymbol{\\varphi}|y_{1:T})\\propto (v)^{-\\frac{T+2}{2}}\\exp{-\\frac{1}{2v}\\sum_{t=1}^T{(y_t-\\mu_t)^2}}\\times N((\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)|\\boldsymbol{0},v\\Omega) \\end{equation*}\\] MCMC: Sample \\((v|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) from inverse-gamma full conditional distribution: Sample \\((\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)\\) using a Metropolis step with Gaussian proposal distributions Sample \\((\\boldsymbol{\\alpha},\\boldsymbol{\\beta}|v,\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) from PACF 1.4 ARIMA (Autoregressive Integrated Moving Average model) 1.4.1 Model Differencing: \\[\\begin{align*} &amp;x^{(1)}_t=x_t-x_{t-1}\\\\ &amp;x^{(2)}_t=x^{(1)}_t-x^{(1)}_{t-1}\\\\ &amp;\\cdots\\\\ &amp;x^{(d)}_t=x^{(d-1)}_t-x^{(d-1)}_{t-1} \\end{align*}\\] ARIMA(p,d,q): \\[\\begin{equation*} \\Phi(L)(1-L)^dx_t=\\Theta(L)\\varepsilon_t \\end{equation*}\\] AR operator: \\(\\Phi(L)=1-\\sum_{i=1}^{p}{\\alpha_iL^i}\\) MA operator: \\(\\Theta(L)=1+\\sum_{i=1}^{q}{\\beta_iL^i}\\) 1.4.2 Estimation LSE - ARIMA(p,1,q): Parameter Collection: \\(\\boldsymbol{\\theta}=(\\alpha_0,\\alpha_1,\\cdots,\\alpha_p,\\beta_1,\\cdots,\\beta_q)\\) Independent Variable Collection: \\(\\boldsymbol{\\omega}_t=(1,z_{t-1},\\cdots,z_{t-p},\\hat{\\varepsilon}_{t-1},\\cdots,\\hat{\\varepsilon}_{t-q})\\) : \\[\\begin{equation*} z_t=\\boldsymbol{\\theta}^T\\boldsymbol{\\omega}_t+\\varepsilon_t \\end{equation*}\\] \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}}_{ols}=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\theta}}{\\sum_{t=2}^{T}{(z_t-\\boldsymbol{\\theta}^T\\boldsymbol{\\omega}_t)^2}} \\end{equation*}\\] "],["v.html", "2 Volatility models 2.1 ARCH (Autoregressive Conditional Heteroskedasticity model) 2.2 GARCH (Generalized Autoregressive Conditional Heteroskedasticity model)", " 2 Volatility models 2.1 ARCH (Autoregressive Conditional Heteroskedasticity model) \\[\\begin{equation*} \\varepsilon_t=\\sigma_t\\zeta_t \\end{equation*}\\] \\(\\zeta_t\\): white noise following \\(N(0,1)\\) \\(\\sigma_t^2\\): variance of \\(\\varepsilon_t\\): \\[\\begin{equation*} \\sigma_t^2=\\alpha_0+\\sum_{i=1}^p{\\alpha_i\\varepsilon_{t-i}^2} \\end{equation*}\\] \\(\\alpha_0&gt;0, \\alpha_i\\geq0\\) 2.2 GARCH (Generalized Autoregressive Conditional Heteroskedasticity model) \\[\\begin{equation*} \\sigma_t^2=\\alpha_0+\\sum_{i=1}^p{\\alpha_i\\varepsilon_{t-i}^2}+\\sum_{i=1}^q{\\beta_j\\sigma_{t-j}^2} \\end{equation*}\\] 2.2.1 Estimation MLE: \\[\\begin{equation*} l_t(\\boldsymbol{\\theta},\\boldsymbol{\\varphi})=-\\frac{1}{2}\\log{2\\pi}-\\frac{1}{2}\\log{\\sigma_t^2}-\\frac{\\varepsilon_t^2}{2\\sigma_t^2} \\end{equation*}\\] FOC: \\[\\begin{align*} \\frac{\\partial l_t}{\\partial\\boldsymbol{\\theta}}&amp;=\\frac{\\varepsilon_t\\omega_t}{\\sigma_t^2}+\\frac{\\sigma_t^2}{2}\\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\theta}}\\bigg(\\frac{\\varepsilon_t^2}{\\sigma_t^2}-1\\bigg)=0 \\\\ \\frac{\\partial l_t}{\\partial\\boldsymbol{\\varphi}}&amp;=\\frac{1}{2\\sigma_t^2}\\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\varphi}}\\bigg(\\frac{\\varepsilon_t^2}{\\sigma_t^2}-1\\bigg)=0 \\end{align*}\\] Recursive algorithm: \\[\\begin{align*} \\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\theta}}&amp;=\\sum_{j=1}^q{\\beta_j\\frac{\\partial\\sigma_{t-j}^2}{\\partial\\boldsymbol{\\theta}}}-2\\sum_{i=1}^p{\\alpha_i\\boldsymbol{\\omega}_t\\varepsilon_{t-i}} \\\\ \\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\varphi}}&amp;=\\sum_{j=1}^q{\\beta_j\\frac{\\partial\\sigma_{t-j}^2}{\\partial\\boldsymbol{\\varphi}}}+\\boldsymbol{\\epsilon}_t \\end{align*}\\] "],["order-selection.html", "3 Order Selection 3.1 Stationarity 3.2 ADF Test (Augmented Dickey-Fuller Test) 3.3 ACF (Autocorrelation Function) 3.4 PACF (Partial Autocorrelation Function) 3.5 Information Criteria", " 3 Order Selection 3.1 Stationarity Covariance/Weak stationarity \\[\\begin{align*} &amp;\\text{E}[x_t]=\\text{E}[x_{t-s}]=\\mu \\\\ &amp;\\text{Var}[x_t]=\\text{Var}[x_{t-s}]=\\sigma^2 \\\\ &amp;\\text{Cov}[x_t,x_{t-s}]=\\text{Cov}[x_{t-i},x_{t-i-s}]=\\gamma_{t-s} \\end{align*}\\] Strong stationarity = Weak stationarity + no seasonal trend MA(q) models satisfy stationarity, but AR(p) models need restrictions. Lag operator \\[\\begin{equation*} L^ix_t\\equiv x_{t-i} \\end{equation*}\\] rewrite AR(p): \\[\\begin{align*} &amp;x_t=\\alpha_0+\\sum_{i=1}^{p}{\\alpha_iL^ix_{t-i}}+\\varepsilon_t \\\\ &amp;x_t=\\frac{\\varepsilon_t}{1-\\sum_{i=1}^{p}{\\alpha_iL^i}}=\\frac{\\varepsilon_t}{\\Phi(L)} \\end{align*}\\] Characteristic polynomial \\[\\begin{equation*} \\Phi(L)=1-\\sum_{i=1}^{p}{\\alpha_iL^i}=\\prod_{i=1}^{p}{(1-\\phi_iL)} \\end{equation*}\\] \\(x_t \\sim\\) AR(\\(p\\)) is stationary \\(\\Leftrightarrow |\\frac{1}{\\phi_i}|&gt;1\\). 3.2 ADF Test (Augmented Dickey-Fuller Test) Intuition: tests the null hypothesis that there exists such a unit root. \\[\\begin{align*} x_t&amp;=\\cdots+\\alpha_{p-2}x_{t-p+2}+\\alpha_{p-1}x_{t-p+1}+\\alpha_{p}x_{t-p} \\\\ &amp;=\\cdots+\\alpha_{p-2}x_{t-p+2}+\\alpha_{p-1}x_{t-p+1}+\\alpha_px_{t-p+1}+\\alpha_{p}x_{t-p}-\\alpha_px_{t-p+1} \\\\ &amp;=\\cdots+\\alpha_{p-2}x_{t-p+2}+(\\alpha_{p-1}+\\alpha_p)x_{t-p+1}-\\alpha_p\\Delta x_{t-p+1} \\\\ &amp;=\\cdots+\\alpha_{p-2}x_{t-p+2}+(\\alpha_{p-1}+\\alpha_p)x_{t-p+2}+(\\alpha_{p-1}+\\alpha_p)x_{t-p+1}-(\\alpha_{p-1}+\\alpha_p)x_{t-p+2}-\\alpha_p\\Delta x_{t-p+1} \\\\ &amp;=\\cdots-(\\alpha_{p-1}+\\alpha_p)\\Delta x_{t-p+2}-\\alpha_p\\Delta x_{t-p+1}=\\cdots \\end{align*}\\] \\(\\Delta x_{t-p+i}=x_{t-p+i}-x_{t-p+i-1}\\) Recursive result: \\[\\begin{equation*} \\Delta x_t=\\alpha_0+\\gamma x_{t-1}+\\sum_{i=2}^p{c_i\\Delta x_{t-i+1}}+\\varepsilon_t \\end{equation*}\\] \\(\\gamma=-\\bigg(1-\\sum_{i=1}^p{\\alpha_i}\\bigg)\\) \\(c_i=\\sum_{j=i}^p{\\alpha_j}\\) 3.3 ACF (Autocorrelation Function) accounts for both direct and indirect effects of \\(x_{t-k}\\) on \\(x_t\\). \\[\\begin{align*} \\rho_k&amp;=\\text{Corr}[x_{t-k},x_t]\\\\ &amp;=\\frac{\\text{E}[(x_{t-k}-\\mu_{t-k})\\overline{(x_t-\\mu_t)}]}{\\sigma_{t-k}\\sigma_t} \\end{align*}\\] 3.4 PACF (Partial Autocorrelation Function) accounts for only direct effects of \\(x_{t-k}\\) on \\(x_t\\). \\[\\begin{equation*} \\phi_{ii}=\\frac{\\rho_i-\\sum_{j=1}^{i-1}{\\phi_{i-1,j}\\rho_{i-j}}}{1-\\sum_{j=1}^{i-1}{\\phi_{i-1,j}\\rho_j}} \\end{equation*}\\] \\(j\\)th order autoregression equation: \\[\\begin{equation*} x_t-\\mu=\\sum_{i=1}^j{\\phi_{ji}(x_{t-i}-\\mu)}+e_t \\end{equation*}\\] joint coefficients: \\[\\begin{equation*} \\phi_{ij}=\\phi_{i-1,j}-\\phi_{ii}\\phi_{i-1,i-j} \\end{equation*}\\] 3.5 Information Criteria Goal: to minimize information loss AIC (Akaike IC): \\[\\begin{equation*} \\text{AIC}=2k-2\\log{\\hat{L}} \\end{equation*}\\] BIC (Bayesian IC): \\[\\begin{equation*} \\text{BIC}=k\\log{n}-2\\log{\\hat{L}} \\end{equation*}\\] "]]
