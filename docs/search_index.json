[["index.html", "Time Series Analysis References", " Time Series Analysis Renyi Qu 2021/01/25 References Bollerslev, T. (1986). Generalized Autoregressive Conditional Heteroskedasticity. Journal of Econometrics. vol. 31. https://doi.org/10.1016/0304-4076(86)90063-1. Enders, W. (2015). Applied Econometric Time Series. JohnWiley &amp; Sons Inc. DOI:10.1198/tech.2004.s813. Engle, R.F. (1982). Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation. Econometrica. vol. 50, issue. 4. https://doi.org/10.2307/1912773. "],["m.html", "1 Mean Models 1.1 AR (Autoregressive model) 1.2 MA (Moving Average model) 1.3 ARMA (Autoregressive Moving Average model) 1.4 ARIMA (Autoregressive Integrated Moving Average model)", " 1 Mean Models 1.1 AR (Autoregressive model) 1.1.1 Model AR(p) - General: \\[\\begin{equation*} x_t=\\alpha_0+\\sum_{i=1}^{p}{\\alpha_i x_{t-i}}+\\varepsilon_t \\end{equation*}\\] \\(x_t\\): time series to be modeled \\(x_{t-i}\\): past time series values \\(p\\): lag limit \\(\\alpha_i\\): params to be estimated \\(\\varepsilon_t \\sim N(0,\\sigma^2)\\): i.i.d. white noise AR(p) - Markovian: \\[\\begin{align*} p(y_{1:T})&amp;=p(y_{1:p})\\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\\\ p(\\boldsymbol{y}|y_{1:p})&amp;=\\prod_{t=p+1}^{T}{p(y_t|y_{(t-p):(t-1)})}\\\\ &amp;=\\prod_{t=p+1}^{T}{N(y_t|\\boldsymbol{f}_t^\\prime\\boldsymbol{\\phi},v)}\\\\ &amp;=N(\\boldsymbol{y}|\\boldsymbol{F}^\\prime\\boldsymbol{\\phi},v\\boldsymbol{I}_n) \\end{align*}\\] \\(T=n+p\\) \\(\\phi=(\\phi_1,\\cdots,\\phi_p)\\) \\(\\boldsymbol{f}_t=(y_{t-1},\\cdots,y_{t-p})\\) \\(\\boldsymbol{F}=[\\boldsymbol{f}_T,\\cdots,\\boldsymbol{f}_{p+1}]\\) AR(p) - State-space: \\[\\begin{align*} y_t&amp;=\\boldsymbol{F}^\\prime\\boldsymbol{x}_t\\\\ \\boldsymbol{x}_t&amp;=\\boldsymbol{G}\\boldsymbol{x}_{t-1}+\\boldsymbol{\\omega}_t \\end{align*}\\] \\(\\boldsymbol{x}_t=(y_t,\\cdots,y_{t-p+1})\\): state vector \\(\\boldsymbol{\\omega}_t=(\\epsilon_t,0,\\cdots,0)\\): error vector \\(\\boldsymbol{F}=(1,0,\\cdots,0)\\) \\(\\boldsymbol{G}=\\begin{bmatrix}\\phi_1 &amp; \\phi_2 &amp; \\cdots &amp; \\phi_{p-1} &amp; \\phi_p \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; 0 &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0\\end{bmatrix}\\) AR(p) - Autoorrelation: \\[\\begin{align*} &amp;\\rho(h)-\\sum_{i=1}^{p}{\\phi_i\\rho(h-i)}=0\\\\ &amp;\\rho(h)=\\sum_{j=1}^{r}{\\alpha_j^hp_j(h)} \\end{align*}\\] \\(\\alpha_1,\\cdots,\\alpha_r\\): reciprocal roots of characteristic polynomial \\(m_1,\\cdots,m_r\\): root multiplicity (\\(\\sum_{i=1}^r{m_i}=p\\)) \\(p_j(h)\\): polynomial of degree \\(m_j-1\\) 1.1.2 Estimation LSE - AR(1): \\[\\begin{align*} \\hat{\\alpha}_{ols}&amp;=\\mathop{\\mathrm{argmin}}_{\\alpha}{\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}}\\\\ &amp;=\\bigg(\\sum_{t=1}^{T}{x_{t-1}^2}\\bigg)^{-1}\\bigg(\\sum_{t=1}^{T}{x_{t-1}x_t}\\bigg) \\end{align*}\\] MLE - AR(1): Conditional likelihood: \\[\\begin{equation*} \\mathcal{L}(\\boldsymbol{x|x_{-1}})=\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^T\\exp{\\bigg(-\\frac{1}{2\\sigma^2}\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}\\bigg)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} l(\\boldsymbol{x|x_{-1}})=-T\\log{\\sqrt{2\\pi}\\sigma}-\\frac{1}{2\\sigma^2}\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2} \\end{equation*}\\] Conditional ML estimator: \\[\\begin{align*} \\hat{\\alpha}_{ml}&amp;=\\mathop{\\mathrm{argmax}}_\\alpha{l(\\boldsymbol{x|x_{-1}})} \\\\ &amp;=\\mathop{\\mathrm{argmax}}_\\alpha{\\bigg(-\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}\\bigg)} \\\\ &amp;=\\mathop{\\mathrm{argmin}}_\\alpha{\\sum_{t=1}^{T}{(x_t-\\alpha x_{t-1})^2}} \\\\ \\end{align*}\\] 1.2 MA (Moving Average model) 1.2.1 Model MA(q): \\[\\begin{equation*} x_t=\\mu+\\sum_{i=1}^{q}{\\beta_i \\varepsilon_{t-i}}+\\varepsilon_t \\end{equation*}\\] \\(\\mu\\): mean of \\(x_t\\) \\(\\varepsilon_{t-i}\\): past error values \\(q\\): lag limit \\(\\beta_i\\): params to be estimated \\(\\varepsilon_t\\): unobservable white noise at the current time 1.2.2 Estimation LSE - MA(1): \\[\\begin{equation*} \\hat{\\beta}_{ols}=\\mathop{\\mathrm{argmin}}_\\beta{\\sum_{t=1}^T{(x_t-\\beta\\varepsilon_{t-1})^2}} \\end{equation*}\\] How to deal with unknown random disturbances: \\[\\begin{align*} &amp;\\varepsilon_1=x_1-\\beta\\varepsilon_0 \\\\ &amp;\\varepsilon_2=x_2-\\beta(x_1-\\beta\\varepsilon_0) \\\\ &amp;\\varepsilon_3=x_3-\\beta(x_2-\\beta(x_1-\\beta\\varepsilon_0)) \\\\ &amp;\\varepsilon_{t-1}=(-\\beta)^{t-1}\\varepsilon_0+\\sum_{k=0}^{t-2}{(-\\beta)^kx_{t-1-k}}\\end{align*}\\] Thus, we obtain the Nonlinear LS estimator: \\[\\begin{equation*} \\hat{\\beta}_{nls}=\\mathop{\\mathrm{argmin}}_{\\beta}{\\sum_{t=1}^{T}{\\bigg(x_t-\\beta\\sum_{k=0}^{t-2}{(-\\beta)^kx_{t-1-k}}\\bigg)^2}} \\end{equation*}\\] MLE - MA(1): How to deal with unknown random disturbances: \\[\\begin{align*} &amp;\\mathcal{L}(x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_1^2}{2\\sigma^2}\\bigg)} \\\\ &amp;\\mathcal{L}(x_2|x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{(x_2-\\beta\\varepsilon_1)^2}{2\\sigma^2}\\bigg)}=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_2^2}{2\\sigma^2}\\bigg)} \\\\ &amp;\\cdots \\\\ &amp;\\mathcal{L}(x_t|x_{t-1},\\cdots,x_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{(x_t-\\beta\\varepsilon_{t-1})^2}{2\\sigma^2}\\bigg)}\\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\bigg(-\\frac{\\varepsilon_t^2}{2\\sigma^2}\\bigg)} \\end{align*}\\] Conditional likelihood: \\[\\begin{equation*} \\mathcal{L}(\\boldsymbol{x|x_{-1}})=\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^T\\exp{\\bigg(-\\frac{1}{2\\sigma^2}\\sum_{t=1}^T{\\varepsilon_t^2}\\bigg)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} l(\\boldsymbol{x|x_{-1}})=-T\\log{\\sqrt{2\\pi}\\sigma}-\\frac{1}{2\\sigma^2}\\sum_{t=1}^T{\\varepsilon_t^2} \\end{equation*}\\] Conditional ML estimator: \\[\\begin{equation*} \\hat{\\beta}_{ml}=\\mathop{\\mathrm{argmax}}_\\beta{l(\\boldsymbol{x|x_{-1}})}=\\mathop{\\mathrm{argmin}}_\\beta{\\sum_{t=1}^T{\\varepsilon_t^2}} \\end{equation*}\\] 1.3 ARMA (Autoregressive Moving Average model) 1.3.1 Model ARMA(p,q) - General: \\[\\begin{equation*} x_t=\\alpha_0+\\sum_{i=1}^{p}{\\alpha_i x_{t-i}}+\\sum_{i=1}^{q}{\\beta_i \\varepsilon_{t-i}}+\\varepsilon_t \\end{equation*}\\] Explanatory variables: \\(x_{t-i}\\ \\&amp;\\ \\varepsilon_{t-i}\\) Parameters: \\(\\alpha_i\\ \\&amp;\\ \\beta_i\\) Hyperparameters: \\(p\\ \\&amp;\\ q\\) Random disturbance: \\(\\varepsilon_t\\) ARMA(p,q) - State-space: \\[\\begin{align*} y_t&amp;=\\boldsymbol{E}_m^\\prime\\boldsymbol{\\theta}_t\\\\ \\boldsymbol{\\theta}_t&amp;=\\boldsymbol{G}\\boldsymbol{\\theta}_{t-1}+\\boldsymbol{\\omega}_t \\end{align*}\\] \\(\\boldsymbol{E}_m=(1,0,\\cdots,0)\\), shape \\(m=\\max{(p,q+1)}\\) \\(\\boldsymbol{\\omega}_t=(1,\\theta_1,\\cdots,\\theta_{m-1})\\epsilon_t\\) \\(\\boldsymbol{G}=\\begin{bmatrix}\\phi_1 &amp; \\phi_2 &amp; \\cdots &amp; \\phi_{m-1} &amp; \\phi_m \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; 0 &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0\\end{bmatrix}\\) 1.3.2 Estimation LSE - ARMA(p,q): Minimize: \\[\\begin{equation*} S(\\boldsymbol{\\theta})=\\sum_{t=1}^{T}{\\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}} \\end{equation*}\\] \\(\\boldsymbol{\\theta}=(\\alpha_1,\\cdots,\\alpha_p,\\beta_1,\\cdots,\\beta_q)\\): parameter set Conditional LS: \\[\\begin{equation*} S_c(\\boldsymbol{\\theta})=\\sum_{t=p+1}^T{\\epsilon_t(\\boldsymbol{\\theta})^2} \\end{equation*}\\] \\(\\epsilon_t(\\boldsymbol{\\theta})=y_t-\\hat{y}_t\\) MLE - ARMA(p,q): Conditional likelihood: \\[\\begin{equation*} p(y_{1:T}|\\boldsymbol{\\theta},v)=\\prod_{t=1}^T{p(y_t|y_{1:(t-1)},\\boldsymbol{\\theta},v)} \\end{equation*}\\] Conditional log likelihood: \\[\\begin{equation*} \\log{p(y_{1:T}|\\boldsymbol{\\theta},v)}=-\\frac{T}{2}\\log{2\\pi v}-\\frac{1}{2}\\sum_{t=1}^T{\\Big(\\log{r_t^{t-1}}+\\frac{(y_t-y_t^{t-1})^2}{r_t^{t-1}}\\Big)} \\end{equation*}\\] \\(y_t^{t-1}=\\text{E}[y_t|y_{1:(t-1)}]\\) \\(vr_t^{t-1}=\\text{Var}[y_t|y_{1:(t-1)}]\\) Bayesian - ARMA(p,q): Likelihood based on parameters: \\[\\begin{equation*} p(y_{1:T}|\\boldsymbol{\\varphi})=\\big(\\frac{1}{\\sqrt{2\\pi v}}\\big)^T\\exp{\\bigg(-\\frac{1}{2v}\\sum_{t=1}^T{(y_t-\\mu_t)^2}\\bigg)} \\end{equation*}\\] \\(\\boldsymbol{\\varphi}=(\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v,\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) \\(\\boldsymbol{\\epsilon}_0=(\\epsilon_0,\\epsilon_{-1},\\cdots,\\epsilon_{1-q})\\) \\(\\mu_1=\\sum_{i=1}^p{\\alpha_iy_{1-i}}+\\sum_{i=1}^q{\\beta_i\\epsilon_{1-i}}\\) \\(\\mu_t=\\sum_{i=1}^p{\\alpha_iy_{t-i}}+\\sum_{i=1}^{t-1}{\\beta_i(y_{t-i}-\\mu_{t-i})}+\\sum_{i=1}^q{\\beta_i\\epsilon_{t-i}},t=2:q\\) \\(\\mu_t=\\sum_{i=1}^p{\\alpha_iy_{t-i}}+\\sum_{i=1}^{t-1}{\\beta_i(y_{t-i}-\\mu_{t-i})},t=q+1:T\\) Prior: \\[\\begin{equation*} \\pi(\\boldsymbol{\\varphi})=\\pi(\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)\\pi(v)\\pi(\\boldsymbol{\\alpha},\\boldsymbol{\\beta}) \\end{equation*}\\] \\(\\pi(\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)=N(\\boldsymbol{0},v\\Omega)\\) \\(\\pi(v)\\propto\\frac{1}{v}\\) \\(\\pi(\\boldsymbol{\\alpha},\\boldsymbol{\\beta})\\): uniform distribution \\(v\\Omega=\\text{Cov}[\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0]\\) Joint Posterior: \\[\\begin{equation*} p(\\boldsymbol{\\varphi}|y_{1:T})\\propto (v)^{-\\frac{T+2}{2}}\\exp{-\\frac{1}{2v}\\sum_{t=1}^T{(y_t-\\mu_t)^2}}\\times N((\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)|\\boldsymbol{0},v\\Omega) \\end{equation*}\\] MCMC: Sample \\((v|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) from inverse-gamma full conditional distribution: Sample \\((\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0|\\boldsymbol{\\alpha},\\boldsymbol{\\beta},v)\\) using a Metropolis step with Gaussian proposal distributions Sample \\((\\boldsymbol{\\alpha},\\boldsymbol{\\beta}|v,\\boldsymbol{x}_0,\\boldsymbol{\\epsilon}_0)\\) from PACF 1.4 ARIMA (Autoregressive Integrated Moving Average model) 1.4.1 Model Differencing: \\[\\begin{align*} &amp;x^{(1)}_t=x_t-x_{t-1}\\\\ &amp;x^{(2)}_t=x^{(1)}_t-x^{(1)}_{t-1}\\\\ &amp;\\cdots\\\\ &amp;x^{(d)}_t=x^{(d-1)}_t-x^{(d-1)}_{t-1} \\end{align*}\\] ARIMA(p,d,q): \\[\\begin{equation*} \\Phi(L)(1-L)^dx_t=\\Theta(L)\\varepsilon_t \\end{equation*}\\] AR operator: \\(\\Phi(L)=1-\\sum_{i=1}^{p}{\\alpha_iL^i}\\) MA operator: \\(\\Theta(L)=1+\\sum_{i=1}^{q}{\\beta_iL^i}\\) 1.4.2 Estimation LSE - ARIMA(p,1,q): Parameter Collection: \\(\\boldsymbol{\\theta}=(\\alpha_0,\\alpha_1,\\cdots,\\alpha_p,\\beta_1,\\cdots,\\beta_q)\\) Independent Variable Collection: \\(\\boldsymbol{\\omega}_t=(1,z_{t-1},\\cdots,z_{t-p},\\hat{\\varepsilon}_{t-1},\\cdots,\\hat{\\varepsilon}_{t-q})\\) : \\[\\begin{equation*} z_t=\\boldsymbol{\\theta}^T\\boldsymbol{\\omega}_t+\\varepsilon_t \\end{equation*}\\] \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}}_{ols}=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\theta}}{\\sum_{t=2}^{T}{(z_t-\\boldsymbol{\\theta}^T\\boldsymbol{\\omega}_t)^2}} \\end{equation*}\\] "],["v.html", "2 Volatility models 2.1 ARCH (Autoregressive Conditional Heteroskedasticity model) 2.2 GARCH (Generalized Autoregressive Conditional Heteroskedasticity model)", " 2 Volatility models 2.1 ARCH (Autoregressive Conditional Heteroskedasticity model) \\[\\begin{equation*} \\varepsilon_t=\\sigma_t\\zeta_t \\end{equation*}\\] \\(\\zeta_t\\): white noise following \\(N(0,1)\\) \\(\\sigma_t^2\\): variance of \\(\\varepsilon_t\\): \\[\\begin{equation*} \\sigma_t^2=\\alpha_0+\\sum_{i=1}^p{\\alpha_i\\varepsilon_{t-i}^2} \\end{equation*}\\] \\(\\alpha_0&gt;0, \\alpha_i\\geq0\\) 2.2 GARCH (Generalized Autoregressive Conditional Heteroskedasticity model) \\[\\begin{equation*} \\sigma_t^2=\\alpha_0+\\sum_{i=1}^p{\\alpha_i\\varepsilon_{t-i}^2}+\\sum_{i=1}^q{\\beta_j\\sigma_{t-j}^2} \\end{equation*}\\] 2.2.1 Estimation MLE: \\[\\begin{equation*} l_t(\\boldsymbol{\\theta},\\boldsymbol{\\varphi})=-\\frac{1}{2}\\log{2\\pi}-\\frac{1}{2}\\log{\\sigma_t^2}-\\frac{\\varepsilon_t^2}{2\\sigma_t^2} \\end{equation*}\\] FOC: \\[\\begin{align*} \\frac{\\partial l_t}{\\partial\\boldsymbol{\\theta}}&amp;=\\frac{\\varepsilon_t\\omega_t}{\\sigma_t^2}+\\frac{\\sigma_t^2}{2}\\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\theta}}\\bigg(\\frac{\\varepsilon_t^2}{\\sigma_t^2}-1\\bigg)=0 \\\\ \\frac{\\partial l_t}{\\partial\\boldsymbol{\\varphi}}&amp;=\\frac{1}{2\\sigma_t^2}\\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\varphi}}\\bigg(\\frac{\\varepsilon_t^2}{\\sigma_t^2}-1\\bigg)=0 \\end{align*}\\] Recursive algorithm: \\[\\begin{align*} \\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\theta}}&amp;=\\sum_{j=1}^q{\\beta_j\\frac{\\partial\\sigma_{t-j}^2}{\\partial\\boldsymbol{\\theta}}}-2\\sum_{i=1}^p{\\alpha_i\\boldsymbol{\\omega}_t\\varepsilon_{t-i}} \\\\ \\frac{\\partial\\sigma_t^2}{\\partial\\boldsymbol{\\varphi}}&amp;=\\sum_{j=1}^q{\\beta_j\\frac{\\partial\\sigma_{t-j}^2}{\\partial\\boldsymbol{\\varphi}}}+\\boldsymbol{\\epsilon}_t \\end{align*}\\] "],["cnn.html", "3 Convolutional Neural Networks 3.1 Basics of CNN 3.2 CNN Examples 3.3 Object Detection 3.4 Face Recognition", " 3 Convolutional Neural Networks 3.1 Basics of CNN Intuition of CNN CNN is mostly used in Computer Vision (image classification, object detection, neural style transfer, etc.) Input: images \\(\\rightarrow\\) volume of numerical values in the shape of width \\(\\times\\) height \\(\\times\\) color-scale (color-scale=3 \\(\\rightarrow\\) RGB; color-scale=1 \\(\\rightarrow\\) BW) In the gif above, the input shape is \\(5\\times5\\times3\\), meaning that the image is colored and the image size \\(5\\times5\\). The \\(7\\times7\\times3\\) results from padding, which will be discussed below. Convolution: For each color layer of the input image, we apply a 2d filter that scans through the layer in order. For each block that the filter scans, we multiply the corresponding filter value and the cell value, and we sum them up. We sum up the output values from all layers of the filter (and add a bias value to it) and output this value to the corresponding output cell. (If there are multiple filters, ) After the first filter finishes scanning, the next filter starts scanning and outputs into a new layer. In the gif above, Apply 2 filters of the shape \\(3\\times3\\times3\\). 1st filter - 1st layer - 1st block: \\[\\begin{equation} 0+0+0+0+0+0+0+(1\\times-1)+0=-1 \\end{equation}\\] 1st filter - 2nd layer - 1st block: \\[\\begin{equation} 0+0+0+0+(2\\times-1)+(1\\times1)+0+(2\\times1)+0=1 \\end{equation}\\] 1st filter - 3rd layer - 1st block: \\[\\begin{equation} 0+0+0+0+(2\\times1)+0+0+(1\\times-1)+0=1 \\end{equation}\\] Sum up + bias \\(\\rightarrow\\) 1st cell of 1st output layer \\[\\begin{equation} -1+1+1+1=2 \\end{equation}\\] Repeat till we finish scanning Edge Detection &amp; Filter Sample filters Gray Scale: 1 = lighter, 0 = gray, -1 = darker Notice that we dont really need to define any filter values. Instead, we are supposed to train the filter values. All the convolution operations above are just the same as the operations in ANN. Filters here correspond to \\(W\\) in ANN. Padding Problem: corner cells &amp; edge cells are detected much fewer times than the middle cells \\(\\rightarrow\\) info loss of corner &amp; edge Solution: pad the edges of the image with 0 cells (as shown in the gif above) Stride: the step size the filter takes (\\(s=2\\) in the gif above) General Formula of Convolution: \\[\\begin{equation} \\text{Output Size}=\\left\\lfloor\\frac{n+2p-f}{s}+1\\right\\rfloor\\times\\left\\lfloor\\frac{n+2p-f}{s}+1\\right\\rfloor \\end{equation}\\] \\(n\\times n\\): image size \\(f\\times f\\): filter size \\(p\\): padding \\(s\\): stride Floor: ignore the computation when the filter sweeps the region outside the image matrix CNN Layers: Convolution (CONV): as described above Pooling (POOL): to reduce #params &amp; computations (most common pooling size = \\(2\\times2\\)) Max Pooling Divide the matrix evenly into regions Take the max value in that region as output value Average Pooling Divide the matrix evenly into regions Take the average value of the cells in that region as output value Stochastic Pooling Divide the matrix evenly into regions Normalize each cell based on the regional sum: \\[\\begin{equation} p_i=\\frac{a_i}{\\sum_{k\\in R_j}{a_k}} \\end{equation}\\] Take a random cell based on multinomial distribution as output value Fully Connected (FC): to flatten the 2D/3D matrices into a single vector (each neuron is connected with all input values) 3.2 CNN Examples LeNet-5: LeNet-5 Digit Recognizer Layer Shape Total Size #params INPUT 32 x 32 x 3 3072 0 CONV1 (Layer 1) 28 x 28 x 6 4704 156 POOL1 (Layer 1) 14 x 14 x 6 1176 0 CONV2 (Layer 2) 10 x 10 x 16 1600 416 POOL2 (Layer 2) 5 x 5 x 16 400 0 FC3 (Layer 3) 120 x 1 120 48001 FC4 (Layer 4) 84 x 1 84 10081 Softmax 10 x 1 10 841 Calculation of #params for CONV: \\((f\\times f+1)\\times n_f\\) \\(f\\): filter size \\(+1\\): bias \\(n_f\\): #filter AlexNet: winner of 2012 ImageNet Large Scale Visual Recognition Challenge Layer Shape Total Size #params INPUT 227 x 227 x 3 154587 0 CONV1 (Layer 1) 55 x 55 x 96 290400 11712 POOL1 (Layer 1) 27 x 27 x 96 69984 0 CONV2 (Layer 2) 27 x 27 x 256 186624 6656 POOL2 (Layer 2) 13 x 13 x 256 43264 0 CONV3 (Layer 3) 13 x 13 x 384 64896 3840 CONV4 (Layer 3) 13 x 13 x 384 64896 3840 CONV5 (Layer 3) 13 x 13 x 256 43264 2560 POOL5 (Layer 3) 6 x 6 x 256 9216 0 FC5 (Flatten) 9216 x 1 9216 0 FC6 (Layer 4) 4096 x 1 4096 37748737 FC7 (Layer 5) 4096 x 1 4096 16777217 Softmax 1000 x 1 1000 4096000 Significantly bigger than LeNet-5 (60M params to be trained) Require multiple GPUs to speed the training up VGG: made by Visual Geometry Group from Oxford Too large: 138M params Inception ResNets Residual Block \\[\\begin{equation} a^{[l+2]}=g(z^{[l+2]}+a^{[l]}) \\end{equation}\\] Intuition: we add activation values from layer \\(l\\) to the activation in layer \\(l+2\\) Why ResNets? ResNets allow parametrization for the identity function \\(f(x)=x\\) ResNets are proven to be more effective than plain networks: ResNets add more complexity to the NN in a very simple way The idea of ResNets further inspired the development of RNN 1x1 Conv (i.e. Network in Network [NiN]) WHY??? This sounds like the stupidest idea ever!! Watch this. In a normal CNN layer like this, we need to do in total 210M calculations. However, if we add a 1x1 Conv layer in between, we only need to do in total 17M calculations. Therefore, 1x1 Conv is significantly more useful than what newbies expect. When we would like to keep the matrix size but reduce #layers, using 1x1 Conv can significantly reduce #computations needed, thus requiring less computing power. The Inception: We need to go deeper! Inception Module Inception Network Conv1D &amp; Conv3D: Although CNN (Conv2D) is undoubtedly most useful in Computer Vision, there are also some other forms of CNN used in other fields: Conv1D: e.g. text classification, heartbeat detection,  use a 1D filter to convolve a 1D input vector e.g. \\(14\\times1\\xrightarrow{5\\times1,16}10\\times16\\xrightarrow{5\\times16,32}6\\times32\\) However, this is almost never used since we have RNN Conv3D: e.g. CT scan,  use a 3D filter to convolve a 3D input cube e.g. \\(14\\times14\\times14\\times1\\xrightarrow{5\\times5\\times5\\times1,16}10\\times10\\times10\\times16\\xrightarrow{5\\times5\\times5\\times16,32}6\\times6\\times6\\times32\\) 3.3 Object Detection Object Localization \\(\\rightarrow\\) 1 obj; Detection \\(\\rightarrow\\) multiple objs. Bounding Box: to capture the obj in the img with a box Params: \\(b_x, b_y\\) = central point \\(b_h, b_w\\) = full height/width New target label (in place of image classification output): \\[\\begin{equation} y=\\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{equation}\\] \\(p_c\\): is there any object in this box? if \\(p_c=0\\), we ignore the remaining params \\(c_i\\): class label \\(i\\) (e.g. \\(c_1\\): cat, \\(c_2\\): dog, \\(c_3\\): bird, ) Landmark Detection: to capture the obj in the img with points Params: \\((l_{ix},l_{iy})\\) = each landmark point New target label: \\[\\begin{equation} y=\\begin{bmatrix} p_c \\\\ l_{1x} \\\\ l_{1y} \\\\ \\vdots \\\\ l_{nx} \\\\ l_{ny} \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{equation}\\] THE LABELS MUST BE CONSISTENT! Always start from the exact same location of the object! (e.g. if you start with the left corner of the left eye for one image, you should always start with the left corner of the left eye for all images.) #landmarks should be the same! I personally have a very awful experience with Landmark Detection. When the algorithms of object detection were not yet well-known in the IT industry, I worked on a project of digital screen defects detection in a Finnish company. Since digital screen defects are 1) black &amp; white 2) in very simple geometric shapes, the usage of bounding boxes could have significantly reduced the complexity of both data collection and NN model building. However, the team insisted to use landmark detection. Due to 1) that screen defects are unstructured 2) that the number of landmark points for two different screen defects can hardly be the same, the dataset was basically unusable, and none of the models we built could learn accurate patterns from it, leading to an unfortunate failure. I personally would argue that bounding box is much better than landmark detection in most practical cases. Sliding Window Apply a sliding window with a fixed size to scan every part of the img left-right and top-bottom (just like CONV), and feed each part to CNN In order to capture the same type of objects in different sizes and positions in the img, shrink the img (i.e. enlarge the sliding window) and scan again, and repeat. Problem: HUGE computational cost! Solution: (contemporary) Convert FC layer into CONV layer Share the former FC info with latter convolutions First run of the CNN. Second run of the same CNN with a bigger size of the same img (due to sliding window). Notice that the FC info from the first run is shared in the second run. Latter runs of the same CNN with bigger sizes of the same img (due to sliding window). Notice that the FC info from all previous runs is shared in this run, thus saving computation power and memories. Intersection over Union Is the purple box a good prediction of the car location? Intersection over Union is defined as: \\[\\begin{equation} \\text{IoU}=\\frac{\\text{area of intersection}}{\\text{area of union}} \\end{equation}\\] In this case, area of intersection is the intersection between the red and purple box, and area of union is the total area covered by the red and purple box. If \\(\\text{IoU}\\leq 0.5\\), then the prediction box is correct. (Other threshold values are also okay but 0.5 is conventional.) YOLO (You Only Look Once) Grids: divide the image into grids &amp; use each grid as a bounding box when \\(p_c=0\\), we ignore the entire grid \\(p_c=1\\) only when the central point of the object \\(\\in\\) the grid target output: \\(Y.\\text{shape}=n_{\\text{grid}}\\times n_{\\text{grid}}\\times y.\\text{length}\\) Non-Max Suppression: what happens when the grid is too small to capture the entire object? Discard all boxes with \\(p_c\\leq 0.6\\) Pick the box with the largest \\(p_c\\) as the prediction Discard any remaining box with \\(\\text{IoU}\\geq 0.5\\) with the prediction Repeat till there is only one box left. Anchor Boxes: what happens when two objects overlap? (e.g. a hot girl standing in front of a car) Predefine Anchor boxes for different objects Redefine the target value as a combination of Anchor 1 + Anchor 2 \\[\\begin{equation} y=\\begin{bmatrix} p_{c1} \\\\ \\vdots \\\\ p_{c2} \\\\ \\vdots \\end{bmatrix} \\end{equation}\\] Each object in the image is assigned to grid cell that contains objects central point &amp; anchor box for the grid cell with the highest \\(\\text{IoU}\\) General Procedure: Divide the images into grids and label the objects Train the CNN Get the prediction for each anchor box in each grid cell Get rid of low probability predictions Get final predictions through non-max suppression for each class R-CNN TO BE CONTINUED 3.4 Face Recognition Face Verification vs Face Recognition Verification Input image, name/ID Output whether the input image is that of the claimed person (1:1) Recognition Input image Output name/ID if the image is any of the \\(K\\) ppl in the database (1:K) Siamese Network One Shot Learning: learn a similarity function The major difference between normal image classification and face recognition is that we dont have enough training examples. Therefore, rather than learning image classification, we Calculate the degree of diff between the imgs as \\(d\\) If \\(d\\leq\\tau\\): same person; If \\(d&gt;\\tau\\): diff person Preparation &amp; Objective: Encode \\(x^{(i)}\\) as \\(f(x^{(i)})\\) (defined by the params of the NN) Compute \\(d(x^{(i)},x^{(j)})=\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) i.e. distance between the two encoding vectors if \\(x^{(i)},x^{(j)}\\) are the same person, \\(\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) is small if \\(x^{(i)},x^{(j)}\\) are different people, \\(\\left\\lVert{f(x^{(i)})-f(x^{(j)})}\\right\\lVert_ 2^2\\) is large Method 1: Triplet Loss Learning Objective: distinguish between Anchor image &amp; Positive/Negative images (i.e. A vs P / A vs N) Initial Objective: \\(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2 \\leq \\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2\\) Intuition: We want to make sure the difference of A vs P is smaller than the difference of A vs N, so that this Anchor image is classified as positive (i.e. recognized) Problem: \\(\\exists\\ &quot;0-0\\leq0&quot;\\), in which case we cant tell any difference Final Objective: \\(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2-\\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2+\\alpha\\leq0\\) Intuition: We apply a margin \\(\\alpha\\) to solve the problem and meanwhile make sure A vs N is significantly larger than A vs P Loss Function: \\[\\begin{equation} \\mathcal{L}(A,P,N)=\\max{(\\left\\lVert{f(A)-f(P)}\\right\\lVert_ 2^2-\\left\\lVert{f(A)-f(N)}\\right\\lVert_ 2^2+\\alpha, 0)} \\end{equation}\\] Intuition: As long as this thing is less than 0, the loss is 0 and thats a successful recognition! Training Process: Given 10k imgs of 1k ppl: use the 10k images to generate triplets \\(A^{(i)}, P^{(i)}, N^{(i)}\\) Make sure to have multiple imgs of the same person in the training set random choosing Choose triplets that are quite hard to train on Method 2: Binary Classification Learning Objective: Check if two imgs represent the same person or diff ppl \\(y=1\\): same person \\(y=0\\): diff ppl Training output: \\[\\begin{equation} \\hat{y}=\\sigma\\Bigg(\\sum_{k=1}^{128}{w_i \\Big|f(x^{(i)})_ k-f(x^{(j)})_ k\\Big|+b}\\Bigg) \\end{equation}\\] Precompute the output vectors \\(f(x^{(i)})\\ \\&amp;\\ f(x^{(j)})\\) so that you dont have to compute them again during each training process Neural Style Transfer Intuition: Content(C) + Style(S) = Generated Image(G) Combine Content image with Style image to Generate a brand new image Cost Function: \\[\\begin{equation} \\mathcal{J}(G)=\\alpha\\mathcal{J}_ \\text{content}(C,G)+\\beta\\mathcal{J}_ \\text{style}(S,G) \\end{equation}\\] \\(\\mathcal{J}\\): the diff between C/S and G \\(\\alpha,\\beta\\): weight params Style: correlation between activations across channels When there is some pattern in one patch, and there is another pattern that changes similarly in the other patch, they are correlated. e.g. vertical texture in one patch \\(\\leftrightarrow\\) orange color in another patch The more often they occur together, the more correlated they are. Content Cost Function: \\[\\begin{equation} \\mathcal{J}_ \\text{content}(C,G)=\\frac{1}{2}\\left\\lVert{a^{[l](C)}-a^{[1](G)}}\\right\\lVert^2 \\end{equation}\\] Use hidden layer \\(l\\) to compute content cost Use pre-trained CNN (e.g. VGG) If \\(a^{[l](C)}\\ \\&amp;\\ a^{[l](G)}\\) are similar, then both imgs have similar content Style Cost Function: \\[\\begin{equation} \\mathcal{J}_ \\text{style}(S,G)=\\sum_l{\\lambda^{[l]}\\mathcal{J}_ \\text{style}^{[l]}(S,G)} \\end{equation}\\] Style Cost per layer: \\[\\begin{equation} \\mathcal{J}^{[l]}_ \\text{style}(S,G)=\\frac{1}{(2n_h^{[l]}n_w^{[l]}n_c^{[l]})^2}\\left\\lVert{G^{[l](S)}-G^{[1](G)}}\\right\\lVert^2_F \\end{equation}\\] the first term is simply a normalization param Style Matrix: \\[\\begin{equation} G_{kk&#39;}^{[l]}=\\sum_{i=1}^{n_H^{[l]}}{\\sum_{j=1}^{n_W^{[l]}}{a_{i,j,k}^{[l]}\\cdot a_{i,j,k&#39;}^{[l]}}} \\end{equation}\\] \\(a_{i,j,k}^{[l]}\\): activation at height \\(i\\), width \\(j\\), channel \\(k\\) \\(G^{[l]}.\\text{shape}=n_c^{[l]}\\times n_c^{[l]}\\) Intuition: sum up the multiplication of the two activations on the same cell in two different channels Training Process: Intialize \\(G\\) randomly (e.g. 100 x 100 x 3) Use GD to minimize \\(\\mathcal{J}(G)\\): \\(G := G-\\frac{\\partial{\\mathcal{J}(G)}}{\\partial{G}}\\) "]]
