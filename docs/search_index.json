[["index.html", "Machine Learning References", " Machine Learning Renyi Qu 2021/01/26 References ColumbiaX. Artificial Intelligence MicroMasters Program. edX Inc. Andrew Ng. CS229 - Machine Learning. Stanford University. Mitchell, T. 10-701 - Machine Learning. Carnegie Mellon University. "],["reg.html", "1 Regression 1.1 Linear Regression 1.2 Polynomial Regression 1.3 Locally Weighted Linear Regression 1.4 Ridge Regression 1.5 Lasso Regression 1.6 GLM", " 1 Regression 1.1 Linear Regression Problem Setting Data: Observed pairs \\((x,y)\\), where \\(x\\in\\mathbb{R}^{n+1}\\) (input) &amp; \\(y\\in\\mathbb{R}\\) (output) Goal: Find a linear function of the unknown \\(w\\)s: \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\ \\ \\text{s.t.}\\ \\ \\forall\\ (x,y): y\\approx f(x,w)\\) Model \\[\\begin{align} \\hat{y}_ i&amp;=\\sum_{j=0}^{n}w_jx_{ij} \\\\ \\\\ \\hat{y}&amp;=Xw \\\\ \\\\ \\begin{bmatrix} \\hat{y}_ 1 \\\\ \\vdots \\\\ \\hat{y}_ m \\end{bmatrix}&amp;= \\begin{bmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{m1} &amp; \\cdots &amp; x_{mn} \\\\ \\end{bmatrix}\\begin{bmatrix} w_0 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\end{align}\\] \\(x_{ij}\\): the \\(j\\)th feature in the \\(i\\)th observation \\(\\hat{y}_ i\\): the model prediction for the \\(i\\)th observation \\(w_j\\): the parameter for the \\(j\\)th feature \\(m\\): #observations \\(n\\): #features Learning Aim: find the optimal \\(w\\) that minimizes a loss function (i.e. cost function) Loss Function: OLS [Ordinary Least Squares] \\[\\begin{equation*} \\mathcal{L}(w)=\\sum_{i=1}^{m}(\\hat{y}_ i-y_i)^2 \\end{equation*}\\] Assumption (i.e. requirement): \\(m &gt; &gt; n\\) Minimization Method 1: Gradient Descent (the practical solution) \\[\\begin{equation} w_j := w_j-\\alpha\\frac{\\partial\\mathcal{L}(w)}{\\partial w_j} \\end{equation}\\] \\(\\alpha\\): learning rate \\(\\frac{\\partial\\mathcal{L}(w)}{\\partial w_j}\\): gradient Stochastic GD (using 1 training observation for each GD step) \\[\\begin{equation} w_j := w_j-\\alpha(\\hat{y}_ i-y_i)x_{ij} \\end{equation}\\] Mini-batch GD (using mini-batches of size \\(m&#39;\\) for each GD step) \\[\\begin{equation*} w_j := w_j-\\alpha\\sum_{i=1}^{m&#39;}(\\hat{y}_ i-y_i)x_{ij} \\end{equation*}\\] Batch GD (LMS) (using the whole training set for each GD step) \\[\\begin{equation} w_j := w_j-\\alpha\\sum_{i=1}^{m}(\\hat{y}_ i-y_i)x_{ij} \\end{equation}\\] Extra: Newtons Method Newtons formula \\[\\begin{equation} w := w-\\frac{f(w)}{f&#39;(w)} \\end{equation}\\] Newtons Method in GD \\[\\begin{equation} w := w-H^{-1}\\nabla_w\\mathcal{L}(w) \\end{equation}\\] where \\(H\\) is Hessian Matrix: \\[\\begin{equation} H_{ij}=\\frac{\\partial^2\\mathcal{L}(w)}{\\partial w_i \\partial w_j} \\end{equation}\\] Newton vs normal GD YES: faster convergence, fewer iterations NO: expensive computing (inverse of a matrix) Minimization Method 2: Normal Equation (the exact solution) \\[\\begin{equation*} w_{\\text{LS}}=(X^TX)^{-1}X^Ty\\ \\Longleftrightarrow\\ w_{\\text{LS}}=\\Big(\\sum_{i=1}^m{x_ix_i^T}\\Big)^{-1}\\Big(\\sum_{i=1}^m{y_ix_i}\\Big) \\end{equation*}\\] Derivation (matrix) \\[\\begin{align} \\DeclareMathOperator{\\Tr}{tr} \\nabla_w\\mathcal{L}(w)&amp;=\\nabla_w(Xw-y)^T(Xw-y) \\\\ &amp;=\\nabla_w\\Tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) \\\\ &amp;=\\nabla_w(\\Tr(w^TX^TXw)-2\\Tr(y^TXw)) \\\\ &amp;=2X^TXw-2X^Ty \\\\ &amp;\\Rightarrow X^TXw-X^Ty=0 \\end{align}\\] Derivation (vector) \\[\\begin{align} \\nabla_w\\mathcal{L}(w)&amp;=\\sum_{i=1}^m{\\nabla_w(w^Tx_ix_i^Tw-2w^Tx_iy_i+y_i^2)} \\\\ &amp;=-\\sum_{i=1}^m{2y_ix_i}+\\Big(\\sum_{i=1}^m{2x_ix_i^T}\\Big)w \\\\ &amp;\\Rightarrow \\Big(\\sum_{i=1}^m{x_ix_i^T}\\Big)w-\\sum_{i=1}^m{y_ix_i}=0 \\end{align}\\] GD vs Normal Equation GD Normal Equation Advantage faster computingless computing power required the exact solution Disadvantage hard to reach the exact solution \\((X^TX)^{-1}\\) must exist(i.e. \\((X^TX)^{-1}\\) must be full rank) Full rank: when the \\(m\\times n\\) matrix \\(X\\) has \\(\\geq n\\) linearly independent rows (i.e. any point in \\(\\mathbb{R}^n\\) can be reached by a weighted combination of \\(n\\) rows of \\(X\\)) Probabilistic Interpretation Probabilistic Model: Gaussian \\[\\begin{equation} p(y_i|x_i,w)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2}} \\end{equation}\\] \\(y_i=w^Tx_i+\\epsilon_i\\) \\(\\epsilon_i\\sim N(0,\\sigma)\\) Likelihood Function \\[\\begin{equation} L(w)=\\prod_{i=1}^{m}p(y_i|x_i,w)=\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2}} \\end{equation}\\] Log Likelihoood \\[\\begin{align} \\mathcal{l}(w)&amp;=\\log{L(w)} \\\\ &amp;=\\log{\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2}}} \\\\ &amp;=\\sum_{i=1}^{m}\\log{\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2}}} \\\\ &amp;=m\\log{\\frac{1}{\\sqrt{2\\pi}\\sigma}}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{m}(y_i-w^Tx_i)^2 \\end{align}\\] Why log? log = monotonic &amp; increasing on \\([0,1]\\rightarrow\\) \\[\\mathop{\\arg\\max}_ {w}L(w)=\\mathop{\\arg\\max}_ {w}\\log{L(w)}\\] log simplifies calculation (especially &amp; obviously for \\(\\prod\\)) MLE (Maximum Likelihood Estimation) \\[\\begin{align} \\mathop{\\arg\\max}_ {w}\\mathcal{l}(w)&amp;=\\mathop{\\arg\\max}_ {w}(m\\log{\\frac{1}{\\sqrt{2\\pi}\\sigma}}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\\\ &amp;=\\mathop{\\arg\\max}_ {w}(-\\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\\\ &amp;=\\mathop{\\arg\\min}_ {w}\\sum_{i=1}^{m}(y_i-w^Tx_i)^2 \\\\ &amp;=\\mathop{\\arg\\min}_ {w}\\|{y-Xw}\\|^2 \\end{align}\\] \\(\\Rightarrow\\) Least Squares &amp; Maximum Likelihood share the exact same solution. Expected Value: \\[\\begin{align} \\mathbb{E}[w_{ML}]&amp;=\\mathbb{E}[(X^TX)^{-1}X^Ty] \\\\ &amp;=(X^TX)^{-1}X^TXw \\\\ &amp;=w \\end{align}\\] Variance: \\[\\begin{align} \\text{Var}[w_{ML}]&amp;=\\mathbb{E}[(w_{ML}-\\mathbb{E}[w_{ML}])(w_{ML}-\\mathbb{E}[w_{ML}])^T] \\\\ &amp;=\\mathbb{E}[w_{ML}w_{ML}^T]-\\mathbb{E}[w_{ML}]\\mathbb{E}[w_{ML}]^T \\\\ &amp;=(X^TX)^{-1}X^T\\mathbb{E}[yy^T]X(X^TX)^{-1}-ww^T \\\\ &amp;=(X^TX)^{-1}X^T(\\sigma^2I+Xww^TX^T)X(X^TX)^{-1}-ww^T\\ \\ (1) \\\\ &amp;=\\sigma^2(X^TX)^{-1} \\\\ \\end{align}\\] \\((1)\\): \\[\\begin{align} \\sigma=\\text{Var}[y]&amp;=\\mathbb{E}[(y-\\mu)(y-\\mu)^T] \\\\ &amp;=\\mathbb{E}[yy^T]-2\\mu\\mu^T+\\mu\\mu^T \\\\ \\Rightarrow \\mathbb{E}[yy^T]&amp;=\\sigma+\\mu\\mu^T \\\\ \\end{align}\\] Summary: Assumption: Gaussian - \\(y\\ ~\\ N(Xw, \\sigma^2I)\\) Expected Value: \\(\\mathbb{E}[w_{ML}]=w\\) Variance: \\(\\text{Var}[w_{ML}]=\\sigma^2(X^TX)^{-1}\\) Problem: Notice how \\(w_{ML}\\) becomes huge when our variance \\(\\sigma^2(X^TX)^{-1}\\) is too large. Regularization: Intuition: in order to prevent the problem above, we want to constrain our model parameters \\(w\\): \\[\\begin{equation} w_{op}=\\mathop{\\arg\\min}_ {w}\\|{y-Xw}\\|^2+\\lambda g(w) \\end{equation}\\] \\(\\lambda&gt;0\\): regularization parameter \\(g(w)&gt;0\\): penalty function Sample Regularizations: Ridge Regression, LASSO Regression,  1.2 Polynomial Regression Polynomial Regression \\(\\in\\) Linear Regression (\\(f\\) = a linear function of unknown parameters \\(w\\)) Different Preprocessing: \\[\\begin{equation} X=\\begin{bmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1n} &amp; x_{11}^2 &amp; \\cdots &amp; x_{1n}^p \\\\ \\vdots &amp; &amp; \\vdots &amp; &amp; &amp; \\vdots &amp; \\\\ 1 &amp; x_{m1} &amp; \\cdots &amp; x_{mn} &amp; x_{m1}^2 &amp; \\cdots &amp; x_{mn}^p \\\\ \\end{bmatrix} \\end{equation}\\] with the width of \\(p\\times n+1\\). Everything else is exactly the same as linear regression. Sample models: 3rd order with 1 feature: \\(y_i=w_0+w_1x_i+w_2x_i^2+w_3x_i^3\\) 2nd order with 2 features: \\(y_i=w_0+w_1x_{i1}+w_2x_{i2}+w_3x_{i1}^2+w_4x_{i2}^2\\) Further Extensions: we can generalize our linear regression model as: \\[\\begin{equation} \\hat{y}_ i\\approx f(x_i,w)=\\sum_{s=1}^S{g_s(x_i)w_s} \\end{equation}\\] where \\(g_s(x_i)\\) can be any function of \\(x_i\\), such as \\(e^{x_{ij}},\\ \\log{x_{ij}},\\ ...\\). Everything else is still the same as linear regression. 1.3 Locally Weighted Linear Regression Problem Setting Underfitting: the model barely fits the data points. One single line is usually not enough to capture the pattern of \\(x\\ \\&amp;\\ y\\). In order to get a better fit, we add more polynomial features (\\(x^j\\)) to the original model: Overfitting: the model fits the given data points too well that it cannot be used on other data points. When we add too much (e.g. \\(y=\\sum_{j=0}^{9}w_jx^j\\)), the model captures the pattern of the given data points \\((x_i,y_i)\\) too much that it cannot perform well on new data points. Intuition: When we would like to estimate \\(y\\) at a certain \\(x\\), instead of applying the original LinReg, we take a subset of data points \\((x_i,y_i)\\) around \\(x\\) and try to do LinReg on that subset only so that we can get a more accurate estimation. Model: Weighted LS Original LinReg \\[\\begin{equation} w\\leftarrow\\mathop{\\arg\\min}_ {w}\\sum_{i=1}^{m}(y_i-w^Tx_i)^2 \\end{equation}\\] We find the \\(w\\) that minimizes the cost function (maximizes the likelihood function) so that our model is optimized to fit the data. LWR \\[\\begin{equation} w\\leftarrow\\mathop{\\arg\\min}_ {w}\\sum_{i=1}^{m}e^{-\\frac{(x_i-x)^2}{2\\tau^2}}\\cdot(y_i-w^Tx_i)^2 \\end{equation}\\] We add the weight function \\(\\mathcal{W}_ i=e^{-\\frac{(x_i-x)^2}{2\\tau^2}}\\) to the OLS, where Numerator: \\[\\begin{align} &amp;\\text{If}\\ |x_i-x|=\\text{small} \\longrightarrow W_i\\approx 1 \\\\ &amp;\\text{If}\\ |x_i-x|=\\text{large} \\longrightarrow W_i\\approx 0 \\end{align}\\] Bandwidth Parameter: \\(\\tau\\) (how fast the weight of \\(x_i\\) falls off the query point \\(x\\)) \\[\\begin{align} &amp;\\text{When}\\ \\tau &gt; &gt; 1, \\text{LWR} \\approx \\text{LinReg} \\\\ &amp;\\text{When}\\ \\tau &lt; &lt; 1, \\text{LWR} \\rightarrow \\text{overfitting} \\end{align}\\] Exact Solution: \\[\\begin{align} \\DeclareMathOperator{\\Tr}{tr} \\nabla_w\\mathcal{L}(w)&amp;=\\nabla_w \\mathcal{W}(Xw-y)^T(Xw-y) \\\\ &amp;\\Rightarrow X^T\\mathcal{W}Xw-X^T\\mathcal{W}y=0 \\\\ \\Rightarrow w&amp;=(X^T\\mathcal{W}X)^{-1}X^T\\mathcal{W}y \\end{align}\\] 1.4 Ridge Regression Problem Setting The OLS LinReg method gives us an accurate expected value: \\(\\mathbb{E}[w_{ML}]=w\\). However, the variance \\(\\text{Var}[w_{ML}]=\\sigma^2(X^TX)^{-1}\\) could be too large that it ruins our model parameters. Ridge Regression \\(\\in\\) regularization methods Model \\[\\begin{equation} w_{RR}=\\mathop{\\arg\\min}_ {w}\\|{y-Xw}\\|^2+\\lambda\\|w\\|^2_2 \\end{equation}\\] \\(\\lambda\\): regularization parameter: \\[\\begin{align} &amp;\\text{If}\\ \\lambda\\rightarrow0\\ \\ \\Longrightarrow w_{RR}\\rightarrow w_{LS} \\\\ &amp;\\text{If}\\ \\lambda\\rightarrow\\infty \\Longrightarrow w_{RR}\\rightarrow \\bf{0} \\end{align}\\] \\(g(w)=\\\\|w\\\\|^2_2=w^Tw\\): L2 penalty function Solution \\[\\begin{align} \\mathcal{L}&amp;=(y-Xw)^T(y-Xw)+\\lambda w^Tw \\\\ \\nabla_w\\mathcal{L}&amp;=-2X^Ty+2X^TXw+2\\lambda w=0 \\\\ \\Rightarrow w_{RR}&amp;=(X^TX+\\lambda I)^{-1}X^Ty \\end{align}\\] Data Preprocessing: Standardization \\(y\\): \\[\\begin{equation} y\\leftarrow y-\\frac{1}{m}\\sum_{i=1}^m{y_i} \\end{equation}\\] \\(x\\): \\[\\begin{equation} x_{ij}\\leftarrow \\frac{x_{ij}-\\bar{x}_ j}{\\sqrt{\\frac{1}{m}\\sum_{i=1}^m{(x_{ij}-\\bar{x}_ j)^2}}} \\end{equation}\\] Singular Value Decomposition Definition: We can write any \\(n\\times d\\ (n&gt;d)\\) matrix \\(X\\) as \\(X=USV^T\\). \\(U\\): left singular vectors \\((m\\times r)\\) orthonormal in cols (i.e. \\(U^TU=I\\)) \\(S\\): singular values \\((r\\times r)\\) non-negative diagonal (i.e. \\(S_{ii}\\geq0, S_{ij}=0\\ \\forall i\\neq j\\)) sorted in decreasing order (i.e. \\(\\sigma_1\\geq\\sigma_2\\geq\\cdots\\geq0\\)) \\(V\\): right singular vectors \\((n\\times r)\\) orthonormal (i.e. \\(V^TV=VV^T=I\\)) \\(m\\): #samples \\(n\\): #features \\(r\\): #concepts \\((r=\\text{rank}(X))\\) \\(\\sigma_i\\): the strength of the \\(i\\)th concept Properties: \\[\\begin{align} X^TX&amp;=VS^2V^T \\\\ XX^T&amp;=US^2U^T \\\\ \\text{If}\\ \\forall i: S_{ii}\\neq0 &amp;\\Rightarrow (X^TX)^{-1}=VS^{-2}V^T \\end{align}\\] Intuition: \\[\\begin{equation} X=USV^T=\\sum{\\sigma_i\\bf{u_i\\times v_i^T}} \\end{equation}\\] Why do we need this? Whats the practical use of this?? As an example, suppose we would like to analyze a dataset of the relationship between Users &amp; Movies, in which: Each row = a user Each col = a movie Each entry \\(X_{ij}\\) = the rating of movie \\(j\\) from user \\(i\\) (0=unwatched, 1=hate, 5=love) And here is the situation: cited from Stanfords Mining Massive Datasets \\(U=\\) User-to-Concept similarity matrix \\(U\\[:,1]=\\) Sci-fi concept of users \\(U\\[:,2]=\\) Romance concept of users \\(S=\\) Strength of Concept matrix \\(S\\[1,1]=\\) Strength of Sci-fi concept \\(S\\[2,2]=\\) Strength of Romance concept \\(\\because S\\[3,3]\\) is very small \\(\\therefore\\) we can ignore this concept and also ignore \\(U\\[:,3]\\) and \\(V^T\\[3,:]\\). \\(V^T=\\) Movie-to-Concept similarity matrix \\(V^T\\[1,1:3]=\\) Sci-fi concept of the Sci-fi movies \\(V^T\\[2,4:5]=\\) Romance concept of the Romance movies Calculation of SVD: \\(X^TX=VS^2V^T\\Rightarrow\\) calculate \\(V,S^2\\) \\(S^2\\ni\\) eigenvalues \\(V\\ni\\) eigenvectors \\(XV=US^2\\Rightarrow\\) calculate \\(U\\) GG. Ridge Regression vs Least Squares LinReg \\[\\begin{equation} w_{\\text{LS}}=(X^TX)^{-1}X^Ty\\ \\Leftrightarrow\\ w_{\\text{RR}}=(\\lambda I+X^TX)^{-1}X^Ty \\end{equation}\\] Problems with LS: \\(\\text{Var}\\[w_{ML}]=\\sigma^2(X^TX)^{-1}=\\sigma^2VS^{-2}V^T\\) When \\(S_{ii}\\) is very small for some values of \\(i\\), \\(\\text{Var}\\[w_{ML}]\\) is very large. \\(y_{\\text{new}}=x_{\\text{new}}^Tw_{LS}=x_{\\text{new}}^T(X^TX)^{-1}X^Ty=x_{\\text{new}}^TVS^{-1}U^Ty\\) When \\(S^{-1}\\) has very large values, our prediction will be very unstable. LS = a special case of RR: \\[\\begin{align} w_{\\text{RR}}&amp;=(\\lambda I+X^TX)^{-1}X^Ty \\\\ &amp;=(\\lambda I+X^TX)^{-1}(X^TX)(X^TX)^{-1}X^Ty \\\\ &amp;=[(X^TX)(\\lambda(X^TX)^{-1}+I)]^{-1}(X^TX)w_{\\text{LS}} \\\\ &amp;=(\\lambda(X^TX)^{-1}+I)^{-1}w_{\\text{LS}} \\\\ &amp;=(\\lambda VS^{-2}V^T+I)^{-1}w_{\\text{LS}} \\\\ &amp;=V(\\lambda S^{-2}+I)^{-1}V^Tw_{\\text{LS}}\\ \\ \\ \\ \\ \\ \\ \\ \\ |\\ \\ \\ VV^T=I\\\\ &amp;:=VMV^Tw_{\\text{LS}} \\end{align}\\] where \\(M=(\\lambda S^{-2}+I)^{-1}\\) is a diagonal matrix with \\(M_{ii}=\\frac{S_{ii}^2}{\\lambda+S_{ii}^2}\\), \\[\\begin{align} w_{\\text{RR}}&amp;:=VMV^Tw_{\\text{LS}} \\\\ &amp;=V(\\lambda S^{-2}+I)^{-1}V^T(VS^{-1}U^Ty) \\\\ &amp;=VS^{-1}_ \\lambda U^Ty \\\\ \\end{align}\\] where \\(S_\\lambda^{-1}\\) is a diagonal matrix with \\(S_{ii}=\\frac{S_{ii}}{\\lambda+S_{ii}^2}\\). Therefore, we get another clearer expression of the relationship between RR and LS: \\[\\begin{equation} w_{\\text{LS}}=VS^{-1}U^Ty\\ \\Leftrightarrow\\ w_{\\text{RR}}=VS_\\lambda^{-1}U^Ty \\end{equation}\\] And \\(w_{LS}\\) is simply a special case of \\(w_{RR}\\) where \\(\\lambda=0\\). RR = a special case of LS: If we do some preprocessing to our model \\(\\hat{y}\\approx\\hat{X}w\\): \\[\\begin{equation}\\begin{bmatrix} y \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\approx\\begin{bmatrix} - &amp; X &amp; - \\\\ \\sqrt{\\lambda} &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; \\sqrt{\\lambda} \\end{bmatrix}\\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\\end{equation}\\] Now we have the exact same loss function: \\[\\begin{equation} (\\hat{y}-\\hat{X}w)^T(\\hat{y}-\\hat{X}w)=\\|y-Xw\\|^2+\\lambda\\|w\\|^2 \\end{equation}\\] Probabilistic Interpretation Expected Value: \\[\\begin{equation} \\mathbb{E}[w_{\\text{RR}}]=(\\lambda I+X^TX)^{-1}X^TXw \\end{equation}\\] Variance: \\[\\begin{align} \\text{Var}[w_{\\text{RR}}]&amp;=\\mathbb{E}[w_{\\text{RR}}w_{\\text{RR}}^T]-\\mathbb{E}[w_{\\text{RR}}]\\mathbb{E}[w_{\\text{RR}}]^T \\\\ &amp;=(\\lambda I+X^TX)^{-1}X^T\\mathbb{E}[yy^T]X(\\lambda I+X^TX)^{-1^T} \\\\ &amp;\\ \\ \\ \\ \\ -(\\lambda I+X^TX)^{-1}X^TXww^TX^TX(\\lambda I+X^TX)^{-1^T} \\\\ &amp;=(\\lambda I+X^TX)^{-1}X^T(\\sigma^2I)X(\\lambda I+X^TX)^{-1^T}\\ \\ (1) \\\\ &amp;=\\sigma^2Z(X^TX)^{-1}Z^T \\\\ \\end{align}\\] where \\(Z=(I+\\lambda(X^TX)^{-1})^{-1}\\). See more info 1.4.1 Bias-Variance Trade-off Ridge vs LS: LS Ridge Expected value \\(\\mathbb{E}\\[w_{\\text{LS}}]=w\\) \\(\\mathbb{E}\\[w_{\\text{RR}}]=(\\lambda I+X^TX)^{-1}X^TXw\\) Variance \\(\\text{Var}\\[w_{\\text{LS}}]=\\sigma^2(X^TX)^{-1}\\) \\(\\text{Var}\\[w_{\\text{LS}}]=\\sigma^2Z(X^TX)^{-1}Z^T\\) The distribution of \\(w_{\\text{RR}}\\) is not centered at \\(w\\), but the variance gets much smaller. 1.5 Lasso Regression Everything is the same as Ridge Regression except the model: \\[\\begin{equation} w_{\\text{lasso}}=\\mathop{\\arg\\min}_ {w}\\|{y-Xw}\\|^2+\\lambda\\|w\\|_ 1 \\end{equation}\\] \\(g(w)=\\\\|w\\\\|_ 1=\\|w\\|\\): L1 penalty function Solution: we are yet able to find a solution to the Multivariate LASSO because of the absolute value. 1.6 GLM What are GLMs? Remember the two models we had in the last post? Regression: \\(p(y|x,w)\\sim N(\\mu,\\sigma^2)\\) Classification: \\(p(y|x,w)\\sim \\text{Bernoulli}(\\phi)\\) They belong to GLM, a collection of models that can be applied to Supervised Learning problems. We will show more examples of GLMs in this markdown. Exponential Family \\[\\begin{equation} p(y,\\eta)=b(y)\\cdot e^{\\eta^TT(y)-a(\\eta)} \\end{equation}\\] \\(\\eta\\): natural parameter (i.e. canonical parameter) different \\(\\eta \\rightarrow\\) different distributions within the family \\(T(y)\\): sufficient statistic (usually, \\(T(y)=y\\)) \\(a(\\eta)\\): log partition function \\(e^{-a(\\eta)}\\): normalization constant (to ensure that \\(\\int{p(y,\\eta)dy}=1\\)) \\(T,a,b\\): fixed choice that defines a family of distributions parametrized by \\(\\eta\\) - Bernoulli Distribution (Classification) $$\\begin{align} p(y|\\phi)&amp;=\\phi^y(1-\\phi)^{1-y} \\\\ &amp;=e^{y\\log{\\phi}+(1-y)\\log{(1-\\phi)}} \\\\ &amp;=e^{y\\log{\\frac{\\phi}{1-\\phi}}+\\log{(1-\\phi)}} \\\\ \\end{align}$$ 1. $\\eta=\\log{\\frac{\\phi}{1-\\phi}}\\Leftrightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ 2. $T(y)=y$ 3. $a(\\eta)=\\log{(1+e^\\eta)}$ 4. $b(y)=1$ - Gaussian Distribution (Regression) $$\\begin{align} p(y|\\mu,\\sigma^2)&amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}(y-\\mu)^2} \\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{\\mu}{\\sigma^2}y-\\frac{1}{2\\sigma^2}y^2-\\frac{1}{2\\sigma^2}\\mu^2-\\log{\\sigma}} \\end{align}$$ 1. $\\eta=\\begin{bmatrix} \\frac{\\mu}{\\sigma^2} ; \\frac{-1}{2\\sigma^2} \\end{bmatrix}$ 2. $T(y)=\\begin{bmatrix} y; y^2 \\end{bmatrix}$ 3. $a(\\eta)=\\frac{1}{2\\sigma^2}\\mu^2-\\log{\\sigma}=-\\frac{\\eta_1^2}{4\\eta_2}-\\frac{1}{2}\\log{(-2\\eta_2)}$ 4. $b(y)=\\frac{1}{\\sqrt{2\\pi}}$ - Poisson Distribution (count-data) $$\\begin{align} p(y|\\lambda)&amp;=\\frac{\\lambda^ye^{-\\lambda}}{y!}\\\\ &amp;=\\frac{1}{y!}e^{y\\log{\\lambda}-\\lambda} \\end{align}$$ 1. $\\eta=\\log{\\lambda}$ 2. $T(y)=y$ 3. $a(\\eta)=e^\\eta$ 4. $b(y)=\\frac{1}{y!}$ - Gamma Distribution (continuous non-negative random variables) $$\\begin{align} p(y|\\lambda,a)&amp;=\\frac{\\lambda^ay^{a-1}e^{-\\lambda y}}{\\Gamma(a)}\\\\ &amp;=\\frac{y^{a-1}}{\\Gamma(a)}e^{-\\lambda y+a\\log{\\lambda}} \\end{align}$$ 1. $\\eta=-\\lambda$ 2. $T(y)=y$ 3. $a(\\eta)=-a\\log{(-\\eta)}$ 4. $b(y)=\\frac{y^{a-1}}{\\Gamma(a)}$ - Beta Distribution (distribution of probabilities) $$\\begin{align} p(y|\\alpha,\\beta)&amp;=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}y^{\\alpha-1}(1-y)^{\\beta-1} \\\\ &amp;=\\frac{(1-y)^\\beta}{y(1-y)\\Gamma(\\beta)}e^{\\alpha\\log{y}- \\log{\\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+\\beta)}}} \\\\ &amp;=\\frac{y^\\alpha}{y(1-y)\\Gamma(\\alpha)}e^{\\beta\\log{(1-y)}- \\log{\\frac{\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}}} \\end{align}$$ 1. $\\eta=\\alpha\\ \\text{or}\\ \\beta$ 2. $T(y)=\\log{y}\\ \\text{or}\\ \\log{(1-y)}$ 3. $a(\\eta)=\\log{\\frac{\\Gamma(\\eta)}{\\Gamma(\\alpha+\\beta)}}$ 4. $b(y)=\\frac{(1-y)^\\beta}{y(1-y)\\Gamma(\\beta)}\\ \\text{or}\\ \\frac{y^\\alpha}{y(1-y)\\Gamma(\\alpha)}$ - Dirichlet Distribution (multivariate beta) $$\\begin{align} p(y|\\alpha)&amp;=\\frac{\\Gamma(\\sum_k\\alpha_k)}{\\prod_k\\Gamma(\\alpha_k)}\\prod_k{y_k^{\\alpha_k-1}} \\\\ &amp;=\\exp{\\big(\\sum_k{(\\alpha_k-1)\\log{y_k}}-\\big[\\sum_k{\\log{\\Gamma(\\alpha_k)}}-\\log{\\Gamma(\\sum_k{\\alpha_k})}\\big]\\big)} \\end{align}$$ 1. $\\eta=\\alpha-1$ 2. $T(y)=\\log{y}$ 3. $a(\\eta)=\\sum_k{\\log{\\Gamma(\\alpha_k)}}-\\log{\\Gamma(\\sum_k{\\alpha_k})}$ 4. $b(y)=1$ 1.6.1 Method of Constructing GLMs 3 Assumptions \\(y\\|x,w \\sim \\text{ExponentialFamily}(\\eta)\\) \\(y\\) given \\(x\\&amp;w\\) follows some exponential family distribution with natural parameter \\(\\eta\\) \\(h(x)=\\text{E}[y\\|x]\\) Our hypothetical model \\(h(x)\\) should predict the expected value of \\(y\\) given \\(x\\) \\(\\eta=w^Tx\\) \\(\\eta\\) is linearly related to \\(x\\) Example 1: OLS (Ordinary Least Squares) (i.e. LinReg) \\[\\begin{align} h(x)&amp;=\\text{E}[y\\|x,w]\\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 2)} \\\\ &amp;=\\mu \\\\ &amp;=\\eta\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 1)} \\\\ &amp;=w^Tx\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 3)} \\end{align}\\] Example 2: Logistic Regression \\[\\begin{align} h(x)&amp;=\\text{E}[y\\|x,w]\\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 2)} \\\\ &amp;=\\phi \\\\ &amp;=\\frac{1}{1+e^{-\\eta}}\\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 1)} \\\\ &amp;=\\frac{1}{1+e^{-w^Tx}}\\ \\ \\ \\ \\ \\ &amp;\\text{(Assumption 3)} \\end{align}\\] Example 3: Softmax Regression Softmax is a method used in multiclass classification to select one output value \\(\\phi_i\\) of the highest probability among all the output values. \\[\\begin{equation} \\hat{y}=\\begin{bmatrix} \\phi_1 \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix} \\end{equation}\\] One-hot Encoding \\[\\begin{equation} y\\in \\{ 1,\\cdots,k \\} \\Rightarrow T(y)\\in \\mathbb{R}^{k} \\end{equation}\\] where \\[\\begin{equation} T(1)=\\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2)=\\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\\cdots, T(k)=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\end{equation}\\] Dummy Encoding \\[\\begin{equation} y\\in \\{ 1,\\cdots,k \\} \\Rightarrow T(y)\\in \\mathbb{R}^{k-1} \\end{equation}\\] where \\[\\begin{equation} T(1)=\\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2)=\\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\\cdots, T(k-1)=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, T(k)=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation}\\] Why Dummy Encoding &gt; One-hot Encoding? It reduces 1 entire column! Indicator Function \\[\\begin{equation} \\text{I}\\{ \\text{True} \\}=1,\\ \\text{I}\\{ \\text{False} \\}=0 \\end{equation}\\] Therefore, \\[\\begin{equation} T(y)_i =\\text{I}\\{ y=i \\} \\end{equation}\\] Therefore, \\[\\begin{equation} \\text{E}[T(y)_i] =P(y=i)=\\phi_i \\end{equation}\\] Exponential Family form \\[\\begin{align} p(y|\\phi)&amp;=\\prod_{i=1}^{k}{\\phi_i^{\\text{I}\\{ y=i \\}}} \\\\ &amp;=\\prod_{i=1}^{k-1}{\\phi_i^{T(y)_i}} \\cdot \\phi_k^{1-\\sum_{i=1}^{k-1}{T(y)_i}} \\\\ &amp;=\\exp{\\big(\\sum_{i=1}^{k-1}{T(y)_i\\log{(\\phi_i)}-\\sum_{i=1}^{k-1}{T(y)_i}\\log{(\\phi_k)}}+\\log{(\\phi_k)}\\big)} \\\\ &amp;=\\exp{\\big(\\sum_{i=1}^{k-1}{T(y)_i\\log{\\big(\\frac{\\phi_i}{\\phi_k}\\big)}+\\log{(\\phi_k)}\\big)}} \\\\ \\end{align}\\] \\(\\eta=\\begin{bmatrix}\\log{\\big(\\frac{\\phi_1}{\\phi_k}\\big)}\\ ;\\ \\cdots\\ ;\\ \\log{\\big(\\frac{\\phi_{k-1}}{\\phi_k}\\big)}\\end{bmatrix}\\) \\(T(y)=\\begin{bmatrix}T(y)_1\\ ;\\ \\cdots\\ ;\\ T(y)_k-1\\end{bmatrix}\\) \\(a(\\eta)=-\\log{(\\phi_k)}\\) \\(b(y)=1\\) Softmax Function (derived from \\(\\eta_i=\\log{\\big(\\frac{\\phi_i}{\\phi_k}\\big)}\\)) \\[\\begin{equation} \\phi_i=\\frac{e^{\\eta_i}}{\\sum_{j=1}^k{e^{\\eta_j}}} \\end{equation}\\] Probabilistic Interpretation of Softmax Regression \\[\\begin{equation} p(y=i|x,w)=\\frac{e^{w_i^Tx}}{\\sum_{j=1}^k{e^{w_i^Tx}}} \\end{equation}\\] Log Likelihood \\[\\begin{align} l(w)&amp;=\\sum_{i=1}^m{\\log{p(y^{(i)}|x^{(i)},w)}} \\\\ &amp;=\\sum_{i=1}^m{\\log{\\prod_{i=1}^{k}{\\Bigg(\\frac{e^{w_l^Tx^{(i)}}}{\\sum_{j=1}^k{e^{w_l^Tx^{(i)}}}}\\Bigg)^{\\text{I}\\{ y^{(i)}=l \\}}}}} \\end{align}\\] "],["cls.html", "2 Classification 2.1 Logistic Regression 2.2 k-Nearest Neighbors 2.3 Gaussian Discriminant Analysis 2.4 Naive Bayes Classifier 2.5 SVM", " 2 Classification 2.1 Logistic Regression Problem Setting Data: Observed pairs \\((x,y)\\), where \\(x\\in\\mathcal{X}\\) &amp; \\(y\\in\\mathcal{Y}\\) \\(\\mathcal{Y}=\\{-1,+1\\}\\lor\\{0,1\\}\\): binary classification \\(\\mathcal{Y}=\\{1,...,K\\}\\): multiclass classification Goal: Find a classifier \\(f\\) that can map input \\(x\\) to class \\(y\\): \\(y=f(x):\\ &quot;x\\in\\mathcal{X}&quot;\\rightarrow\\ &quot;y\\in\\mathcal{Y}&quot;\\) Model \\[\\begin{equation} \\hat{y}=g(w^Tx) \\end{equation}\\] \\(g(z)\\): a function that converts \\(w^Tx\\) to binary value Sigmoid Function (see Deep Learning for more funcs) \\[\\begin{equation} g(z)=\\sigma(z)=\\frac{1}{1+e^{-z}} \\end{equation}\\] Derivative (you will know why we need this in Deep Learning) \\[\\begin{align} g&#39;(z)&amp;=\\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\ &amp;=\\frac{e^{-z}(+1-1)}{(1+e^{-z})^2} \\\\ &amp;=g(z)(1-g(z)) \\end{align}\\] Cost Function single training example (derivation later) \\[\\begin{equation} \\mathcal{L}(\\hat{y},y)=-(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}) \\end{equation}\\] If \\(y=1\\rightarrow\\mathcal{L}(\\hat{y},y)=-\\log{\\hat{y}}\\rightarrow\\) want \\(\\mathcal{L}\\downarrow\\leftrightarrow\\hat{y}\\uparrow\\)\\(\\rightarrow\\hat{y}=1\\) If \\(y=0\\rightarrow\\mathcal{L}(\\hat{y},y)=-\\log{(1-\\hat{y})}\\rightarrow\\) want \\(\\mathcal{L}\\downarrow\\leftrightarrow\\hat{y}\\downarrow\\)\\(\\rightarrow\\hat{y}=0\\) entire training set \\[\\begin{equation} \\mathcal{J}(w)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})=\\text{mean}(\\mathcal{L}) \\end{equation}\\] Probabilistic Interpretation Assumptions \\[\\begin{align} P(y=1|x,w)&amp;=\\hat{y} \\\\ P(y=0|x,w)&amp;=1-\\hat{y} \\end{align}\\] Probabilistic Model of LogReg \\[\\begin{equation} p(y|x,w)=\\hat{y}^y(1-\\hat{y})^{1-y} \\end{equation}\\] Likelihood Function \\[\\begin{equation} L(w)=\\prod_{i=1}^{m}(\\hat{y}^{(i)})^{y^{(i)}}(1-\\hat{y}^{(i)})^{1-y^{(i)}} \\end{equation}\\] Log Likelihood \\[\\begin{align} l(w)&amp;=\\sum_{i=1}^{m}(y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log{(1-\\hat{y}^{(i)})}) \\\\ l(w)&amp;=-\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y},y) \\end{align}\\] MLE \\[\\begin{align} \\frac{\\partial l(w)}{\\partial w_j}&amp;=(\\frac{y}{g(w^Tx)}-\\frac{1-y}{1-g(w^Tx)})\\frac{\\partial g(w^Tx)}{\\partial w_j} \\\\ &amp;=(\\frac{y}{g(w^Tx)}-\\frac{1-y}{1-g(w^Tx)})g(w^Tx)(1-g(w^Tx))\\frac{\\partial(w^Tx)}{\\partial w_j} \\\\ &amp;=(y(1-g(w^Tx))-(1-y)g(w^Tx))x_j \\\\ &amp;=(y-\\hat{y})x_j \\end{align}\\] Gradient Descent \\[\\begin{align} w_j &amp;:= w_j-\\alpha\\frac{\\partial\\mathcal{L}(w)}{\\partial w_j} \\\\ &amp;=w_j+\\alpha(y-\\hat{y})x_j \\end{align}\\] Why is it also called Gradient Ascent? \\(\\because\\) we are trying to minimize the loss function \\(\\Leftrightarrow\\) maximize the likelihood function 2.2 k-Nearest Neighbors Algorithm For a new input \\(x\\), Return the \\(k\\) points closest to \\(x\\), indexed as \\(x_{i_1},...,x_{i_k}\\). Return the majority votes of \\(y_{i_1},...,y_{i_k}\\). Distances (how to measure closest) Euclidean distance: default measurement \\[\\begin{equation} \\|u-v\\|_ 2=\\Big(\\sum_{i=1}^n(u_i-v_i)^2\\Big)^{\\frac{1}{2}} \\end{equation}\\] \\(l_p\\): variation on Euclidean \\[\\begin{equation} \\|u-v\\|_ p=\\Big(\\sum_{i=1}^n|u_i-v_i|^p\\Big)^{\\frac{1}{p}}\\ \\ \\ |\\ p\\in[1,\\infty] \\end{equation}\\] Edit distance: for strings #modifications required to transform one string to the other Correlation distance: for signals how correlated 2 vectors are for signal detection \\(k\\) Smaller \\(k\\) \\(\\Rightarrow\\) smaller training error but could lead to overfitting Larger \\(k\\) \\(\\Rightarrow\\) more stable predictions due to voting Statistical Setting for Classification Performance Prediction accuracy: \\(P(f(x)=y)\\) Prediction error: \\(P(f(x)\\neq y)\\) Key Assumption for Supervised Learning \\[\\begin{equation} (x_i,y_i)\\sim\\mathcal{P}\\ \\ \\ |\\ \\ \\ i=1,\\cdots,n \\end{equation}\\] i.i.d. (independent &amp; identically distributed) We assume that the future should look like the past. 2.3 Gaussian Discriminant Analysis Learning Algorithms Discriminative Learning Algorithms \\[\\begin{equation} \\text{model }p(y|x)\\text{ directly}\\ \\ \\ (X \\Rightarrow Y) \\end{equation}\\] Generative Learning Algorithms \\[\\begin{equation} \\text{model }p(x|y)\\ \\&amp;\\ p(y)\\Rightarrow\\text{ use Bayes Theorem to get }p(y|x) \\end{equation}\\] Bayes Theorem \\[\\begin{equation} p(y|x)=\\frac{p(x|y)p(y)}{p(x)} \\end{equation}\\] Prior: \\(p(y)\\) Posterior: \\(p(y\\|x)\\) Simplification: \\(\\because\\) we are trying to find the output \\(y\\) with the highest probability given \\(x\\) \\(\\therefore\\) we can simplify Bayes Theorem for our purpose: \\[\\begin{align} \\mathop{\\arg\\max}_ {y}{p(y|x)}&amp;=\\mathop{\\arg\\max}_ {y}{\\frac{p(x|y)p(y)}{p(x)}} \\\\ &amp;=\\mathop{\\arg\\max}_ {y}{p(x|y)p(y)} \\end{align}\\] Bayes Theorem = the core of Generative Learning Algorithms Assumption: Multivariate Gaussian Distribution \\[\\begin{equation} p(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{\\big(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\big)} \\end{equation}\\] It is literally the same as Gaussian Distribution but with vector parameters: mean vector:  \\(\\mu\\in\\mathbb{R}^n\\) covariance matrix: \\(\\Sigma\\in\\mathbb{R}^{n\\times n}\\)  As a reminder and a comparison, here is the univariate version: \\[\\begin{equation} p(x|\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\end{equation}\\] Model \\[\\begin{align} y&amp;\\sim \\text{Bernoulli}{(\\phi)} \\\\ x|y=0&amp;\\sim N(\\mu_0,\\Sigma) \\\\ x|y=1&amp;\\sim N(\\mu_1,\\Sigma) \\\\ \\end{align}\\] Probabilistic Interpretation \\[\\begin{align} p(y)&amp;=\\phi^y(1-\\phi)^{1-y} \\\\ p(x|y=0)&amp;=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{\\big(-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)\\big)} \\\\ p(x|y=1)&amp;=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{\\big(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)\\big)} \\end{align}\\] log likelihood \\[\\begin{equation} l(\\phi,\\mu_0,\\mu_1,\\Sigma)=\\log{\\prod_{i=1}^{m}{p(x^{(i)}|y^{(i)};\\mu_0,\\mu_1,\\Sigma)p(y^{(i)};\\phi)}} \\end{equation}\\] MLE \\[\\begin{align} \\phi &amp;= \\frac{1}{m}\\sum_{i=1}^m{\\text{I}\\{ y^{(i)}=l \\}} \\\\ \\mu_0 &amp;= \\frac{\\sum_{i=1}^m{\\text{I}\\{ y^{(i)}=0 \\}x^{(i)}}}{\\sum_{i=1}^m{\\text{I}\\{ y^{(i)}=0 \\}}} \\\\ \\mu_1 &amp;= \\frac{\\sum_{i=1}^m{\\text{I}\\{ y^{(i)}=1 \\}x^{(i)}}}{\\sum_{i=1}^m{\\text{I}\\{ y^{(i)}=1 \\}}} \\\\ \\Sigma &amp;= \\frac{1}{m}\\sum_{i=1}^m{(x^{(i)}-\\mu_{y^{(i)}})(x^{(i)}-\\mu_{y^{(i)}})^T} \\end{align}\\] GDA vs LogReg GDA makes stronger modeling assumptions about data data efficient when assumptions (Gaussian distributions) are approximately correct LogReg makes weaker modeling assumptions about data data efficient when assumptions (Gaussian distributions) are not necessarily correct (e.g. \\(x\\|y\\sim \\text{Poisson}(\\lambda_1)\\) instead of \\(N(\\mu_0,\\Sigma)\\)) 2.4 Naive Bayes Classifier GDA vs NB GDA: \\(x\\) = continuous, real-valued vectors NB: \\(x\\) = discrete-valued vectors (e.g. text classification) Text Encoding (more in DL/RNN) We encode a text sentence into a vector of the same length as our dictionary (like a Python dictionary with vocabulary and their indices as key-value pairs): \\[\\begin{equation} x=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\begin{matrix} \\text{a} \\\\ \\text{abandon} \\\\ \\vdots \\\\ \\text{pewdiepie} \\\\ \\vdots \\\\ \\text{subscribe} \\\\ \\text{to} \\\\ \\vdots \\\\ \\text{zuck} \\end{matrix} \\end{equation}\\] The original sentence was Subscribe to Pewdiepie! and this text encoding method uses lowercases, throws punctuations and ignores the order of the sentence. This is convenient in some cases (e.g. spam email classification) but awful in the other cases (e.g. news/report-writer bots) Notice that \\(x\\in \\{0,1\\}^{\\text{len(dict)}}\\). Why notice this? Because we now have \\(2^\\text{len(dict)}\\) possible outcomes for \\(x\\). When we have a dictionary of over 20000 words, we have a \\(\\(2^{20000}-1\\)\\)-dimensional parameter vector. Have fun with that, laptop. Assumption: Conditional Independence \\[\\begin{equation} p(x_i|y)=p(x_i|y,x_j)\\ \\ \\ \\forall j\\neq i \\end{equation}\\] meaning: Given \\(y\\) as the condition, \\(x_i\\) is independent of \\(x_j\\). In the case of spam email classification, if we know that the email is spam, then whether or not pewdiepie is in the sentence does not change our belief of whether or not subscribe is in the sentence. Therefore, we can simplify our \\(p(x\\|y)\\) into: \\[\\begin{equation} p(x_1,...,x_{\\text{len(dict)}}|y)=\\prod_{i=1}^{n}{p(x_i|y)} \\end{equation}\\] Model \\[\\begin{align} \\phi_{i|y=1}&amp;=p(x_i=1|y=1) \\\\ \\phi_{i|y=0}&amp;=p(x_i=1|y=0) \\\\ \\phi_y&amp;=p(y=1) \\end{align}\\] Joint Likelihood \\[\\begin{equation} \\mathcal{L}(\\phi_y,\\phi_{i|y=0},\\phi_{i|y=1})=\\prod_{i=1}^{m}{p(x^{(i)},y^{(i)})} \\end{equation}\\] MLE \\[\\begin{align} \\phi_{j|y=1}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_j^{(i)}=1\\land y^{(i)}=1\\}}}{\\sum_{i=1}^m{I\\{y^{(i)}=1\\}}} \\\\ \\phi_{j|y=0}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_j^{(i)}=1\\land y^{(i)}=0\\}}}{\\sum_{i=1}^m{I\\{y^{(i)}=0\\}}} \\\\ \\phi_y&amp;=\\frac{\\sum_{i=1}^m{I\\{y^{(i)}=1\\}}}{m} \\end{align}\\] Quite intuitive. For example, \\(\\phi_{j\\|y=0}\\) = the fraction of non-spam emails with the word \\(j\\) in it. Prediction \\[\\begin{align} p(y=1|x_\\text{new})&amp;=\\frac{p(x_\\text{new}|y=1)p(y=1)}{p(x_\\text{new})} \\\\ &amp;=\\frac{\\prod_{i=1}^n{p(x_i|y=1)}\\cdot p(y=1)}{\\prod_{i=1}^n{p(x_i|y=1)}\\cdot p(y=1)+\\prod_{i=1}^n{p(x_i|y=0)}\\cdot p(y=0)} \\end{align}\\] Again, the formula is tedious but very intuitive. The \\(y\\) with the higher posterior probability will be chosen as the final prediction. Apply NB in GDA cases? Discretize: Just cut the continuous, real-valued \\(x\\) into small intervals and label them with a discrete-valued scale. 2.4.1 Laplace Smoothing Problem: What if there is a new word mrbeast in the email for prediction that our NB classifier has never learnt ever since it was born? A human would look it up on a dictionary, and so would our NB classifier. Assume the word mrbeast is the 1234th word in the dictionary, then: \\[\\begin{align} \\phi_{1234|y=1}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_{1234}^{(i)}=1\\land y^{(i)}=1\\}}}{\\sum_{i=1}^m{I\\{y^{(i)}=1\\}}}=0 \\\\ \\phi_{1234|y=0}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_{1234}^{(i)}=1\\land y^{(i)}=0\\}}}{\\sum_{i=1}^m{I\\{y^{(i)}=0\\}}}=0 \\\\ \\end{align}\\] Yes. NB thinks that the probability of seeing this word in either spam or non-spam email is \\(0\\), and therefore it would predict that: \\[\\begin{align} p(y=1|x_\\text{new})&amp;=\\frac{\\prod_{i=1}^n{p(x_i|y=1)}\\cdot p(y=1)}{\\prod_{i=1}^n{p(x_i|y=1)}\\cdot p(y=1)+\\prod_{i=1}^n{p(x_i|y=0)}\\cdot p(y=0)} \\\\ &amp;=\\frac{0}{0} \\end{align}\\] Because both numerator and denominator contains \\(p(x_{1234\\|y})=0\\). In summary, during prediction, if NB has never learnt a word \\(j\\), there will always \\(\\phi_j=0\\) ruining the entire prediction. How do we estimate the unknown? Algorithm: \\[\\begin{equation} \\phi_j=\\frac{\\sum_{i=1}^m{I\\{z^{(i)}=j\\}}+1}{m+k} \\end{equation}\\] where \\(k=\\text{#features}\\) if you forget. Lets check if it still satisfies our condition: \\[\\begin{equation} \\sum_{j=1}^k{\\phi_j}=\\sum_{j=1}^k{\\frac{\\sum_{i=1}^m{I\\{z^{(i)}=j\\}}+1}{m+k}}=\\frac{m+k}{m+k}=1 \\end{equation}\\] Nice. It still satisfies the basic sum rule. The estimates in NB will now become: \\[\\begin{align} \\phi_{j|y=1}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_{j}^{(i)}=1\\land y^{(i)}=1\\}}+1}{\\sum_{i=1}^m{I\\{y^{(i)}=1\\}}+2} \\\\ \\phi_{j|y=0}&amp;=\\frac{\\sum_{i=1}^m{I\\{x_{j}^{(i)}=1\\land y^{(i)}=0\\}}+1}{\\sum_{i=1}^m{I\\{y^{(i)}=0\\}}+2} \\\\ \\end{align}\\] 2.5 SVM 2.5.1 Intro Problem with Classification: This is a binary classification. The circles &amp; crosses are training examples with two different labels. The black line is the classifier, and it is able to classify circle and cross. For points like \\(\\text{A}\\) that are distant from the classifier, we are quite confident that they belong to cross. However, what about \\(\\text{B}\\) and \\(\\text{C}\\) that are super close to the decision boundary? Based on this classifier, \\(\\text{B}\\) belongs to cross and \\(\\text{C}\\) belongs to circle, but how confident are we about our classifier? What if our classifier is just slightly off and \\(\\text{C}\\) was actually cross? This, is SVM in a nutshell. 2.5.2 Margins Functional Margin \\[\\begin{equation} \\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx+b)\\ \\ \\ \\ \\ \\ \\|\\ y\\in\\{-1,1\\} \\end{equation}\\] Intuition: \\(\\hat{\\gamma}^{(i)}\\uparrow\\uparrow\\ \\rightarrow\\text{confidence}\\uparrow\\uparrow\\) When \\(y=1\\ \\rightarrow w^Tx+b \\&gt;\\&gt; 0\\). When \\(y=-1\\\\rightarrow w^Tx+b \\&lt;\\&lt; 0\\). Problem with functional margin: if \\(w\\rightarrow kw\\) and \\(b\\rightarrow kb\\) (where \\(k&gt;0\\)), then \\(g(w^Tx+b)=g(k(w^Tx+b))\\) but our \\(g(z)\\) here follows: \\[g(z)=\\begin{cases} -1&amp; \\text{if $z&lt;0$} \\\\ 1&amp; \\text{if $z&gt;0$} \\\\ \\end{cases}\\] that is, \\(z\\) and \\(kz\\) makes no difference for \\(g(z)\\). HOWEVER, the functional margin does change by a factor of \\(k\\) here, meaning that a large functional margin does not necessarily represent a confident prediction in this case.  Geometric Margin Refer back to the figure above. If we want to find the distance between point \\(A\\) and the decision boundary, which is \\(AA&#39;=\\gamma^{(i)}\\), what should we do? We normalize \\(w\\) to find the unit vector \\(\\frac{w}{\\lVert w \\rVert}\\), and we also have \\(A=x^{(i)}\\). Because \\(AA&#39;\\parallel \\overrightarrow{w}\\), we can find \\(A&#39;\\) by: \\[\\begin{equation} A&#39;=x^{(i)}-\\gamma^{(i)}\\frac{w}{\\lVert w \\rVert} \\end{equation}\\] and because \\(A&#39;\\) is on the decision boundary \\(w^Tx+b=0\\), we get \\[\\begin{align} &amp;w^TA&#39;+b=0 \\\\ \\Longrightarrow\\ &amp;w^Tx^{(i)}+b=w^T\\frac{w}{\\lVert w \\rVert}\\gamma^{(i)} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\bigg(w^T\\frac{w}{\\lVert w \\rVert}=\\frac{\\lVert w \\rVert^2}{\\lVert w \\rVert}\\bigg) \\\\ \\Longrightarrow\\ &amp;\\gamma^{(i)}=\\bigg(\\frac{w}{\\lVert w \\rVert}\\bigg)^Tx^{(i)}+\\frac{b}{\\lVert w \\rVert} \\end{align}\\] and if we generalize it with both classes of \\(y^{(i)}\\): \\[\\begin{equation} \\gamma^{(i)}=y^{(i)}\\Bigg(\\bigg(\\frac{w}{\\lVert w \\rVert}\\bigg)^Tx^{(i)}+\\frac{b}{\\lVert w \\rVert}\\Bigg) \\end{equation}\\] 2.5.3 Optimization: Lagrange Duality Constrained optimization problem \\[\\begin{equation} \\mathop{\\min}_ {w} f(w)\\ \\ \\text{s.t.}\\ h_i(w)=0\\ \\ \\forall i\\in\\{1,...,m\\} \\end{equation}\\] Interpretation: Minimize a function \\(f(w)\\) on the set \\(\\{w\\ \\|\\ h_i(w)=0\\ \\forall i\\in\\{1,...,m\\}\\}\\) where \\(w\\) satisfies the equality constraints. Lagrangian \\[\\begin{equation} \\mathcal{L}(w,\\beta)=f(w)+\\sum_{i=1}^{m}{\\beta_ih_i(w)} \\end{equation}\\] where \\(\\beta_i=\\) Lagrange multipliers, and then we solve it by \\(\\frac{\\partial{\\mathcal{L}}}{\\partial{w_i}}=0\\) and \\(\\frac{\\partial{\\mathcal{L}}}{\\partial{\\beta_i}}=0\\) Generalized constrained optimization problem \\[\\begin{align} \\mathop{\\min}_ {w} f(w)\\ \\ \\text{s.t.}\\ h_i(w)=0\\ \\ &amp;\\forall i\\in\\{1,...,m\\} \\\\ g_i(w)\\leq 0\\ \\ &amp;\\forall i\\in\\{1,...,n\\} \\end{align}\\] Interpretation: Add an inequality constraint to the original optimization problem. Generalized Lagrangian \\[\\begin{equation} \\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^{m}{\\beta_ih_i(w)}+\\sum_{i=1}^{n}{\\alpha_ig_i(w)} \\end{equation}\\] Primal optimization problem \\[\\begin{equation} p^* =\\mathop{\\min}_ {w} \\theta_{\\mathcal{P}}(w)=\\mathop{\\min}_ {w} \\mathop{\\max}_ {\\alpha,\\beta:\\alpha_i\\geq0} \\mathcal{L}(w,\\alpha,\\beta) \\end{equation}\\] Interpretation: Under the 2 primal constraints above, the maximum of our generalized lagrangian (labeled as \\(\\theta_{\\mathcal{P}}(w)\\)) is basically just \\(f(w)\\) as long as \\(\\alpha_i\\geq0\\ \\forall i\\in\\{1,...,m\\}\\): \\[\\begin{align} &amp;\\sum_{i=1}^{m}{\\beta_ih_i(w)}\\longrightarrow\\sum_{i=1}^{m}{\\beta_i\\cdot0}\\longrightarrow0 \\\\ &amp;\\sum_{i=1}^{m}{\\alpha_ig_i(w)}\\xrightarrow{\\alpha\\geq0,g(w)\\leq0}\\sum_{i=1}^{m}{(+0\\cdot-0)}\\longrightarrow0 \\end{align}\\] Therefore, this is just another way to write our generalized optimization problem. Dual optimization problem \\[\\begin{equation} d^* =\\mathop{\\max}_ {\\alpha,\\beta:\\alpha_i\\geq0} \\theta_{\\mathcal{D}}(\\alpha,\\beta)=\\mathop{\\max}_ {\\alpha,\\beta:\\alpha_i\\geq0} \\mathop{\\min}_ {w} \\mathcal{L}(w,\\alpha,\\beta) \\end{equation}\\] Interpretation: This is basically the same problem as primal except that \\(\\mathop{\\max}\\) and \\(\\mathop{\\min}\\) are exchanged. However, their values are not necessarily equal. Instead, they follow the following relationship: \\[\\begin{equation} d^* \\leq p^* \\end{equation}\\] The intuition is simple. Suppose we have a function \\(f(x,y)\\), then: \\[\\begin{align} \\mathop{\\min}_ {w} f(x,w)\\leq f(x,y)\\leq \\mathop{\\max}_ {v} f(v,y) \\\\ \\mathop{\\min}_ {u} f(u,y)\\leq f(x,y)\\leq \\mathop{\\max}_ {t} f(x,t) \\end{align}\\] This definitely holds for all functions in the world. Therefore, the following also holds: \\[\\begin{equation} \\mathop{\\max}_ {x} \\big(\\mathop{\\min}_ {w} f(x,w)\\big)\\leq \\mathop{\\min}_ {y} \\big(\\mathop{\\max}_ {v} f(v,y)\\big) \\end{equation}\\] which is basically saying that \\(\\mathop{\\max}\\mathop{\\min}\\leq\\mathop{\\min}\\mathop{\\max}\\) for all multivariate functions, including our Lagrangian. Karush-Kuhn-Tucker Conditions (KKT) Under the above assumptions, there must exist \\(w^*,\\alpha^ *,\\beta^ *\\) so that \\(w^*\\) is the solution to the primal problem \\(\\alpha^ *,\\beta^ *\\) are the solution to the dual problem \\(p^* =d^* =\\mathcal{L}(w^* ,\\alpha^ * ,\\beta^ * )\\) KKT Conditions: \\(w^*,\\alpha^ *,\\beta^ *\\) must satisfy: \\[\\begin{align} \\frac{\\partial}{\\partial w_i}\\mathcal{L}(w^*,\\alpha^*,\\beta^* )&amp;=0\\ \\ i=1,\\cdots,n \\\\ \\frac{\\partial}{\\partial \\beta_i}\\mathcal{L}(w^*,\\alpha^*,\\beta^* )&amp;=0\\ \\ i=1,\\cdots,l \\\\ \\alpha_i^* g_i(w^* )&amp;=0\\ \\ i=1,\\cdots,k \\\\ g_i(w^* )&amp;\\leq0\\ \\ i=1,\\cdots,k \\\\ \\alpha_i^* &amp;\\geq0\\ \\ i=1,\\cdots,k \\end{align}\\] "]]
